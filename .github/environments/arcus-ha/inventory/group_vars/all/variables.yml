# Unset the network ID so that a network + router are provisioned
infra_network_id:

# Unset the infra IP so we can use the ingress IP for the ingress controller
infra_fixed_floatingip:
capi_cluster_addons_ingress_load_balancer_ip: "{{ lookup('env', 'INGRESS_IP') }}"

# Flavor auto-detection picks the wrong flavors on Arcus, so override them
#   The flavor to use for the seed VM
infra_flavor_id: >-
  {{ lookup('pipe', 'openstack flavor show vm.azimuth.ci.ec1.medium -f value -c id') }}
#   The flavor to use for the control plane nodes
capi_cluster_control_plane_flavor: vm.azimuth.ci.ec1.medium
#   The flavor to use for worker nodes
capi_cluster_worker_flavor: vm.azimuth.ci.en1.medium

# Although this is a "HA" test, what we are really testing is the spawning
# of the CAPI cluster and deployment of Azimuth onto that
# So one control plane node is sufficient for that
capi_cluster_control_plane_count: 1
capi_cluster_worker_count: 2

# Use a single replica for Consul
# The risk of failed upgrades is too great, and it is going away soon
consul_server_replicas: 1

# Pick up the reserved IP for the Zenith SSHD LB
zenith_sshd_service_load_balancer_ip: "{{ lookup('env', 'ZENITH_SSHD_IP') }}"

# Enable Velero just to check that installation works
velero_enabled: false
