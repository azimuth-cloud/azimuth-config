# Unset the network ID so that a network + router are provisioned
infra_network_id:

# Unset the infra IP so we can use the ingress IP for the ingress controller
infra_fixed_floatingip:
capi_cluster_addons_ingress_load_balancer_ip: "{{ lookup('env', 'INGRESS_IP') }}"

# Flavor auto-detection picks the wrong flavors on Arcus, so override them
#   The flavor to use for the seed VM
infra_flavor_id: >-
  {{ lookup('pipe', 'openstack flavor show vm.azimuth.ci.ec1.medium -f value -c id') }}
#   The flavor to use for the control plane nodes
capi_cluster_control_plane_flavor: vm.azimuth.ci.ec1.medium
#   The flavor to use for worker nodes
capi_cluster_worker_flavor: vm.azimuth.ci.en1.medium

# Although this is a "HA" test, what we are really testing is the spawning
# of the CAPI cluster and deployment of Azimuth onto that
# So one control plane node is sufficient for that
capi_cluster_control_plane_count: 1
capi_cluster_worker_count: 2

# Use a single replica for Consul
# The risk of failed upgrades is too great, and it is going away soon
consul_server_replicas: 1

# Pick up the reserved IP for the Zenith SSHD LB
zenith_sshd_service_load_balancer_ip: "{{ lookup('env', 'ZENITH_SSHD_IP') }}"

# Configure Velero backups
velero_enabled: "{{ not (not velero_aws_access_key_id) }}"
velero_s3_url: https://object.arcus.openstack.hpc.cam.ac.uk
velero_bucket_name: azimuth-ci-backups
velero_aws_access_key_id: "{{ lookup('env', 'VELERO_S3_ACCESS_KEY') }}"
velero_aws_secret_access_key: "{{ lookup('env', 'VELERO_S3_SECRET_KEY') }}"
