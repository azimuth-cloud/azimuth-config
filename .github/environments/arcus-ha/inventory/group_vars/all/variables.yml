# Unset the network ID so that a network + router are provisioned
infra_network_id:

# Unset the infra IP so we can use the ingress IP for the ingress controller
infra_fixed_floatingip:
capi_cluster_addons_ingress_load_balancer_ip: "{{ lookup('env', 'INGRESS_IP') }}"

# Flavor auto-detection picks the wrong flavors on Arcus, so override them
#   The flavor to use for the seed VM
infra_flavor_id: >-
  {{ lookup('pipe', 'openstack flavor show vm.azimuth.ci.ec1.medium -f value -c id') }}
#   The flavor to use for the control plane nodes
capi_cluster_control_plane_flavor: vm.azimuth.ci.ec1.medium
#   The flavor to use for worker nodes
capi_cluster_worker_flavor: vm.azimuth.ci.en1.medium

# Although this is a "HA" test, what we are really testing is the spawning
# of the CAPI cluster and deployment of Azimuth onto that
# So one control plane node is sufficient for that
capi_cluster_control_plane_count: 1
capi_cluster_worker_count: 2

# Use a single replica for Consul
# The risk of failed upgrades is too great, and it is going away soon
consul_server_replicas: 1

# Enable Velero just to check that installation works
velero_enabled: true
velero_s3_url: https://required-but-not-used.com
velero_bucket_name: not-used
velero_backup_schedule_enabled: true
velero_backup_schedule_name: default
velero_backup_schedule_timings: "0 0 * * *"
velero_backup_schedule_ttl: "168h"
velero_aws_access_key_id: required-but-not-used
velero_aws_secret_access_key : required-but-not-used
