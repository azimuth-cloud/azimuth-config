#####
# Configuration for the seed node (HA) or single node
#####

# The size in GB for the data volume
# This will hold all cluster data, including Kubernetes resources, and also PVC data
infra_data_volume_size: 50

#####
# Configuration for the HA cluster
#####

# The Kubernetes version that will be used for the HA cluster
#   N.B. This is populated automatically using community images by default
# capi_cluster_kubernetes_version: 1.23.8
# The ID of the image that will be used for the nodes of the HA cluster
#   N.B. This is populated automatically using community images by default
# capi_cluster_machine_image_id: "<image id>"
# The name of the flavor to use for control plane nodes
capi_cluster_control_plane_flavor: 8cpu-8gbmem-30gbdisk
# The name of the flavor to use for worker nodes
capi_cluster_worker_flavor: 16cpu-16gbmem-30gbdisk
# The number of control plane nodes
capi_cluster_control_plane_count: 3
# The number of worker nodes
capi_cluster_worker_count: 3
# The fixed floating IP to associate with the load balancer for the ingress controller
# This IP must be pre-allocated to the project and should have the wildcard ingress domain assigned to it
# TODO: not sure where this came from, doesn't appear to be in use on TAV1,CCI1 or CCI2
#capi_cluster_addons_ingress_load_balancer_ip: "136.156.131.33"
capi_cluster_root_volume_size: 100
capi_cluster_apiserver_floating_ip: true

# Storage settings for tenant clusters
azimuth_capi_operator_capi_helm_root_volume_size: 50
azimuth_capi_operator_capi_helm_root_volume_availability_zone: "nova"

# Separate etcd device config 

# Management cluster

# Default size of the etcd block device for management clusters (in GB)
capi_cluster_etcd_blockdevice_size: 8
# Default block device type for etcd in management clusters ('Local' or 'Volume')
capi_cluster_etcd_blockdevice_type: "Volume"
# Default availability zone for the etcd block device if 'Volume' type is used in management clusters
capi_cluster_etcd_blockdevice_volume_az: "nova"

# Tenant Clusters

# Default size of the etcd block device for tenant clusters (in GB)
azimuth_capi_operator_capi_helm_etcd_blockdevice_size: 8
# Default block device type for etcd in tenant clusters ('Local' or 'Volume')
azimuth_capi_operator_capi_helm_etcd_blockdevice_type: "Volume" 
# Default availability zone for the etcd block device if 'Volume' type is used
azimuth_capi_operator_capi_helm_etcd_blockdevice_volume_az: "nova"

#####
# Ingress configuration
#####
# The base domain to use for ingress resources
ingress_base_domain: "{{ capi_cluster_addons_ingress_load_balancer_ip | replace('.', '-') }}.sslip.io"

# Indicates if cert-manager should be enabled
# Currently, TLS is enabled for ingress iff cert-manager is enabled
certmanager_enabled: yes

# Indicates if Harbor should be enabled to provide pull-through caches
harbor_enabled: no

# TODO: not sure where this came from, doesn't appear to be in use on TAV1,CCI1 or CCI2
#zenith_sshd_service_load_balancer_ip: "136.156.131.209"

#####
# Azimuth configuration
#####

azimuth_capi_operator_external_network_id: "{{ infra_external_network_id }}"

openstack_loadbalancer_provider: ovn

# The name of the current cloud
azimuth_current_cloud_name: ecmwf
# The label for the current cloud
azimuth_current_cloud_label: ECMWF

# Indicates if the Zenith app proxy should be enabled
azimuth_apps_enabled: yes
# Indicates if Kubernetes support should be enabled
azimuth_kubernetes_enabled: yes
# Indicates if Cluster-as-a-Service (CaaS) should be enabled
#azimuth_clusters_enabled: yes

#####
# Configuration of authenticators / authentication methods
#####
# Whether the password authenticator should be enabled (enabled by default)
azimuth_authenticator_password_enabled: true
# The label for the password authenticator
azimuth_authenticator_password_label: "Username + Password"

# Whether the federated authenticator should be enabled (not enabled by default)
azimuth_authenticator_federated_enabled: false
# The label for the federated authenticator
azimuth_authenticator_federated_label: "Federated"
# The provider for the federated authenticator
# This should correspond to the Keystone federation URL, e.g. <auth url>/auth/OS-FEDERATION/websso/<provider>
azimuth_authenticator_federated_provider: oidc

#####
# Configuration for CaaS appliances
#####
azimuth_clusters_enabled: yes
# If CaaS is enabled and the StackHPC Slurm appliance is enabled (the default), this
# is the id of a Rocky 8 image that will be used for Slurm clusters
#   N.B. This is populated automatically using community images by default
# azimuth_caas_stackhpc_slurm_appliance_image: "<image id>"
# The name of the flavor to use for login nodes
#azimuth_caas_stackhpc_slurm_appliance_login_flavor_name: "<flavor name>"
# The name of the flavor to use for control nodes
#azimuth_caas_stackhpc_slurm_appliance_control_flavor_name: "<flavor name>"

# The ID of the desktop or webconsole image to use for the workstation appliance
# See https://object.arcus.openstack.hpc.cam.ac.uk/swift/v1/AUTH_f0dc9cb312144d0aa44037c9149d2513/azimuth-images-prerelease/
#   N.B. This is populated automatically using community images by default
# azimuth_caas_stackhpc_workstation_image: "<image id>"

# The ID of the repo2docker image to use for the repo2docker appliance
# See https://object.arcus.openstack.hpc.cam.ac.uk/swift/v1/AUTH_f0dc9cb312144d0aa44037c9149d2513/azimuth-images-prerelease/
#   N.B. This is populated automatically using community images by default
# azimuth_caas_stackhpc_repo2docker_image: "<image id>"

#####
# Terraform State
####

terraform_backend_type: s3

# The region to use
# Ceph does not normally use the region, but Terraform requires it
terraform_s3_region: not-used-but-required
terraform_s3_skip_region_validation: "true"

# The key to use for the state for the environment
#
# Using the azimuth_environment variable in the key means that the state
# for each concrete environment is stored in a separate key, even if this
# configuration is in a shared mixin environment
terraform_s3_key: "{{ azimuth_environment }}.tfstate"

# The STS API doesn't exist for Ceph
terraform_s3_skip_credentials_validation: "true"

# Tell Terraform to use path-style URLs, e.g. <host>/<bucket>, instead of
# subdomain-style URLs, e.g. <bucket>.<host>
terraform_s3_force_path_style: "true"

terraform_backend_config:
  endpoint: "{{ terraform_s3_endpoint }}"
  region: "{{ terraform_s3_region }}"
  bucket: "{{ terraform_s3_bucket }}"
  key: "{{ terraform_s3_key }}"
  skip_credentials_validation: "{{ terraform_s3_skip_credentials_validation }}"
  force_path_style: "{{ terraform_s3_force_path_style }}"
  skip_region_validation: "{{ terraform_s3_skip_region_validation }}" 
  access_key: "{{ terraform_s3_access_key }}"
  secret_key: "{{ terraform_s3_secret_key }}"

# Valero backup & recovery
velero_enabled: true
velero_bucket_name: velero-backup

# Whether or not to perform scheduled backups
velero_backup_schedule_enabled: true
# Name for backup schedule kubernetes resource
velero_backup_schedule_name: default
# Schedule to use for backups (defaults to every day at midnight)
# See https://en.wikipedia.org/wiki/Cron for format options
velero_backup_schedule_timings: "0 0 * * *"
# Time-to-live for existing backups (defaults to 1 week)
# See https://pkg.go.dev/time#ParseDuration for duration format options
velero_backup_schedule_ttl: "168h"
