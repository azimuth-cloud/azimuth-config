{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Azimuth Operator Documentation This documentation describes how to manage deployments of Azimuth , including all the required dependencies. Deploying Azimuth Azimuth is deployed using Ansible with playbooks from the azimuth-ops Ansible collection , driven by configuration derived from the azimuth-config reference configuration . The azimuth-config repository is designed to be forked for a specific site and is structured into multiple environments . This structure allows common configuration to be shared but overridden where required using composition of environments. To try out Azimuth on your OpenStack cloud, you can follow these instructions to get a simple single-node deployment. For a production-ready deployment, you should follow the steps in the best practice document . Structure of an Azimuth deployment A fully-featured Azimuth deployment consists of many components, such as Zenith , Cluster API and the CaaS operator , which require a Kubernetes cluster to run. However when you consider an Azimuth deployment as a whole, the only real dependency is an OpenStack cloud to target - we can create a Kubernetes cluster within an OpenStack project on the target cloud to host our Azimuth deployment. This is exactly what the playbooks in the azimuth-ops collection will do, when driven by a configuration derived from azimuth-config . There are two methods that azimuth-ops can use to deploy Azimuth and all of its dependencies: Onto a managed single-node K3s cluster in an OpenStack project. Onto a managed highly-available Kubernetes cluster in an OpenStack project. Option 1 is useful for development or demo deployments, but is not suitable for a production deployment. Option 2 is the recommended deployment mechanism for most deployments. In this mode, OpenTofu , an open-source fork of Terraform , is used to provision a single-node K3s cluster that is configured as a Cluster API management cluster. Cluster API is then used to provision a highly-available Kubernetes cluster in the same OpenStack project onto which Azimuth is deployed. Warning Option 2 requires that Octavia is available on the target cloud to provide load-balancers for Azimuth components.","title":"Home"},{"location":"#azimuth-operator-documentation","text":"This documentation describes how to manage deployments of Azimuth , including all the required dependencies.","title":"Azimuth Operator Documentation"},{"location":"#deploying-azimuth","text":"Azimuth is deployed using Ansible with playbooks from the azimuth-ops Ansible collection , driven by configuration derived from the azimuth-config reference configuration . The azimuth-config repository is designed to be forked for a specific site and is structured into multiple environments . This structure allows common configuration to be shared but overridden where required using composition of environments. To try out Azimuth on your OpenStack cloud, you can follow these instructions to get a simple single-node deployment. For a production-ready deployment, you should follow the steps in the best practice document .","title":"Deploying Azimuth"},{"location":"#structure-of-an-azimuth-deployment","text":"A fully-featured Azimuth deployment consists of many components, such as Zenith , Cluster API and the CaaS operator , which require a Kubernetes cluster to run. However when you consider an Azimuth deployment as a whole, the only real dependency is an OpenStack cloud to target - we can create a Kubernetes cluster within an OpenStack project on the target cloud to host our Azimuth deployment. This is exactly what the playbooks in the azimuth-ops collection will do, when driven by a configuration derived from azimuth-config . There are two methods that azimuth-ops can use to deploy Azimuth and all of its dependencies: Onto a managed single-node K3s cluster in an OpenStack project. Onto a managed highly-available Kubernetes cluster in an OpenStack project. Option 1 is useful for development or demo deployments, but is not suitable for a production deployment. Option 2 is the recommended deployment mechanism for most deployments. In this mode, OpenTofu , an open-source fork of Terraform , is used to provision a single-node K3s cluster that is configured as a Cluster API management cluster. Cluster API is then used to provision a highly-available Kubernetes cluster in the same OpenStack project onto which Azimuth is deployed. Warning Option 2 requires that Octavia is available on the target cloud to provide load-balancers for Azimuth components.","title":"Structure of an Azimuth deployment"},{"location":"best-practice/","text":"Best practice for production deployments This document guides you through the process of setting up a production-ready Azimuth deployment following recommended best practice. Repository Before building your Azimuth configuration, you must first set up your configuration repository , including initialising git-crypt for encrypting secrets . It is recommended to use a feature branch workflow to make changes to your Azimuth configuration in a controlled way. Environments An Azimuth configuration repository contains multiple environments , some of which contain common configuration (\"mixin\" environments), and some of which represent a deployment (\"concrete\" environments). For a production deployment of Azimuth, there should be at least two concrete environments that are deployed from the main branch: production - the Azimuth deployment that is provided to end users staging - used to validate changes before pushing to production Both of these environments should be highly-available deployments, in order to fully test the upgrade process. It is recommended to have a single node aio environment that also follows the production environment as closely as possible. This can be used for testing changes to the configuration before they are merged to main . In order for validation in the aio and staging environments to be meaningful, the configuration of these environments should be as similar to production as possible. To do this, two site-specific mixins should be used: site - contains configuration that is common between aio , staging and production , e.g. enabled features, available platforms site-ha - contains configuration for the HA setup that is common between staging and production The environments would then be layered as follows: base --> singlenode --> site --> aio base --> ha --> site --> site-ha --> staging base --> ha --> site --> site-ha --> production with only necessary differences configured in each environment, e.g. the ingress base domain, between staging and production . OpenStack projects Azimuth is usually deployed on the cloud that is being targeted for workloads. It is recommended to have three OpenStack projects for a production Azimuth deployment, to contain: A highly-available (HA) production deployment, e.g. azimuth-production A HA staging deployment, e.g. azimuth-staging All-in-one (AIO) deployments for validating changes, e.g. azimuth-cicd The production and staging projects must have sufficient quota for a HA Azimuth deployment. The required quota in the CI/CD project will depend on the number of proposed changes that are open concurrently. Users that login to Azimuth will see a list of the OpenStack projects they can access. Any OpenStack project can be used to deploy Azimuth workloads, as long as the networking requirements have been met. Azimuth creates application credentials to be able to create the user workloads using the appropriate automation. Deployment Prerequisites Before proceeding with an Azimuth deployment , you should ensure that you can fulfil all the prerequisites . The prerequisites include verifying that the target cloud has the required features available, generating OpenStack application credentials and ensuring that the required DNS records and TLS certificates are available. OpenTofu state Azimuth deployments use OpenTofu , an open-source fork of Terraform , to manage some parts of the infrastructure. A remote state store must be configured in order to persist the OpenTofu state across playbook executions. If GitLab is being used for the Azimuth configuration repository, it is recommended to use GitLab-managed Terraform state for this. If not, S3 is the preferred approach. Continuous delivery A production Azimuth deployment should use continuous delivery , where changes to the configuration are automatically deployed to the aio and staging environments. The ansible-playbook command should never be executed manually. The recommended approach is to automatically deploy an independent aio environment for each feature branch, also known as per-branch dynamic review environments . This allows changes to be validated before they are merged to main . Once a change is merged to main , it will be deployed automatically to the staging environment. Merging to main will also create a job to deploy to production , but that job will require a manual trigger. Once the change has been validated in staging , the job to deploy to production can be actioned. It is also possible to run integration tests against a deployment. It is recommended that this is configured to run automatically for at least aio and staging environments. It should also be safe to run against production as long as the tests are configured to target an isolated project. A sample GitLab CI/CD configuration is provided that implements this workflow for GitLab-hosted repositories. Monitoring Before going into production, be sure you can access the monitoring . Also ensure you have configured alert manager such that you will notice any alerts on staging and production. Disaster recovery Azimuth uses Velero to backup the data that is required to restore an Azimuth instance in the event of a catastrophic failure. This functionality is not enabled by default, as it requires credentials for an S3 bucket in which the backups will be stored. It is recommended that disaster recovery is enabled for a production deployment. Configuration You are now ready to begin adding configuration to your environments. When building an environment for the first time, it is recommended to follow each documentation page in order, beginning with the Deployment method . Tip Remember to share as much configuration as possible between all your environments !","title":"Best practice for production deployments"},{"location":"best-practice/#best-practice-for-production-deployments","text":"This document guides you through the process of setting up a production-ready Azimuth deployment following recommended best practice.","title":"Best practice for production deployments"},{"location":"best-practice/#repository","text":"Before building your Azimuth configuration, you must first set up your configuration repository , including initialising git-crypt for encrypting secrets . It is recommended to use a feature branch workflow to make changes to your Azimuth configuration in a controlled way.","title":"Repository"},{"location":"best-practice/#environments","text":"An Azimuth configuration repository contains multiple environments , some of which contain common configuration (\"mixin\" environments), and some of which represent a deployment (\"concrete\" environments). For a production deployment of Azimuth, there should be at least two concrete environments that are deployed from the main branch: production - the Azimuth deployment that is provided to end users staging - used to validate changes before pushing to production Both of these environments should be highly-available deployments, in order to fully test the upgrade process. It is recommended to have a single node aio environment that also follows the production environment as closely as possible. This can be used for testing changes to the configuration before they are merged to main . In order for validation in the aio and staging environments to be meaningful, the configuration of these environments should be as similar to production as possible. To do this, two site-specific mixins should be used: site - contains configuration that is common between aio , staging and production , e.g. enabled features, available platforms site-ha - contains configuration for the HA setup that is common between staging and production The environments would then be layered as follows: base --> singlenode --> site --> aio base --> ha --> site --> site-ha --> staging base --> ha --> site --> site-ha --> production with only necessary differences configured in each environment, e.g. the ingress base domain, between staging and production .","title":"Environments"},{"location":"best-practice/#openstack-projects","text":"Azimuth is usually deployed on the cloud that is being targeted for workloads. It is recommended to have three OpenStack projects for a production Azimuth deployment, to contain: A highly-available (HA) production deployment, e.g. azimuth-production A HA staging deployment, e.g. azimuth-staging All-in-one (AIO) deployments for validating changes, e.g. azimuth-cicd The production and staging projects must have sufficient quota for a HA Azimuth deployment. The required quota in the CI/CD project will depend on the number of proposed changes that are open concurrently. Users that login to Azimuth will see a list of the OpenStack projects they can access. Any OpenStack project can be used to deploy Azimuth workloads, as long as the networking requirements have been met. Azimuth creates application credentials to be able to create the user workloads using the appropriate automation.","title":"OpenStack projects"},{"location":"best-practice/#deployment-prerequisites","text":"Before proceeding with an Azimuth deployment , you should ensure that you can fulfil all the prerequisites . The prerequisites include verifying that the target cloud has the required features available, generating OpenStack application credentials and ensuring that the required DNS records and TLS certificates are available.","title":"Deployment Prerequisites"},{"location":"best-practice/#opentofu-state","text":"Azimuth deployments use OpenTofu , an open-source fork of Terraform , to manage some parts of the infrastructure. A remote state store must be configured in order to persist the OpenTofu state across playbook executions. If GitLab is being used for the Azimuth configuration repository, it is recommended to use GitLab-managed Terraform state for this. If not, S3 is the preferred approach.","title":"OpenTofu state"},{"location":"best-practice/#continuous-delivery","text":"A production Azimuth deployment should use continuous delivery , where changes to the configuration are automatically deployed to the aio and staging environments. The ansible-playbook command should never be executed manually. The recommended approach is to automatically deploy an independent aio environment for each feature branch, also known as per-branch dynamic review environments . This allows changes to be validated before they are merged to main . Once a change is merged to main , it will be deployed automatically to the staging environment. Merging to main will also create a job to deploy to production , but that job will require a manual trigger. Once the change has been validated in staging , the job to deploy to production can be actioned. It is also possible to run integration tests against a deployment. It is recommended that this is configured to run automatically for at least aio and staging environments. It should also be safe to run against production as long as the tests are configured to target an isolated project. A sample GitLab CI/CD configuration is provided that implements this workflow for GitLab-hosted repositories.","title":"Continuous delivery"},{"location":"best-practice/#monitoring","text":"Before going into production, be sure you can access the monitoring . Also ensure you have configured alert manager such that you will notice any alerts on staging and production.","title":"Monitoring"},{"location":"best-practice/#disaster-recovery","text":"Azimuth uses Velero to backup the data that is required to restore an Azimuth instance in the event of a catastrophic failure. This functionality is not enabled by default, as it requires credentials for an S3 bucket in which the backups will be stored. It is recommended that disaster recovery is enabled for a production deployment.","title":"Disaster recovery"},{"location":"best-practice/#configuration","text":"You are now ready to begin adding configuration to your environments. When building an environment for the first time, it is recommended to follow each documentation page in order, beginning with the Deployment method . Tip Remember to share as much configuration as possible between all your environments !","title":"Configuration"},{"location":"environments/","text":"Environments An Azimuth configuration repository is structured as multiple \"environments\" that can be composed. Some of these environments are \"concrete\", meaning that they provide enough information to make a deployment (e.g. development, staging, production), and some are \"mixin\" environments providing common configuration that can be incorporated into concrete environments using Ansible's support for multiple inventories . A concrete environment must be \"activated\" before any operations can be performed. Environments live in the environments directory of your configuration repository. A mixin environment contains only group_vars files and an empty hosts file (so that Ansible treats it as an inventory). A concrete environment must contain an ansible.cfg file defining the \"layering\" of inventories and a clouds.yaml file containing an OpenStack Application Credential for the project into which Azimuth will be deployed. Using mixin environments The following fragment demonstrates how to layer inventories in the ansible.cfg file for a highly-available (HA) deployment: ansible.cfg [defaults] inventory = ../base/inventory,../ha/inventory,./inventory For a single node deployment, replace the ha environment with singlenode . Tip If the same variable is defined in multiple inventories, the right-most inventory takes precedence. Available mixin environments The following mixin environments are provided and maintained in this repository, and should be used as the basis for your concrete environments: base Contains the core configuration required to enable an environment and sets defaults. ha Contains overrides that are specific to an HA deployment. singlenode Contains overrides that are specific to a single-node deployment. By keeping the azimuth-config repository as an upstream of your site configuration repository, you can rebase onto or merge the latest configuration to pick up changes to these mixins. The azimuth-config repository contains an example of a concrete environment in environments/example that should be used as a basis for your own concrete environment(s). Depending how many concrete environments you have, you may wish to define mixin environments containing site-specific information that is common to several concrete environments, e.g. image and flavor IDs or the location of an ACME server. A typical layering of inventories might be: base -> singlenode -> site -> development base -> ha -> site -> staging base -> ha -> site -> production Linux environment variables azimuth-config environments are able to define Linux environment variables that are exported into the current shell when the environment is activated. This is accomplished by using statements of the form: env MY_VAR = \"some value\" The azimuth-config activate script exports environment variables defined in the following files: env and env.secret Contain environment variables that are common across all environments. environments/<env name>/env and environments/<env name>/env.secret Contain environment variables that are specific to the environment. In both cases, environment variables whose values should be kept private should be placed in the env.secret variant and should be encrypted .","title":"Environments"},{"location":"environments/#environments","text":"An Azimuth configuration repository is structured as multiple \"environments\" that can be composed. Some of these environments are \"concrete\", meaning that they provide enough information to make a deployment (e.g. development, staging, production), and some are \"mixin\" environments providing common configuration that can be incorporated into concrete environments using Ansible's support for multiple inventories . A concrete environment must be \"activated\" before any operations can be performed. Environments live in the environments directory of your configuration repository. A mixin environment contains only group_vars files and an empty hosts file (so that Ansible treats it as an inventory). A concrete environment must contain an ansible.cfg file defining the \"layering\" of inventories and a clouds.yaml file containing an OpenStack Application Credential for the project into which Azimuth will be deployed.","title":"Environments"},{"location":"environments/#using-mixin-environments","text":"The following fragment demonstrates how to layer inventories in the ansible.cfg file for a highly-available (HA) deployment: ansible.cfg [defaults] inventory = ../base/inventory,../ha/inventory,./inventory For a single node deployment, replace the ha environment with singlenode . Tip If the same variable is defined in multiple inventories, the right-most inventory takes precedence.","title":"Using mixin environments"},{"location":"environments/#available-mixin-environments","text":"The following mixin environments are provided and maintained in this repository, and should be used as the basis for your concrete environments: base Contains the core configuration required to enable an environment and sets defaults. ha Contains overrides that are specific to an HA deployment. singlenode Contains overrides that are specific to a single-node deployment. By keeping the azimuth-config repository as an upstream of your site configuration repository, you can rebase onto or merge the latest configuration to pick up changes to these mixins. The azimuth-config repository contains an example of a concrete environment in environments/example that should be used as a basis for your own concrete environment(s). Depending how many concrete environments you have, you may wish to define mixin environments containing site-specific information that is common to several concrete environments, e.g. image and flavor IDs or the location of an ACME server. A typical layering of inventories might be: base -> singlenode -> site -> development base -> ha -> site -> staging base -> ha -> site -> production","title":"Available mixin environments"},{"location":"environments/#linux-environment-variables","text":"azimuth-config environments are able to define Linux environment variables that are exported into the current shell when the environment is activated. This is accomplished by using statements of the form: env MY_VAR = \"some value\" The azimuth-config activate script exports environment variables defined in the following files: env and env.secret Contain environment variables that are common across all environments. environments/<env name>/env and environments/<env name>/env.secret Contain environment variables that are specific to the environment. In both cases, environment variables whose values should be kept private should be placed in the env.secret variant and should be encrypted .","title":"Linux environment variables"},{"location":"try/","text":"Try Azimuth If you have access to a project on an OpenStack cloud, you can try Azimuth! The azimuth-config repository contains a special environment called demo that will provision a short-lived Azimuth deployment for demonstration purposes only . This environment attempts to infer all required configuration from the target OpenStack cloud - if this process is unsuccessful, an error will be produced. Danger Inferring configuration in the way that the demo environment does is not recommended as it is not guaranteed to produce the same result each time. For production deployments it is better to be explicit. To get started with a production deployment, see the best practice guide . Deploying a demo instance The Azimuth deployment requires a clouds.yaml to run. Ideally, this should be an Application Credential . Once you have a clouds.yaml , run the following to deploy the Azimuth demo environment: # Set OpenStack configuration variables export OS_CLOUD = openstack export OS_CLIENT_CONFIG_FILE = /path/to/clouds.yaml # Clone the azimuth-config repository git clone https://github.com/stackhpc/azimuth-config cd azimuth-config # Set up the virtual environment ./bin/ensure-venv # Activate the demo environment source ./bin/activate demo # Install Ansible dependencies ansible-galaxy install -f -r requirements.yml # Deploy Azimuth ansible-playbook stackhpc.azimuth_ops.provision The URL for the Azimuth UI is printed at the end of the playbook run. The credentials you use to authenticate with Azimuth are the same as you would use with the underlying OpenStack cloud. Warning Azimuth is deployed using Ansible, which does not support Windows as a controller . Azimuth deployment has been tested on Linux and macOS. Limitations The demo deployment has a number of limitations in order to give it the best chance of running on any given cloud: It uses the single node deployment method . Community images are uploaded as private images, so Azimuth will only be able to provision Kubernetes clusters and Cluster-as-a-Service appliances in the same project as it is deployed in. sslip.io is used to provide DNS. This avoids the need for a DNS entry to be provisioned in advance. TLS is disabled for ingress , allowing the Azimuth to work even when the deployment is not reachable from the internet ( outbound internet connectivity is still required). Verification of SSL certificates for the OpenStack API is disabled, allowing Azimuth to work even when the target cloud uses a custom CA. The deployment secrets are not secret , as they are stored in plain text in the azimuth-config repository on GitHub.","title":"Try Azimuth"},{"location":"try/#try-azimuth","text":"If you have access to a project on an OpenStack cloud, you can try Azimuth! The azimuth-config repository contains a special environment called demo that will provision a short-lived Azimuth deployment for demonstration purposes only . This environment attempts to infer all required configuration from the target OpenStack cloud - if this process is unsuccessful, an error will be produced. Danger Inferring configuration in the way that the demo environment does is not recommended as it is not guaranteed to produce the same result each time. For production deployments it is better to be explicit. To get started with a production deployment, see the best practice guide .","title":"Try Azimuth"},{"location":"try/#deploying-a-demo-instance","text":"The Azimuth deployment requires a clouds.yaml to run. Ideally, this should be an Application Credential . Once you have a clouds.yaml , run the following to deploy the Azimuth demo environment: # Set OpenStack configuration variables export OS_CLOUD = openstack export OS_CLIENT_CONFIG_FILE = /path/to/clouds.yaml # Clone the azimuth-config repository git clone https://github.com/stackhpc/azimuth-config cd azimuth-config # Set up the virtual environment ./bin/ensure-venv # Activate the demo environment source ./bin/activate demo # Install Ansible dependencies ansible-galaxy install -f -r requirements.yml # Deploy Azimuth ansible-playbook stackhpc.azimuth_ops.provision The URL for the Azimuth UI is printed at the end of the playbook run. The credentials you use to authenticate with Azimuth are the same as you would use with the underlying OpenStack cloud. Warning Azimuth is deployed using Ansible, which does not support Windows as a controller . Azimuth deployment has been tested on Linux and macOS.","title":"Deploying a demo instance"},{"location":"try/#limitations","text":"The demo deployment has a number of limitations in order to give it the best chance of running on any given cloud: It uses the single node deployment method . Community images are uploaded as private images, so Azimuth will only be able to provision Kubernetes clusters and Cluster-as-a-Service appliances in the same project as it is deployed in. sslip.io is used to provide DNS. This avoids the need for a DNS entry to be provisioned in advance. TLS is disabled for ingress , allowing the Azimuth to work even when the deployment is not reachable from the internet ( outbound internet connectivity is still required). Verification of SSL certificates for the OpenStack API is disabled, allowing Azimuth to work even when the target cloud uses a custom CA. The deployment secrets are not secret , as they are stored in plain text in the azimuth-config repository on GitHub.","title":"Limitations"},{"location":"configuration/","text":"Customising the Azimuth configuration The roles in the azimuth-ops collection support a vast array of variables to customise an Azimuth deployment. However azimuth-ops endeavours to pick sensible defaults where appropriate, so this documentation focuses on configuration that is required or commonly changed. For more advanced cases, the role defaults files are extensively documented and can be consulted directly. Note Make sure you are familiar with the Azimuth and Zenith architectures before continuing. It is assumed that you have already followed the steps in Setting up a configuration repository , and so have an environment for your site that is ready to be configured.","title":"Customising the Azimuth configuration"},{"location":"configuration/#customising-the-azimuth-configuration","text":"The roles in the azimuth-ops collection support a vast array of variables to customise an Azimuth deployment. However azimuth-ops endeavours to pick sensible defaults where appropriate, so this documentation focuses on configuration that is required or commonly changed. For more advanced cases, the role defaults files are extensively documented and can be consulted directly. Note Make sure you are familiar with the Azimuth and Zenith architectures before continuing. It is assumed that you have already followed the steps in Setting up a configuration repository , and so have an environment for your site that is ready to be configured.","title":"Customising the Azimuth configuration"},{"location":"configuration/01-prerequisites/","text":"Prerequisites In order to deploy Azimuth, a small number of prerequisites must be fulfilled. OpenStack cloud Currently, Azimuth is able to target OpenStack clouds with the following services enabled: Identity (Keystone) Image (Glance) Compute (Nova) Block storage (Cinder) Network (Neutron) OVN and OVS/ML2 drivers supported Load balancer (Octavia) Amphora and OVN drivers supported Azimuth has no specific requirement on the version of OpenStack. It is known to work on the OpenStack Train release and newer. Networking OpenStack projects that are used for Azimuth deployments (infra projects), or into which Azimuth will deploy platforms on behalf of users (workload projects), require access to a network that is shared as external in Neutron, on which floating IPs can be allocated. This network must provide egress to the internet, but does not need to provide ingress from the internet. This network should be given the Neutron tag portal-external so that it can be correctly detected by Azimuth, especially in the case where there are multiple external networks. Tip Adding the Neutron tag can be done using the OpenStack CLI (usually as admin): openstack network set --tag portal-external ${ external_network_name } Machines provisioned as part of an Azimuth deployment, or as part of platforms in workload projects, will be attached to a private network that is connected to this external network using a router. These machines must be able to access the OpenStack API for the cloud in which they are deployed. Cinder volumes and Kubernetes etcd , the distributed key-value store used by Kubernetes , is extremely sensitive to slow disks , requiring at least 50 sequential write IOPs. The recommended OpenStack configuration is to use local disk on the hypervisor for ephemeral root disks if possible. With this configuration, etcd just about works with spinning disk. If Cinder volumes are used for the root disks of control plane nodes in a Kubernetes cluster, they must be from an SSD-backed pool. Danger Network-attached spinning disks will not be fast enough for etcd, resulting in performance and stability issues for Kubernetes clusters. In fact, even network-attached SSDs are not ideal as network instability can cause spikes in the latency, which etcd does not like. Tip If you do not have much SSD capacity, it is possible to configure Kubernetes nodes so that etcd is on a separate block device, using a different volume type. See etcd configuration for details. OpenStack project quotas A standard high-availability (HA) deployment with a seed node, 3 control plane nodes and 3 worker nodes, requires the following resources: 1 x network, 1 x subnet, 1 x router 1 x seed node (4 vCPU, 8 GB) 4 x control plane nodes (4 vCPU, 8 GB) 3 x during normal operation, 4 x during rolling upgrade 4 x worker nodes (8 vCPU, 16 GB) 3 x during normal operation, 4 x during rolling upgrade 3 x load-balancers 500GB Cinder storage 3 x floating IPs One for accessing the seed node One fo the ingress controller for accessing HTTP services One for the Zenith SSHD server Tip It is recommended to have a project for each concrete environment that is being deployed, particularly for high-availability (HA) deployments. Application Credential You should create an Application Credential for the project and save the resulting clouds.yaml as ./environments/<name>/clouds.yaml . Warning Each concrete environment should have a separate application credential. Wildcard DNS Zenith exposes HTTP services using random subdomains under a parent domain. This means that Azimuth and Zenith require control of a entire subdomain, e.g. *.azimuth.example.org . The Azimuth UI will be exposed as portal.azimuth.example.org , with Zenith services exposed as <random subdomain>.azimuth.example.org . Azimuth leverages Kubernetes Ingress as a dynamically configurable HTTP proxy to route traffic from these domains to the correct services. ingress-nginx is deployed as the ingress controller and is exposed using a LoadBalancer service in Kubernetes. This results in an Octavia load-balancer being deployed with a floating IP attached that routes traffic to the ingress controller. In order for traffic to be routed correctly for these domains, a wildcard DNS record must exist for *.azimuth.example.org that points at the floating IP of the load-balancer for the ingress controller. Azimuth does not manage this DNS record. Transport Layer Security (TLS) In order to provide secure connections to users, Azimuth needs to be able to obtain a TLS certificate and private key for any of the subdomains under its wildcard domain. This can be achieved in two ways: Using a pre-existing wildcard TLS certificate for all subdomains Using an ACME server (e.g. Let's Encrypt) to issue certificates dynamically These approaches are discussed in more detail in the Ingress section .","title":"Prerequisites"},{"location":"configuration/01-prerequisites/#prerequisites","text":"In order to deploy Azimuth, a small number of prerequisites must be fulfilled.","title":"Prerequisites"},{"location":"configuration/01-prerequisites/#openstack-cloud","text":"Currently, Azimuth is able to target OpenStack clouds with the following services enabled: Identity (Keystone) Image (Glance) Compute (Nova) Block storage (Cinder) Network (Neutron) OVN and OVS/ML2 drivers supported Load balancer (Octavia) Amphora and OVN drivers supported Azimuth has no specific requirement on the version of OpenStack. It is known to work on the OpenStack Train release and newer.","title":"OpenStack cloud"},{"location":"configuration/01-prerequisites/#networking","text":"OpenStack projects that are used for Azimuth deployments (infra projects), or into which Azimuth will deploy platforms on behalf of users (workload projects), require access to a network that is shared as external in Neutron, on which floating IPs can be allocated. This network must provide egress to the internet, but does not need to provide ingress from the internet. This network should be given the Neutron tag portal-external so that it can be correctly detected by Azimuth, especially in the case where there are multiple external networks. Tip Adding the Neutron tag can be done using the OpenStack CLI (usually as admin): openstack network set --tag portal-external ${ external_network_name } Machines provisioned as part of an Azimuth deployment, or as part of platforms in workload projects, will be attached to a private network that is connected to this external network using a router. These machines must be able to access the OpenStack API for the cloud in which they are deployed.","title":"Networking"},{"location":"configuration/01-prerequisites/#cinder-volumes-and-kubernetes","text":"etcd , the distributed key-value store used by Kubernetes , is extremely sensitive to slow disks , requiring at least 50 sequential write IOPs. The recommended OpenStack configuration is to use local disk on the hypervisor for ephemeral root disks if possible. With this configuration, etcd just about works with spinning disk. If Cinder volumes are used for the root disks of control plane nodes in a Kubernetes cluster, they must be from an SSD-backed pool. Danger Network-attached spinning disks will not be fast enough for etcd, resulting in performance and stability issues for Kubernetes clusters. In fact, even network-attached SSDs are not ideal as network instability can cause spikes in the latency, which etcd does not like. Tip If you do not have much SSD capacity, it is possible to configure Kubernetes nodes so that etcd is on a separate block device, using a different volume type. See etcd configuration for details.","title":"Cinder volumes and Kubernetes"},{"location":"configuration/01-prerequisites/#openstack-project-quotas","text":"A standard high-availability (HA) deployment with a seed node, 3 control plane nodes and 3 worker nodes, requires the following resources: 1 x network, 1 x subnet, 1 x router 1 x seed node (4 vCPU, 8 GB) 4 x control plane nodes (4 vCPU, 8 GB) 3 x during normal operation, 4 x during rolling upgrade 4 x worker nodes (8 vCPU, 16 GB) 3 x during normal operation, 4 x during rolling upgrade 3 x load-balancers 500GB Cinder storage 3 x floating IPs One for accessing the seed node One fo the ingress controller for accessing HTTP services One for the Zenith SSHD server Tip It is recommended to have a project for each concrete environment that is being deployed, particularly for high-availability (HA) deployments.","title":"OpenStack project quotas"},{"location":"configuration/01-prerequisites/#application-credential","text":"You should create an Application Credential for the project and save the resulting clouds.yaml as ./environments/<name>/clouds.yaml . Warning Each concrete environment should have a separate application credential.","title":"Application Credential"},{"location":"configuration/01-prerequisites/#wildcard-dns","text":"Zenith exposes HTTP services using random subdomains under a parent domain. This means that Azimuth and Zenith require control of a entire subdomain, e.g. *.azimuth.example.org . The Azimuth UI will be exposed as portal.azimuth.example.org , with Zenith services exposed as <random subdomain>.azimuth.example.org . Azimuth leverages Kubernetes Ingress as a dynamically configurable HTTP proxy to route traffic from these domains to the correct services. ingress-nginx is deployed as the ingress controller and is exposed using a LoadBalancer service in Kubernetes. This results in an Octavia load-balancer being deployed with a floating IP attached that routes traffic to the ingress controller. In order for traffic to be routed correctly for these domains, a wildcard DNS record must exist for *.azimuth.example.org that points at the floating IP of the load-balancer for the ingress controller. Azimuth does not manage this DNS record.","title":"Wildcard DNS"},{"location":"configuration/01-prerequisites/#transport-layer-security-tls","text":"In order to provide secure connections to users, Azimuth needs to be able to obtain a TLS certificate and private key for any of the subdomains under its wildcard domain. This can be achieved in two ways: Using a pre-existing wildcard TLS certificate for all subdomains Using an ACME server (e.g. Let's Encrypt) to issue certificates dynamically These approaches are discussed in more detail in the Ingress section .","title":"Transport Layer Security (TLS)"},{"location":"configuration/02-deployment-method/","text":"Deployment method azimuth-ops supports two deployment methods - singlenode and ha . Networking automation azimuth-ops will create an internal network, onto which all nodes for the deployment will be placed. It will also create a router connecting the internal network to the external network where floating IPs are allocated. Single node In this deployment method, a single node is provisioned with OpenTofu and configured as a K3s cluster. The full Azimuth stack is then deployed onto this cluster. Warning This deployment method is only suitable for development or demonstration. To use the single node deployment method, use the singlenode environment in your ansible.cfg : ansible.cfg [defaults] inventory = ../base/inventory,../singlenode/inventory,./inventory The following variables must be set to define the properties of the K3s node: environments/my-site/inventory/group_vars/all/variables.yml # The ID of the external network infra_external_network_id : \"<network id>\" # The floating IP to which to wildcard DNS entry has been assigned infra_fixed_floatingip : \"<pre-allocated floating ip>\" # The ID of the flavor to use for the K3s node # A flavor with at least 4 CPUs and 16GB RAM is recommended infra_flavor_id : \"<flavor id>\" # The size of the volume to use for K3s cluster data infra_data_volume_size : 100 Highly-available (HA) For the HA deployment method, OpenTofu is also used to provision a single node that is configured as a K3s cluster. However rather than hosting the Azimuth components, as in the single node case, this K3s cluster is only configured as a Cluster API management cluster . Cluster API on the K3s cluster is then used to manage a HA cluster, in the same project and on the same network. The Azimuth stack is then deployed onto this cluster. To use the HA deployment method, use the ha environment in your ansible.cfg : ansible.cfg [defaults] inventory = ../base/inventory,../ha/inventory,./inventory The following variables must be set to define the properties of the K3s node and the Cluster API managed nodes: environments/my-site/inventory/group_vars/all/variables.yml # The ID of the external network infra_external_network_id : \"<network id>\" # The ID of the flavor to use for the K3s node # A flavor with at least 2 CPUs and 8GB RAM is recommended infra_flavor_id : \"<flavor id>\" # The name of the flavor to use for control plane nodes # A flavor with at least 2 CPUs, 8GB RAM and 100GB root disk is recommended capi_cluster_control_plane_flavor : \"<flavor name>\" # The name of the flavor to use for worker nodes # A flavor with at least 4 CPUs, 16GB RAM and 100GB root disk is recommended capi_cluster_worker_flavor : \"<flavor name>\" # The number of worker nodes capi_cluster_worker_count : 3 # The pre-allocated floating IP to which to wildcard DNS entry has been assigned capi_cluster_addons_ingress_load_balancer_ip : \"<pre-allocated floating ip>\" # The pre-allocated floating IP for the Zenith SSHD server zenith_sshd_service_load_balancer_ip : \"<pre-allocated floating ip>\"","title":"Deployment method"},{"location":"configuration/02-deployment-method/#deployment-method","text":"azimuth-ops supports two deployment methods - singlenode and ha . Networking automation azimuth-ops will create an internal network, onto which all nodes for the deployment will be placed. It will also create a router connecting the internal network to the external network where floating IPs are allocated.","title":"Deployment method"},{"location":"configuration/02-deployment-method/#single-node","text":"In this deployment method, a single node is provisioned with OpenTofu and configured as a K3s cluster. The full Azimuth stack is then deployed onto this cluster. Warning This deployment method is only suitable for development or demonstration. To use the single node deployment method, use the singlenode environment in your ansible.cfg : ansible.cfg [defaults] inventory = ../base/inventory,../singlenode/inventory,./inventory The following variables must be set to define the properties of the K3s node: environments/my-site/inventory/group_vars/all/variables.yml # The ID of the external network infra_external_network_id : \"<network id>\" # The floating IP to which to wildcard DNS entry has been assigned infra_fixed_floatingip : \"<pre-allocated floating ip>\" # The ID of the flavor to use for the K3s node # A flavor with at least 4 CPUs and 16GB RAM is recommended infra_flavor_id : \"<flavor id>\" # The size of the volume to use for K3s cluster data infra_data_volume_size : 100","title":"Single node"},{"location":"configuration/02-deployment-method/#highly-available-ha","text":"For the HA deployment method, OpenTofu is also used to provision a single node that is configured as a K3s cluster. However rather than hosting the Azimuth components, as in the single node case, this K3s cluster is only configured as a Cluster API management cluster . Cluster API on the K3s cluster is then used to manage a HA cluster, in the same project and on the same network. The Azimuth stack is then deployed onto this cluster. To use the HA deployment method, use the ha environment in your ansible.cfg : ansible.cfg [defaults] inventory = ../base/inventory,../ha/inventory,./inventory The following variables must be set to define the properties of the K3s node and the Cluster API managed nodes: environments/my-site/inventory/group_vars/all/variables.yml # The ID of the external network infra_external_network_id : \"<network id>\" # The ID of the flavor to use for the K3s node # A flavor with at least 2 CPUs and 8GB RAM is recommended infra_flavor_id : \"<flavor id>\" # The name of the flavor to use for control plane nodes # A flavor with at least 2 CPUs, 8GB RAM and 100GB root disk is recommended capi_cluster_control_plane_flavor : \"<flavor name>\" # The name of the flavor to use for worker nodes # A flavor with at least 4 CPUs, 16GB RAM and 100GB root disk is recommended capi_cluster_worker_flavor : \"<flavor name>\" # The number of worker nodes capi_cluster_worker_count : 3 # The pre-allocated floating IP to which to wildcard DNS entry has been assigned capi_cluster_addons_ingress_load_balancer_ip : \"<pre-allocated floating ip>\" # The pre-allocated floating IP for the Zenith SSHD server zenith_sshd_service_load_balancer_ip : \"<pre-allocated floating ip>\"","title":"Highly-available (HA)"},{"location":"configuration/03-kubernetes-config/","text":"Kubernetes configuration The concepts in this section apply to any Kubernetes clusters created using Cluster API, i.e. the HA cluster in a HA deployment and tenant clusters. The variable names differ slightly for the two cases. Images When building a cluster, Cluster API requires that an image exists in the target cloud that is accessible to the target project and has the correct version of kubelet and kubeadm available. Suitable images are uploaded as part of an Azimuth deployment using the Community images functionality and the IDs are automatically propagated where they are needed, i.e. for the Azimuth HA cluster and the Kubernetes cluster templates . If required, e.g. for custom Kubernetes templates , the IDs of these images can be referenced using the community_images_image_ids variable: environments/my-site/inventory/group_vars/all/variables.yml kube_1_25_image_id : \"{{ community_images_image_ids.kube_1_25 }}\" kube_1_26_image_id : \"{{ community_images_image_ids.kube_1_26 }}\" kube_1_27_image_id : \"{{ community_images_image_ids.kube_1_27 }}\" Docker Hub mirror Docker Hub imposes rate limits on image downloads, which can cause issues for both the Azimuth HA cluster and, in particular, tenant clusters. This can be worked around by mirroring the images to a local registry. If you have a Docker Hub mirror available, this can be configured using the following variables: #### For the HA cluster #### # The ID of the external network to use capi_cluster_registry_mirrors : docker.io : [ \"https://docker-hub-mirror.example.org/v2\" ] #### For tenant clusters #### azimuth_capi_operator_capi_helm_registry_mirrors : docker.io : [ \"https://docker-hub-mirror.example.org/v2\" ] Tip If you do not have a Docker Hub mirror available, one can be deployed as part of an Azimuth deployment . This mirror will be automatically configured for tenant Kubernetes clusters. This mirror cannot be used for the Azimuth HA cluster as it is deployed on that cluster. Multiple external networks In the case where multiple external networks are available, you must tell Azimuth which one to use: environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # The ID of the external network to use capi_cluster_external_network_id : \"<network id>\" #### For tenant clusters #### azimuth_capi_operator_external_network_id : \"<network id>\" Note This does not currently respect the portal-external tag. Volume-backed instances Flavors with 100GB root disks are recommended for Kubernetes nodes, both for the Azimuth deployment and for tenant clusters. If flavors with large root disks are not available on the target cloud, it is possible to use volume-backed instances instead. etcd and spinning disks The configuration options in this section should be used subject to the advice in the prerequisites about using Cinder volumes with Kubernetes. etcd on a separate block device If you only have a limited amount of SSD or, even better, local disk, available, consider placing etcd on a separate block device to make best use of the limited capacity. To configure Kubernetes clusters to use volume-backed instances (i.e. use a Cinder volume as the root disk), the following variables can be used: environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # The size of the root volumes for Kubernetes nodes capi_cluster_root_volume_size : 100 # The volume type to use for root volumes for Kubernetes nodes capi_cluster_root_volume_type : nvme #### For tenant clusters #### azimuth_capi_operator_capi_helm_root_volume_size : 100 azimuth_capi_operator_capi_helm_root_volume_type : nvme Tip You can see the available volume types using the OpenStack CLI: openstack volume type list Etcd configuration As discussed in the prerequisites , etcd is extremely sensitive to write latency. Azimuth is able to configure Kubernetes nodes, both for the HA cluster and tenant clusters, so that etcd is on a separate block device. This block device can be of a different volume type to the root disk, allowing efficient use of SSD-backed storage. When supported by the flavor, the etcd block device can also use local disk even if the root volume is from Cinder. Use local disk for etcd whenever possible Using local disk when possible minises the write latency for etcd and also eliminates network instability as a cause of latency problems. The following variables are used to configure the etcd block device for the HA cluster: environments/my-site/inventory/group_vars/all/variables.yml # Specifies the size of the etcd block device in GB # This is typically between 2GB and 10GB - Amazon recommends 8GB for EKS # Defaults to 0, meaning etcd stays on the root device capi_cluster_etcd_blockdevice_size : 8 # The type of block device that will be used for etcd # Specify \"Volume\" (the default) to use a Cinder volume # Specify \"Local\" to use local disk (the flavor must support ephemeral disk) capi_cluster_etcd_blockdevice_type : Volume # The Cinder volume type to use for the etcd block device # Only used if \"Volume\" is specified as block device type # If not given, the default volume type for the cloud will be used capi_cluster_etcd_blockdevice_volume_type : nvme # The Cinder availability zone to use for the etcd block device # Only used if \"Volume\" is specified as block device type # Defaults to \"nova\" capi_cluster_etcd_blockdevice_volume_az : nova The equivalent variables for tenant clusters are: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_capi_helm_etcd_blockdevice_size : azimuth_capi_operator_capi_helm_etcd_blockdevice_type : azimuth_capi_operator_capi_helm_etcd_blockdevice_volume_type : azimuth_capi_operator_capi_helm_etcd_blockdevice_volume_az : Load-balancer provider If the target cloud uses OVN networking , and the OVN Octavia provider is enabled, then Kubernetes clusters should be configured to use the OVN provider for any load-balancers that are created: environments/my-site/inventory/group_vars/all/variables.yml openstack_loadbalancer_provider : ovn Tip You can see the available load-balancer providers using the OpenStack CLI: openstack loadbalancer provider list Availability zones By default, an Azimuth installation assumes that there is a single availability zone (AZ) called nova - this is the default set up and common for small-to-medium sized clouds. If this is not the case for your target cloud, you may need to set additional variables specifying the AZs to use, both for the HA cluster and for tenant Kubernetes clusters. The default behaviour when scheduling Kubernetes nodes using Cluster API is: All available AZs are considered for control plane nodes, and Cluster API will attempt to spread the nodes across multiple AZs, if available. Worker nodes are scheduled into the nova AZ explicitly. If this AZ does not exist, scheduling will fail. If this default behaviour does not work for your target cloud, the following options are available. Note Cluster API refers to \"failure domains\" which, in the OpenStack provider, correspond to availability zones (AZs). Ignore availability zones It is possible to configure Cluster API clusters in such a way that AZs are not specified at all for Kubernetes nodes. This allows other placement constraints such as flavor traits and host aggregates to be used, and a suitable AZ to be selected by OpenStack. environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # Indicate that the failure domain should be omitted for control plane nodes capi_cluster_control_plane_omit_failure_domain : true # Specify no failure domain for worker nodes capi_cluster_worker_failure_domain : null #### For tenant clusters #### azimuth_capi_operator_capi_helm_control_plane_omit_failure_domain : true azimuth_capi_operator_capi_helm_worker_failure_domain : null Tip This is the recommended configuration for new deployments, unless you have a specific need to use specific availability zones. Use specific availability zones To use specific availability zones for Kubernetes nodes, the following variables can be used: environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # A list of failure domains that should be considered for control plane nodes capi_cluster_control_plane_failure_domains : [ az1 , az2 ] # The failure domain for worker nodes capi_cluster_worker_failure_domain : az1 #### For tenant clusters #### azimuth_capi_operator_capi_helm_control_plane_failure_domains : [ az1 , az2 ] azimuth_capi_operator_capi_helm_worker_failure_domain : az1","title":"Kubernetes configuration"},{"location":"configuration/03-kubernetes-config/#kubernetes-configuration","text":"The concepts in this section apply to any Kubernetes clusters created using Cluster API, i.e. the HA cluster in a HA deployment and tenant clusters. The variable names differ slightly for the two cases.","title":"Kubernetes configuration"},{"location":"configuration/03-kubernetes-config/#images","text":"When building a cluster, Cluster API requires that an image exists in the target cloud that is accessible to the target project and has the correct version of kubelet and kubeadm available. Suitable images are uploaded as part of an Azimuth deployment using the Community images functionality and the IDs are automatically propagated where they are needed, i.e. for the Azimuth HA cluster and the Kubernetes cluster templates . If required, e.g. for custom Kubernetes templates , the IDs of these images can be referenced using the community_images_image_ids variable: environments/my-site/inventory/group_vars/all/variables.yml kube_1_25_image_id : \"{{ community_images_image_ids.kube_1_25 }}\" kube_1_26_image_id : \"{{ community_images_image_ids.kube_1_26 }}\" kube_1_27_image_id : \"{{ community_images_image_ids.kube_1_27 }}\"","title":"Images"},{"location":"configuration/03-kubernetes-config/#docker-hub-mirror","text":"Docker Hub imposes rate limits on image downloads, which can cause issues for both the Azimuth HA cluster and, in particular, tenant clusters. This can be worked around by mirroring the images to a local registry. If you have a Docker Hub mirror available, this can be configured using the following variables: #### For the HA cluster #### # The ID of the external network to use capi_cluster_registry_mirrors : docker.io : [ \"https://docker-hub-mirror.example.org/v2\" ] #### For tenant clusters #### azimuth_capi_operator_capi_helm_registry_mirrors : docker.io : [ \"https://docker-hub-mirror.example.org/v2\" ] Tip If you do not have a Docker Hub mirror available, one can be deployed as part of an Azimuth deployment . This mirror will be automatically configured for tenant Kubernetes clusters. This mirror cannot be used for the Azimuth HA cluster as it is deployed on that cluster.","title":"Docker Hub mirror"},{"location":"configuration/03-kubernetes-config/#multiple-external-networks","text":"In the case where multiple external networks are available, you must tell Azimuth which one to use: environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # The ID of the external network to use capi_cluster_external_network_id : \"<network id>\" #### For tenant clusters #### azimuth_capi_operator_external_network_id : \"<network id>\" Note This does not currently respect the portal-external tag.","title":"Multiple external networks"},{"location":"configuration/03-kubernetes-config/#volume-backed-instances","text":"Flavors with 100GB root disks are recommended for Kubernetes nodes, both for the Azimuth deployment and for tenant clusters. If flavors with large root disks are not available on the target cloud, it is possible to use volume-backed instances instead. etcd and spinning disks The configuration options in this section should be used subject to the advice in the prerequisites about using Cinder volumes with Kubernetes. etcd on a separate block device If you only have a limited amount of SSD or, even better, local disk, available, consider placing etcd on a separate block device to make best use of the limited capacity. To configure Kubernetes clusters to use volume-backed instances (i.e. use a Cinder volume as the root disk), the following variables can be used: environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # The size of the root volumes for Kubernetes nodes capi_cluster_root_volume_size : 100 # The volume type to use for root volumes for Kubernetes nodes capi_cluster_root_volume_type : nvme #### For tenant clusters #### azimuth_capi_operator_capi_helm_root_volume_size : 100 azimuth_capi_operator_capi_helm_root_volume_type : nvme Tip You can see the available volume types using the OpenStack CLI: openstack volume type list","title":"Volume-backed instances"},{"location":"configuration/03-kubernetes-config/#etcd-configuration","text":"As discussed in the prerequisites , etcd is extremely sensitive to write latency. Azimuth is able to configure Kubernetes nodes, both for the HA cluster and tenant clusters, so that etcd is on a separate block device. This block device can be of a different volume type to the root disk, allowing efficient use of SSD-backed storage. When supported by the flavor, the etcd block device can also use local disk even if the root volume is from Cinder. Use local disk for etcd whenever possible Using local disk when possible minises the write latency for etcd and also eliminates network instability as a cause of latency problems. The following variables are used to configure the etcd block device for the HA cluster: environments/my-site/inventory/group_vars/all/variables.yml # Specifies the size of the etcd block device in GB # This is typically between 2GB and 10GB - Amazon recommends 8GB for EKS # Defaults to 0, meaning etcd stays on the root device capi_cluster_etcd_blockdevice_size : 8 # The type of block device that will be used for etcd # Specify \"Volume\" (the default) to use a Cinder volume # Specify \"Local\" to use local disk (the flavor must support ephemeral disk) capi_cluster_etcd_blockdevice_type : Volume # The Cinder volume type to use for the etcd block device # Only used if \"Volume\" is specified as block device type # If not given, the default volume type for the cloud will be used capi_cluster_etcd_blockdevice_volume_type : nvme # The Cinder availability zone to use for the etcd block device # Only used if \"Volume\" is specified as block device type # Defaults to \"nova\" capi_cluster_etcd_blockdevice_volume_az : nova The equivalent variables for tenant clusters are: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_capi_helm_etcd_blockdevice_size : azimuth_capi_operator_capi_helm_etcd_blockdevice_type : azimuth_capi_operator_capi_helm_etcd_blockdevice_volume_type : azimuth_capi_operator_capi_helm_etcd_blockdevice_volume_az :","title":"Etcd configuration"},{"location":"configuration/03-kubernetes-config/#load-balancer-provider","text":"If the target cloud uses OVN networking , and the OVN Octavia provider is enabled, then Kubernetes clusters should be configured to use the OVN provider for any load-balancers that are created: environments/my-site/inventory/group_vars/all/variables.yml openstack_loadbalancer_provider : ovn Tip You can see the available load-balancer providers using the OpenStack CLI: openstack loadbalancer provider list","title":"Load-balancer provider"},{"location":"configuration/03-kubernetes-config/#availability-zones","text":"By default, an Azimuth installation assumes that there is a single availability zone (AZ) called nova - this is the default set up and common for small-to-medium sized clouds. If this is not the case for your target cloud, you may need to set additional variables specifying the AZs to use, both for the HA cluster and for tenant Kubernetes clusters. The default behaviour when scheduling Kubernetes nodes using Cluster API is: All available AZs are considered for control plane nodes, and Cluster API will attempt to spread the nodes across multiple AZs, if available. Worker nodes are scheduled into the nova AZ explicitly. If this AZ does not exist, scheduling will fail. If this default behaviour does not work for your target cloud, the following options are available. Note Cluster API refers to \"failure domains\" which, in the OpenStack provider, correspond to availability zones (AZs).","title":"Availability zones"},{"location":"configuration/03-kubernetes-config/#ignore-availability-zones","text":"It is possible to configure Cluster API clusters in such a way that AZs are not specified at all for Kubernetes nodes. This allows other placement constraints such as flavor traits and host aggregates to be used, and a suitable AZ to be selected by OpenStack. environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # Indicate that the failure domain should be omitted for control plane nodes capi_cluster_control_plane_omit_failure_domain : true # Specify no failure domain for worker nodes capi_cluster_worker_failure_domain : null #### For tenant clusters #### azimuth_capi_operator_capi_helm_control_plane_omit_failure_domain : true azimuth_capi_operator_capi_helm_worker_failure_domain : null Tip This is the recommended configuration for new deployments, unless you have a specific need to use specific availability zones.","title":"Ignore availability zones"},{"location":"configuration/03-kubernetes-config/#use-specific-availability-zones","text":"To use specific availability zones for Kubernetes nodes, the following variables can be used: environments/my-site/inventory/group_vars/all/variables.yml #### For the HA cluster #### # A list of failure domains that should be considered for control plane nodes capi_cluster_control_plane_failure_domains : [ az1 , az2 ] # The failure domain for worker nodes capi_cluster_worker_failure_domain : az1 #### For tenant clusters #### azimuth_capi_operator_capi_helm_control_plane_failure_domains : [ az1 , az2 ] azimuth_capi_operator_capi_helm_worker_failure_domain : az1","title":"Use specific availability zones"},{"location":"configuration/04-target-cloud/","text":"Target OpenStack cloud The main piece of site-specific configuration required by Azimuth is the connection information for the target OpenStack cloud. Azimuth uses the Keystone Service Catalog to discover the endpoints for OpenStack services, so only needs to be told where to find the Keystone v3 endpoint. By default, the auth URL from the application credential used to deploy Azimuth will be used. If you want Azimuth to target a different OpenStack cloud than the one it is deployed in, this can be overridden: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_auth_url : https://openstack.example-cloud.org:5000/v3 Warning Make sure to include the trailing /v3 , otherwise authentication will fail. Azimuth does not currently have support for specifying a custom CA for verifying TLS. If the target cloud uses a TLS certificate that is not verifiable using the operating-system default trustroots, TLS verification must be disabled: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_verify_ssl : false If you are using the password authenticator and use a domain other than default , you will also need to tell Azimuth the name of the domain to use when authenticating: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_domain : my-domain Cloud name Azimuth presents the name of the current cloud in various places in the interface. To configure this, set the following variables: environments/my-site/inventory/group_vars/all/variables.yml # The machine-readable cloud name azimuth_current_cloud_name : my-cloud # The human-readable name for the cloud azimuth_current_cloud_label : My Cloud Federated authentication By default the password authenticator is enabled, which accepts a username and password and swaps them for an OpenStack token. This requires no additional configuration. If the target cloud consumes identities from an external provider via Keystone federation , then Azimuth should be configured to obtain an OpenStack token from Keystone using the same flow as Horizon. To enable this, additional configuration is required for both Azimuth and Keystone on the target cloud. First, the Keystone configuration of the target cloud must be modified to add Azimuth as a trusted dashboard , otherwise it will be unable to retrieve a token via the federated flow. When configuring Azimuth as a trusted dashboard, you must specify the URL that will receive token data, where the portal domain depends on the ingress configuration : Keystone configuration [federation] trusted_dashboard = https://portal.azimuth.example.org/auth/federated/complete/ In your Azimuth configuration, enable the federated authenticator and tell it the provider and protocol to use: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_federated_enabled : yes azimuth_authenticator_federated_provider : \"<provider>\" azimuth_authenticator_federated_protocol : \"<protocol>\" This will result in Azimuth using URLs of the following form for the federated authentication flow: <auth url>/auth/OS-FEDERATION/identity_providers/<provider>/protocols/<protocol>/websso The provider and protocol will depend on the Keystone configuration of the target OpenStack cloud. To also disable the password authenticator - so that federation is the only supported login - set the following variable: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_password_enabled : no To change the human-readable names for the authenticators, which are presented in the authentication method selection form, use the following variables: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_password_label : \"Username + Password\" azimuth_authenticator_federated_label : \"Federated\" Networking configuration Azimuth uses Neutron resource tags to discover the networks it should use, and the tags it looks for are portal-internal and portal-external for the internal and external networks respectively. These tags must be applied by the cloud operator. If it cannot find a tagged internal network, the default behaviour is for Azimuth to create an internal network to use (and the corresponding router to attach it to the external network). The discovery and auto-creation process is described in detail in Network discovery and auto-creation . To disable the auto-creation of internal networks, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_create_internal_net : false The CIDR of the auto-created subnet can also be changed, although it is the same for every project. For example, you may need to do this if the default CIDR conflicts with resources elsewhere on your network that machines provisioned by Azimuth need to access: environments/my-site/inventory/group_vars/all/variables.yml # Defaults to 192.168.3.0/24 azimuth_openstack_internal_net_cidr : 10.0.3.0/24 Monitoring Cloud Capacity Azimuth is able to federate cloud metrics from a Prometheus running within your OpenStack cloud enviroment, such as the one deployed by stackhpc-kayobe-config . We also assume the os-capacity exporter is being used to query the current capacity of your cloud, mostly using data from OpenStack placement. First you need to enable the project metrics and cloud metrics links within Azimuth by configuring: environments/my-site/inventory/group_vars/all/variables.yml # Defaults to no cloud_metrics_enabled : yes You then need to tell Azimuth how to access the OpenStack cloud Prometheus: environments/my-site/inventory/group_vars/all/variables.yml # hostname needed to match TLS certificate name cloud_metrics_prometheus_host : \"mycloud.example.com\" # ip that matches the above hostname cloud_metrics_prometheus_ip : \"<ip prometheus vip>\" cloud_metrics_prometheus_port : 9091 cloud_metrics_prometheus_basic_auth_username : \"<basic-auth-username>\" cloud_metrics_prometheus_basic_auth_password : \"<basic-auth-password>\"","title":"Target OpenStack cloud"},{"location":"configuration/04-target-cloud/#target-openstack-cloud","text":"The main piece of site-specific configuration required by Azimuth is the connection information for the target OpenStack cloud. Azimuth uses the Keystone Service Catalog to discover the endpoints for OpenStack services, so only needs to be told where to find the Keystone v3 endpoint. By default, the auth URL from the application credential used to deploy Azimuth will be used. If you want Azimuth to target a different OpenStack cloud than the one it is deployed in, this can be overridden: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_auth_url : https://openstack.example-cloud.org:5000/v3 Warning Make sure to include the trailing /v3 , otherwise authentication will fail. Azimuth does not currently have support for specifying a custom CA for verifying TLS. If the target cloud uses a TLS certificate that is not verifiable using the operating-system default trustroots, TLS verification must be disabled: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_verify_ssl : false If you are using the password authenticator and use a domain other than default , you will also need to tell Azimuth the name of the domain to use when authenticating: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_domain : my-domain","title":"Target OpenStack cloud"},{"location":"configuration/04-target-cloud/#cloud-name","text":"Azimuth presents the name of the current cloud in various places in the interface. To configure this, set the following variables: environments/my-site/inventory/group_vars/all/variables.yml # The machine-readable cloud name azimuth_current_cloud_name : my-cloud # The human-readable name for the cloud azimuth_current_cloud_label : My Cloud","title":"Cloud name"},{"location":"configuration/04-target-cloud/#federated-authentication","text":"By default the password authenticator is enabled, which accepts a username and password and swaps them for an OpenStack token. This requires no additional configuration. If the target cloud consumes identities from an external provider via Keystone federation , then Azimuth should be configured to obtain an OpenStack token from Keystone using the same flow as Horizon. To enable this, additional configuration is required for both Azimuth and Keystone on the target cloud. First, the Keystone configuration of the target cloud must be modified to add Azimuth as a trusted dashboard , otherwise it will be unable to retrieve a token via the federated flow. When configuring Azimuth as a trusted dashboard, you must specify the URL that will receive token data, where the portal domain depends on the ingress configuration : Keystone configuration [federation] trusted_dashboard = https://portal.azimuth.example.org/auth/federated/complete/ In your Azimuth configuration, enable the federated authenticator and tell it the provider and protocol to use: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_federated_enabled : yes azimuth_authenticator_federated_provider : \"<provider>\" azimuth_authenticator_federated_protocol : \"<protocol>\" This will result in Azimuth using URLs of the following form for the federated authentication flow: <auth url>/auth/OS-FEDERATION/identity_providers/<provider>/protocols/<protocol>/websso The provider and protocol will depend on the Keystone configuration of the target OpenStack cloud. To also disable the password authenticator - so that federation is the only supported login - set the following variable: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_password_enabled : no To change the human-readable names for the authenticators, which are presented in the authentication method selection form, use the following variables: environments/my-site/inventory/group_vars/all/variables.yml azimuth_authenticator_password_label : \"Username + Password\" azimuth_authenticator_federated_label : \"Federated\"","title":"Federated authentication"},{"location":"configuration/04-target-cloud/#networking-configuration","text":"Azimuth uses Neutron resource tags to discover the networks it should use, and the tags it looks for are portal-internal and portal-external for the internal and external networks respectively. These tags must be applied by the cloud operator. If it cannot find a tagged internal network, the default behaviour is for Azimuth to create an internal network to use (and the corresponding router to attach it to the external network). The discovery and auto-creation process is described in detail in Network discovery and auto-creation . To disable the auto-creation of internal networks, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_openstack_create_internal_net : false The CIDR of the auto-created subnet can also be changed, although it is the same for every project. For example, you may need to do this if the default CIDR conflicts with resources elsewhere on your network that machines provisioned by Azimuth need to access: environments/my-site/inventory/group_vars/all/variables.yml # Defaults to 192.168.3.0/24 azimuth_openstack_internal_net_cidr : 10.0.3.0/24","title":"Networking configuration"},{"location":"configuration/04-target-cloud/#monitoring-cloud-capacity","text":"Azimuth is able to federate cloud metrics from a Prometheus running within your OpenStack cloud enviroment, such as the one deployed by stackhpc-kayobe-config . We also assume the os-capacity exporter is being used to query the current capacity of your cloud, mostly using data from OpenStack placement. First you need to enable the project metrics and cloud metrics links within Azimuth by configuring: environments/my-site/inventory/group_vars/all/variables.yml # Defaults to no cloud_metrics_enabled : yes You then need to tell Azimuth how to access the OpenStack cloud Prometheus: environments/my-site/inventory/group_vars/all/variables.yml # hostname needed to match TLS certificate name cloud_metrics_prometheus_host : \"mycloud.example.com\" # ip that matches the above hostname cloud_metrics_prometheus_ip : \"<ip prometheus vip>\" cloud_metrics_prometheus_port : 9091 cloud_metrics_prometheus_basic_auth_username : \"<basic-auth-username>\" cloud_metrics_prometheus_basic_auth_password : \"<basic-auth-password>\"","title":"Monitoring Cloud Capacity"},{"location":"configuration/05-secret-key/","text":"Secret key Azimuth requires a secret key that is used primarily for signing cookies: environments/my-site/inventory/group_vars/all/secrets.yml azimuth_secret_key : \"<some secret key>\" Tip This key should be a long, random string - at least 32 bytes (256 bits) is recommended. A suitable key can be generated using openssl rand -hex 32 . Danger This key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Secret key"},{"location":"configuration/05-secret-key/#secret-key","text":"Azimuth requires a secret key that is used primarily for signing cookies: environments/my-site/inventory/group_vars/all/secrets.yml azimuth_secret_key : \"<some secret key>\" Tip This key should be a long, random string - at least 32 bytes (256 bits) is recommended. A suitable key can be generated using openssl rand -hex 32 . Danger This key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Secret key"},{"location":"configuration/06-ingress/","text":"Ingress As mentioned in the prerequisites, Azimuth and Zenith expect to be given control of an entire subdomain, e.g. *.azimuth.example.org , and this domain must be assigned to a pre-allocated floating IP using a wildcard DNS entry. To tell azimuth-ops what domain it should use, simply set the following variable: environments/my-site/inventory/group_vars/all/variables.yml ingress_base_domain : azimuth.example.org This will result in azimuth-ops using portal.azimuth.example.org for the Azimuth portal interface, and Zenith will use domains of the form <random subdomain>.azimuth.example.org for user-facing services. Other services deployed by Azimuth, such as Harbor and the monitoring and alerting dashboards will also be allocated subdomains under this domain. Transport Layer Security (TLS) TLS for Azimuth can be configured in two ways: Pre-existing wildcard certificate If you have a pre-existing wildcard TLS certificate issued for the ingress base domain, you can use this to provide TLS for Azimuth and Zenith services. Tip This is the recommended mechanism for production deployments, despite the lack of automation, for two reasons: It is not affected by rate limits Zenith services become available faster as it avoids the overhead of obtaining a certificate per service Warning It is your responsibility to renew the wildcard certificate before it expires. The Azimuth monitoring will produce alerts when the certificate is approaching its expiry date. To configure a pre-existing wildcard certificate for ingress, just create the following files in your environment: environments/my-site/tls/tls.crt Must contain the full certificate chain , with the most specific certificate at the top (e.g. the wildcard certificate), any intermediate certificates and the root CA at the bottom. environments/my-site/tls/tls.key The corresponding private key for the wildcard certificate. Danger The TLS key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted . When these files exist in an environment, azimuth-ops will automatically pick them up and use them. Info In the future, support for a wildcard certificate managed by cert-manager may be implemented. However this will require the use of a supported DNS provider in order to fulfil the DNS01 challenge (wildcard certificates cannot be issued for an HTTP01 challenge). Automated with cert-manager azimuth-ops is able to configure Ingress resources for Azimuth and Zenith services so that their TLS certificates are managed automatically using cert-manager . In this configuration, a certificate is issued for each separate subdomain by an ACME provider using the HTTP-01 challenge type . By default, cert-manager is enabled and this mechanism will be used to issue TLS certificates for Azimuth and Zenith services with no further configuration. The default ACME service is Let's Encrypt , which issues certificates that are trusted by all major operating systems and browsers. Let's Encrypt rate limits Let's Encrypt imposes rate limits to ensure fair usage. At the time of writing, the number of new certificates that can be issued is 50 per week per registed domain . The \"registered domain\" is the part of the domain that is purchased from the registrar so, for an Azimuth deployment with an ingress base domain of *.azimuth.example.org , the Let's Encrypt rate limit is imposed on example.org . If there are a large number of Zenith services and the rate limit is reached, cert-manager will not be able to obtain a certificate and Azimuth and Zenith services will never become available. Using another ACME provider It is possible to configure the issuer to use a different ACME server, e.g. if your institution has an ACME server that issues trusted certificates: environments/my-site/inventory/group_vars/all/variables.yml # The name of the issuer certmanager_acmehttp01issuer_name : example-acme # The URL of the ACME endpoint certmanager_acmehttp01issuer_server : https://acme.example.org If ACME server requires External Account Binding (EAB) it needs to be enabled and credentials added to encrypted secrets file: environments/my-site/inventory/group_vars/all/variables.yml # Indicates whether an External Account Binding (EAB) is required certmanager_acmehttp01issuer_eab_required : yes environments/my-site/inventory/group_vars/all/secrets.yml # The key ID of the EAB certmanager_acmehttp01issuer_eab_kid : \"EXAMPLE_EAB_KID\" # The HMAC key of the EAB certmanager_acmehttp01issuer_eab_key : \"EXAMPLE_EAB_KEY\"","title":"Ingress"},{"location":"configuration/06-ingress/#ingress","text":"As mentioned in the prerequisites, Azimuth and Zenith expect to be given control of an entire subdomain, e.g. *.azimuth.example.org , and this domain must be assigned to a pre-allocated floating IP using a wildcard DNS entry. To tell azimuth-ops what domain it should use, simply set the following variable: environments/my-site/inventory/group_vars/all/variables.yml ingress_base_domain : azimuth.example.org This will result in azimuth-ops using portal.azimuth.example.org for the Azimuth portal interface, and Zenith will use domains of the form <random subdomain>.azimuth.example.org for user-facing services. Other services deployed by Azimuth, such as Harbor and the monitoring and alerting dashboards will also be allocated subdomains under this domain.","title":"Ingress"},{"location":"configuration/06-ingress/#transport-layer-security-tls","text":"TLS for Azimuth can be configured in two ways:","title":"Transport Layer Security (TLS)"},{"location":"configuration/06-ingress/#pre-existing-wildcard-certificate","text":"If you have a pre-existing wildcard TLS certificate issued for the ingress base domain, you can use this to provide TLS for Azimuth and Zenith services. Tip This is the recommended mechanism for production deployments, despite the lack of automation, for two reasons: It is not affected by rate limits Zenith services become available faster as it avoids the overhead of obtaining a certificate per service Warning It is your responsibility to renew the wildcard certificate before it expires. The Azimuth monitoring will produce alerts when the certificate is approaching its expiry date. To configure a pre-existing wildcard certificate for ingress, just create the following files in your environment: environments/my-site/tls/tls.crt Must contain the full certificate chain , with the most specific certificate at the top (e.g. the wildcard certificate), any intermediate certificates and the root CA at the bottom. environments/my-site/tls/tls.key The corresponding private key for the wildcard certificate. Danger The TLS key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted . When these files exist in an environment, azimuth-ops will automatically pick them up and use them. Info In the future, support for a wildcard certificate managed by cert-manager may be implemented. However this will require the use of a supported DNS provider in order to fulfil the DNS01 challenge (wildcard certificates cannot be issued for an HTTP01 challenge).","title":"Pre-existing wildcard certificate"},{"location":"configuration/06-ingress/#automated-with-cert-manager","text":"azimuth-ops is able to configure Ingress resources for Azimuth and Zenith services so that their TLS certificates are managed automatically using cert-manager . In this configuration, a certificate is issued for each separate subdomain by an ACME provider using the HTTP-01 challenge type . By default, cert-manager is enabled and this mechanism will be used to issue TLS certificates for Azimuth and Zenith services with no further configuration. The default ACME service is Let's Encrypt , which issues certificates that are trusted by all major operating systems and browsers. Let's Encrypt rate limits Let's Encrypt imposes rate limits to ensure fair usage. At the time of writing, the number of new certificates that can be issued is 50 per week per registed domain . The \"registered domain\" is the part of the domain that is purchased from the registrar so, for an Azimuth deployment with an ingress base domain of *.azimuth.example.org , the Let's Encrypt rate limit is imposed on example.org . If there are a large number of Zenith services and the rate limit is reached, cert-manager will not be able to obtain a certificate and Azimuth and Zenith services will never become available.","title":"Automated with cert-manager"},{"location":"configuration/07-platform-identity/","text":"Platform Identity In Azimuth, there are two kinds of users: Platform Users who are able to access one or more platforms deployed by Azimuth. Platform Admins who are able to sign in to Azimuth, manage the deployed platforms in a tenancy and administer access to those platforms. Only Platform Admins need to have an OpenStack account. Each Azimuth tenancy has an associated Identity Realm that provides the Platform Users for that tenancy and is administered by the Platform Admins for the tenancy. This allow access to platforms to be granted for users that do not have an OpenStack account. This separation of Platform Users from OpenStack accounts opens up a wide range of use cases that are not possible when platforms can only be accessed by users with OpenStack accounts. Example use case Imagine that a project with quota on an OpenStack cloud wishes to host an open workshop using Jupyter notebooks for teaching. In this case, they definitely don't want to grant every workshop attendee access to OpenStack, as these users may not be trusted. Using Azimuth, a trusted project member (i.e. a Project Admin) can deploy a JupyterHub in Azimuth and create users in their Identity Realm for the workshop attendees. These users can be granted access to JupyterHub for the duration of the workshop, and at the end of the workshop their access can be revoked. Use of Keycloak In order to accomplish this, every Azimuth installation includes an instance of the Keycloak open-source identity management platform. Each Azimuth tenancy (i.e. OpenStack project) has an associated realm in this Keycloak instance that is used to manage access to platforms deployed in that tenancy. The realm provides authentication and authorization for the Zenith services associated with the platforms in the tenancy using OpenID Connect (OIDC) . Each realm is created with two groups - admins and platform-users . Users who are in the platform-users group for a realm are granted access to all platforms deployed in the corresponding Azimuth tenancy. Users who are in the admins group are granted admin status for the realm, meaning they can perform actions in the Keycloak admin console such as: Managing users Assigning users to groups Configuring authentication policies , e.g. password requirements, multi-factor authentication Integrating external identity providers The Keycloak realms created by Azimuth are configured with Azimuth as an identity provider, so that Platform Admins who belong to a tenancy in Azimuth can sign in to the corresponding realm using the Azimuth identity provider. Platform Admins who sign in to a realm via Azimuth are automatically placed in the admins and platform-users groups described above. When a platform is deployed in an Azimuth tenancy a group is created in the corresponding identity realm, and access to the platform is controlled by membership of this group. Each Zenith service for the platform has a child group under the platform group that can be used to grant access to a single service within a platform. For example, the standard Linux Workstation platform (see Cluster-as-a-Service (CaaS) ) exposes two Zenith services - \"Web Console\" and \"Monitoring\". If an instance of this platform is deployed with the name my-workstation , then access to the \"Web Console\" service can be granted by either: Adding the user to the platform-users group. This will also grant the user access to all other platforms in the tenancy. Adding the user to the caas-my-workstation group. This will grant the user access to the \"Web Console\" and \"Monitoring\" services. Adding the user to the caas-my-workstation/webconsole group. This will grant the user access to the \"Web Console\" service only. Keycloak admin password The only required configuration for platform identity is to set the admin password for Keycloak: environments/my-site/inventory/group_vars/all/secrets.yml keycloak_admin_password : \"<secure password>\" Danger This password should be kept secret. If you want to keep the password in Git - which is recommended - then it must be encrypted .","title":"Platform Identity"},{"location":"configuration/07-platform-identity/#platform-identity","text":"In Azimuth, there are two kinds of users: Platform Users who are able to access one or more platforms deployed by Azimuth. Platform Admins who are able to sign in to Azimuth, manage the deployed platforms in a tenancy and administer access to those platforms. Only Platform Admins need to have an OpenStack account. Each Azimuth tenancy has an associated Identity Realm that provides the Platform Users for that tenancy and is administered by the Platform Admins for the tenancy. This allow access to platforms to be granted for users that do not have an OpenStack account. This separation of Platform Users from OpenStack accounts opens up a wide range of use cases that are not possible when platforms can only be accessed by users with OpenStack accounts.","title":"Platform Identity"},{"location":"configuration/07-platform-identity/#example-use-case","text":"Imagine that a project with quota on an OpenStack cloud wishes to host an open workshop using Jupyter notebooks for teaching. In this case, they definitely don't want to grant every workshop attendee access to OpenStack, as these users may not be trusted. Using Azimuth, a trusted project member (i.e. a Project Admin) can deploy a JupyterHub in Azimuth and create users in their Identity Realm for the workshop attendees. These users can be granted access to JupyterHub for the duration of the workshop, and at the end of the workshop their access can be revoked.","title":"Example use case"},{"location":"configuration/07-platform-identity/#use-of-keycloak","text":"In order to accomplish this, every Azimuth installation includes an instance of the Keycloak open-source identity management platform. Each Azimuth tenancy (i.e. OpenStack project) has an associated realm in this Keycloak instance that is used to manage access to platforms deployed in that tenancy. The realm provides authentication and authorization for the Zenith services associated with the platforms in the tenancy using OpenID Connect (OIDC) . Each realm is created with two groups - admins and platform-users . Users who are in the platform-users group for a realm are granted access to all platforms deployed in the corresponding Azimuth tenancy. Users who are in the admins group are granted admin status for the realm, meaning they can perform actions in the Keycloak admin console such as: Managing users Assigning users to groups Configuring authentication policies , e.g. password requirements, multi-factor authentication Integrating external identity providers The Keycloak realms created by Azimuth are configured with Azimuth as an identity provider, so that Platform Admins who belong to a tenancy in Azimuth can sign in to the corresponding realm using the Azimuth identity provider. Platform Admins who sign in to a realm via Azimuth are automatically placed in the admins and platform-users groups described above. When a platform is deployed in an Azimuth tenancy a group is created in the corresponding identity realm, and access to the platform is controlled by membership of this group. Each Zenith service for the platform has a child group under the platform group that can be used to grant access to a single service within a platform. For example, the standard Linux Workstation platform (see Cluster-as-a-Service (CaaS) ) exposes two Zenith services - \"Web Console\" and \"Monitoring\". If an instance of this platform is deployed with the name my-workstation , then access to the \"Web Console\" service can be granted by either: Adding the user to the platform-users group. This will also grant the user access to all other platforms in the tenancy. Adding the user to the caas-my-workstation group. This will grant the user access to the \"Web Console\" and \"Monitoring\" services. Adding the user to the caas-my-workstation/webconsole group. This will grant the user access to the \"Web Console\" service only.","title":"Use of Keycloak"},{"location":"configuration/07-platform-identity/#keycloak-admin-password","text":"The only required configuration for platform identity is to set the admin password for Keycloak: environments/my-site/inventory/group_vars/all/secrets.yml keycloak_admin_password : \"<secure password>\" Danger This password should be kept secret. If you want to keep the password in Git - which is recommended - then it must be encrypted .","title":"Keycloak admin password"},{"location":"configuration/08-zenith/","text":"Zenith Application Proxy The Zenith application proxy is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_apps_enabled : no For all deployments, Zenith requires a secret key to be configured. This is used to sign and verify the single-use tokens issued by the registrar (see the Zenith architecture document for details): environments/my-site/inventory/group_vars/all/secrets.yml zenith_registrar_subdomain_token_signing_key : \"<some secret key>\" Tip This key must be a long, random string - at least 32 bytes (256 bits) is required. A suitable key can be generated using openssl rand -hex 32 . Danger This key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted . SSHD load-balancer IP address HA deployments only This section only applies to HA deployments. In a HA deployment, the Zenith SSHD server has a dedicated load-balancer with its own IP address. The floating IP to use for this load balancer must be pre-allocated and specified using the following variable: environments/my-site/inventory/group_vars/all/variables.yml zenith_sshd_service_load_balancer_ip : \"<ip address>\" SSHD port number By default, the Zenith SSHD server will use port 22 on a dedicated IP address for a HA deployment and port 2222 on the pre-allocated floating IP for a single node deployment (port 22 is used for regular SSH to configure the node). This can be changed using the following variable, if required: environments/my-site/inventory/group_vars/all/variables.yml zenith_sshd_service_port : 22222","title":"Zenith Application Proxy"},{"location":"configuration/08-zenith/#zenith-application-proxy","text":"The Zenith application proxy is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_apps_enabled : no For all deployments, Zenith requires a secret key to be configured. This is used to sign and verify the single-use tokens issued by the registrar (see the Zenith architecture document for details): environments/my-site/inventory/group_vars/all/secrets.yml zenith_registrar_subdomain_token_signing_key : \"<some secret key>\" Tip This key must be a long, random string - at least 32 bytes (256 bits) is required. A suitable key can be generated using openssl rand -hex 32 . Danger This key should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Zenith Application Proxy"},{"location":"configuration/08-zenith/#sshd-load-balancer-ip-address","text":"HA deployments only This section only applies to HA deployments. In a HA deployment, the Zenith SSHD server has a dedicated load-balancer with its own IP address. The floating IP to use for this load balancer must be pre-allocated and specified using the following variable: environments/my-site/inventory/group_vars/all/variables.yml zenith_sshd_service_load_balancer_ip : \"<ip address>\"","title":"SSHD load-balancer IP address"},{"location":"configuration/08-zenith/#sshd-port-number","text":"By default, the Zenith SSHD server will use port 22 on a dedicated IP address for a HA deployment and port 2222 on the pre-allocated floating IP for a single node deployment (port 22 is used for regular SSH to configure the node). This can be changed using the following variable, if required: environments/my-site/inventory/group_vars/all/variables.yml zenith_sshd_service_port : 22222","title":"SSHD port number"},{"location":"configuration/09-community-images/","text":"Community images Azimuth requires a number of specialised images to be available on the target cloud for Cluster-as-a-Service appliances and Kubernetes clusters. Images for Kubernetes and CaaS appliances, with the exception of Slurm, are built using Packer from the definitions in the azimuth-images repository . For Kubernetes, we make use of the recipes from the upstream Cluster API image-builder . Each release of azimuth-images has an associated manifest that describes the images in the release and where to download them from, along with some additional metadata. The Azimuth deployment playbooks are able to consume these manifests. Images for the Slurm cluster appliance are built using Packer from definitions in the slurm-image-builder respository , with builds uploaded here . azimuth-ops is able to download, convert (if required) and then create Glance images on the target cloud from these sources. Images are uploaded as community images meaning they are accessible by all projects in the target cloud, but not included by default when listing images in Horizon, the OpenStack CLI or the OpenStack API. By default, azimuth-ops uploads the set of images that are needed to support Azimuth's functionality and auto-wires them into the correct places for the K3s node, the HA Kubernetes cluster, the default Kubernetes templates and the default Cluster-as-a-Service appliances. Image properties Properties can be set on the uploaded images, if required. The properties are given as strings of the form key=value in the following variable: environments/my-site/inventory/group_vars/all/variables.yml community_images_custom_properties : - 'prop1=value1' - 'prop2=value2' Image conversion Images can be converted from a \"source\" format to a \"target\" format for the target cloud. The majority of images available for download are in qcow2 format, and this is the format in which images built from azimuth-images are distributed. However your cloud may require images to be uploaded in a different format, e.g. raw or vmdk . To specify the target format for your cloud, just set the following (the default is qcow2 ): environments/my-site/inventory/group_vars/all/variables.yml community_images_disk_format : raw When this differs from the source format for an image, azimuth-ops will convert the image using qemu-img convert before uploading it to the target cloud. Disabling community images It is possible to prevent azimuth-ops from uploading any images, even the default ones, by setting the following: environments/my-site/inventory/group_vars/all/variables.yml community_images : {} Warning If community images are disabled you will need to ensure suitable images are uploaded via another mechanism, and the correct variables populated with the image IDs in your Azimuth configuration. See the azimuth-ops roles for more details. Custom images If you want to upload custom images as part of your Azimuth installation, for example to support a custom Cluster-as-a-Service appliance or for older Kubernetes versions, you can use the community_images_extra variable: environments/my-site/inventory/group_vars/all/variables.yml community_images_extra : # Key, used to refer to the image ID debian_11 : # The name of the image to create on the target cloud name : debian-11-generic-amd64-20220711-1073 # The URL of the source image source_url : https://cloud.debian.org/images/cloud/bullseye/20220711-1073/debian-11-generic-amd64-20220711-1073.qcow2 # The disk format of the source image source_disk_format : qcow2 # The container format of the source image container_format : bare The ID of this image can then be referred to elsewhere in your Azimuth configuration using: environments/my-site/inventory/group_vars/all/variables.yml my_custom_caas_appliance_image : \"{{ community_images_image_ids.debian_11 }}\" Warning It is assumed that the name in a community image specification refers to a unique image. azimuth-ops will only download, convert and upload images to the target cloud for names that do not already exist. For this reason, it is recommended to include a timestamp or build reference in the image name in order to identify different images that serve the same purpose (e.g. different builds of the same pipeline).","title":"Community images"},{"location":"configuration/09-community-images/#community-images","text":"Azimuth requires a number of specialised images to be available on the target cloud for Cluster-as-a-Service appliances and Kubernetes clusters. Images for Kubernetes and CaaS appliances, with the exception of Slurm, are built using Packer from the definitions in the azimuth-images repository . For Kubernetes, we make use of the recipes from the upstream Cluster API image-builder . Each release of azimuth-images has an associated manifest that describes the images in the release and where to download them from, along with some additional metadata. The Azimuth deployment playbooks are able to consume these manifests. Images for the Slurm cluster appliance are built using Packer from definitions in the slurm-image-builder respository , with builds uploaded here . azimuth-ops is able to download, convert (if required) and then create Glance images on the target cloud from these sources. Images are uploaded as community images meaning they are accessible by all projects in the target cloud, but not included by default when listing images in Horizon, the OpenStack CLI or the OpenStack API. By default, azimuth-ops uploads the set of images that are needed to support Azimuth's functionality and auto-wires them into the correct places for the K3s node, the HA Kubernetes cluster, the default Kubernetes templates and the default Cluster-as-a-Service appliances.","title":"Community images"},{"location":"configuration/09-community-images/#image-properties","text":"Properties can be set on the uploaded images, if required. The properties are given as strings of the form key=value in the following variable: environments/my-site/inventory/group_vars/all/variables.yml community_images_custom_properties : - 'prop1=value1' - 'prop2=value2'","title":"Image properties"},{"location":"configuration/09-community-images/#image-conversion","text":"Images can be converted from a \"source\" format to a \"target\" format for the target cloud. The majority of images available for download are in qcow2 format, and this is the format in which images built from azimuth-images are distributed. However your cloud may require images to be uploaded in a different format, e.g. raw or vmdk . To specify the target format for your cloud, just set the following (the default is qcow2 ): environments/my-site/inventory/group_vars/all/variables.yml community_images_disk_format : raw When this differs from the source format for an image, azimuth-ops will convert the image using qemu-img convert before uploading it to the target cloud.","title":"Image conversion"},{"location":"configuration/09-community-images/#disabling-community-images","text":"It is possible to prevent azimuth-ops from uploading any images, even the default ones, by setting the following: environments/my-site/inventory/group_vars/all/variables.yml community_images : {} Warning If community images are disabled you will need to ensure suitable images are uploaded via another mechanism, and the correct variables populated with the image IDs in your Azimuth configuration. See the azimuth-ops roles for more details.","title":"Disabling community images"},{"location":"configuration/09-community-images/#custom-images","text":"If you want to upload custom images as part of your Azimuth installation, for example to support a custom Cluster-as-a-Service appliance or for older Kubernetes versions, you can use the community_images_extra variable: environments/my-site/inventory/group_vars/all/variables.yml community_images_extra : # Key, used to refer to the image ID debian_11 : # The name of the image to create on the target cloud name : debian-11-generic-amd64-20220711-1073 # The URL of the source image source_url : https://cloud.debian.org/images/cloud/bullseye/20220711-1073/debian-11-generic-amd64-20220711-1073.qcow2 # The disk format of the source image source_disk_format : qcow2 # The container format of the source image container_format : bare The ID of this image can then be referred to elsewhere in your Azimuth configuration using: environments/my-site/inventory/group_vars/all/variables.yml my_custom_caas_appliance_image : \"{{ community_images_image_ids.debian_11 }}\" Warning It is assumed that the name in a community image specification refers to a unique image. azimuth-ops will only download, convert and upload images to the target cloud for names that do not already exist. For this reason, it is recommended to include a timestamp or build reference in the image name in order to identify different images that serve the same purpose (e.g. different builds of the same pipeline).","title":"Custom images"},{"location":"configuration/10-kubernetes-clusters/","text":"Kubernetes clusters Kubernetes support in Azimuth is implemented using Cluster API with the OpenStack provider . Support for cluster addons is provided by the Cluster API addon provider , which provides functionality for installing Helm charts and additional manifests. Azimuth provides an opinionated interface on top of Cluster API by implementing its own Kubernetes operator . This operator exposes two custom resources which are used by the Azimuth API to manage Kubernetes clusters: clustertemplates.azimuth.stackhpc.com A cluster template represents a \"type\" of Kubernetes cluster. In particular, this is used to provide different Kubernetes versions, but can also be used to provide advanced options, e.g. networking configuration or additional addons that are installed by default on the cluster. Cluster templates can be deprecated, e.g. when a new Kubernetes version is released, resulting in a warning being shown to the user that they should upgrade. clusters.azimuth.stackhpc.com A cluster represents the user-facing definition of a Kubernetes cluster. It references a template, from which the Kubernetes version and other advanced options are taken, but allows the user to specify one or more node groups and toggle a few simple options such as auto-healing and whether the monitoring stack is deployed on the cluster. For each Cluster , the operator manages a release of the openstack-cluster Helm chart . The Helm release in turn manages Cluster API resources for the cluster, including addons. To get the values for the release, the operator first derives some values from the Cluster object which are merged with the values defined in the referenced template. The result of that merge is then merged with any global configuration that has been specified before being passed to Helm. Disabling Kubernetes Kubernetes support is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_kubernetes_enabled : no Kubernetes configuration Kubernetes configuration is very similar for both the Azimuth HA cluster and tenant Kubernetes clusters, since both are deployed using Cluster API. The Kubernetes configuration section discusses the possible options for networking, volumes and availability zones in detail. Cluster templates azimuth-ops is able to manage the available Kubernetes cluster templates using the variable azimuth_capi_operator_cluster_templates . This variable is a dictionary that maps cluster template names to their specifications, and represents the current (i.e. not deprecated) cluster templates. azimuth-ops will not remove cluster templates, but it will mark any templates that are not present in this variable as deprecated. By default, azimuth-ops will ensure a cluster template is present for the latest patch version of each Kubernetes release that is currently maintained. These templates are configured so that Kubernetes nodes will go onto the Azimuth portal-internal network for the project in which the cluster is being deployed. Disabling the default templates To disable the default templates, just set the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_cluster_templates_default : {} Custom cluster templates If you want to include custom cluster templates in addition to the default templates, e.g. for advanced networking configurations, you can specify them using the variable azimuth_capi_operator_cluster_templates_extra . For example, the following demonstrates how to configure a template where the cluster worker nodes have two networks attached - the control plane nodes and workers are all attached to the Azimuth internal network but the workers are attached to an additional SR-IOV capable network: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_cluster_templates_extra : # The index in the dict is the template name kube-1-24-2-multinet : # Access control annotations annotations : {} # The cluster template specification spec : # A human-readable label for the template label : v1.24.2 / multinet # A brief description of the template description : >- Kubernetes 1.24.2 with HA control plane and high-performance networking. # Values for the openstack-cluster Helm chart values : # Specify the image and version for the cluster # These are the only required values kubernetesVersion : 1.24.2 machineImageId : \"{{ community_images_image_ids.kube_1_24 }}\" # Use the portal-internal network as the main cluster network clusterNetworking : internalNetwork : networkFilter : tags : portal-internal # Configure an extra SR-IOV port on worker nodes using an SR-IOV capable network nodeGroupDefaults : machineNetworking : ports : - {} - network : tags : sriov-vlan securityGroups : [] vnicType : direct Access control See Access control for more details on the access control annotations. Harbor registry azimuth-ops is able to manage a Harbor registry as part of an Azimuth installation. This registry is primarily used as a proxy cache to limit the number of times that images that are pulled directly from the internet. If enabled, the Harbor registry will be made available at registry.<ingress base domain> , e.g. registry.azimuth.example.org and will be configured for tenant clusters by default. By default, a single proxy cache is defined for Docker Hub in order to mitigate rate limiting . Tip If you have a Docker Hub mirror available, you can configure Kubernetes clusters to use it instead of deploying Harbor. In this case, you should disable Harbor by setting: harbor_enabled : no Harbor is enabled by default, and requires two secrets to be set: environments/my-site/inventory/group_vars/all/secrets.yml # The admin password for Harbor harbor_admin_password : \"<secure password>\" # The secret key for Harbor # This MUST be exactly 16 alphanumeric characters harbor_secret_key : \"<secure secret key>\" Danger These values should be kept secret. If you want to keep them in Git - which is recommended - then they must be encrypted . Disabling Harbor The Harbor registry can be disabled entirely: environments/my-site/inventory/group_vars/all/variables.yml harbor_enabled : no Additional proxy caches By default, only Docker Hub has a proxy cache. Additional proxy caches can be configured for other registries, if desired: environments/my-site/inventory/group_vars/all/variables.yml harbor_proxy_cache_extra_projects : quay.io : # The name of the project in Harbor name : quay-public # The type of the upstream registry, e.g.: # aws-ecr # azure-acr # docker-hub # docker-registry # gitlab # google-gcr # harbor # jfrog-artifactory # quay-io type : quay-io # The endpoint URL for the registry url : https://quay.io Warning Defining authentication for the upstream registries is not currently supported, i.e. only public repositories can be proxied.","title":"Kubernetes clusters"},{"location":"configuration/10-kubernetes-clusters/#kubernetes-clusters","text":"Kubernetes support in Azimuth is implemented using Cluster API with the OpenStack provider . Support for cluster addons is provided by the Cluster API addon provider , which provides functionality for installing Helm charts and additional manifests. Azimuth provides an opinionated interface on top of Cluster API by implementing its own Kubernetes operator . This operator exposes two custom resources which are used by the Azimuth API to manage Kubernetes clusters: clustertemplates.azimuth.stackhpc.com A cluster template represents a \"type\" of Kubernetes cluster. In particular, this is used to provide different Kubernetes versions, but can also be used to provide advanced options, e.g. networking configuration or additional addons that are installed by default on the cluster. Cluster templates can be deprecated, e.g. when a new Kubernetes version is released, resulting in a warning being shown to the user that they should upgrade. clusters.azimuth.stackhpc.com A cluster represents the user-facing definition of a Kubernetes cluster. It references a template, from which the Kubernetes version and other advanced options are taken, but allows the user to specify one or more node groups and toggle a few simple options such as auto-healing and whether the monitoring stack is deployed on the cluster. For each Cluster , the operator manages a release of the openstack-cluster Helm chart . The Helm release in turn manages Cluster API resources for the cluster, including addons. To get the values for the release, the operator first derives some values from the Cluster object which are merged with the values defined in the referenced template. The result of that merge is then merged with any global configuration that has been specified before being passed to Helm.","title":"Kubernetes clusters"},{"location":"configuration/10-kubernetes-clusters/#disabling-kubernetes","text":"Kubernetes support is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_kubernetes_enabled : no","title":"Disabling Kubernetes"},{"location":"configuration/10-kubernetes-clusters/#kubernetes-configuration","text":"Kubernetes configuration is very similar for both the Azimuth HA cluster and tenant Kubernetes clusters, since both are deployed using Cluster API. The Kubernetes configuration section discusses the possible options for networking, volumes and availability zones in detail.","title":"Kubernetes configuration"},{"location":"configuration/10-kubernetes-clusters/#cluster-templates","text":"azimuth-ops is able to manage the available Kubernetes cluster templates using the variable azimuth_capi_operator_cluster_templates . This variable is a dictionary that maps cluster template names to their specifications, and represents the current (i.e. not deprecated) cluster templates. azimuth-ops will not remove cluster templates, but it will mark any templates that are not present in this variable as deprecated. By default, azimuth-ops will ensure a cluster template is present for the latest patch version of each Kubernetes release that is currently maintained. These templates are configured so that Kubernetes nodes will go onto the Azimuth portal-internal network for the project in which the cluster is being deployed.","title":"Cluster templates"},{"location":"configuration/10-kubernetes-clusters/#disabling-the-default-templates","text":"To disable the default templates, just set the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_cluster_templates_default : {}","title":"Disabling the default templates"},{"location":"configuration/10-kubernetes-clusters/#custom-cluster-templates","text":"If you want to include custom cluster templates in addition to the default templates, e.g. for advanced networking configurations, you can specify them using the variable azimuth_capi_operator_cluster_templates_extra . For example, the following demonstrates how to configure a template where the cluster worker nodes have two networks attached - the control plane nodes and workers are all attached to the Azimuth internal network but the workers are attached to an additional SR-IOV capable network: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_cluster_templates_extra : # The index in the dict is the template name kube-1-24-2-multinet : # Access control annotations annotations : {} # The cluster template specification spec : # A human-readable label for the template label : v1.24.2 / multinet # A brief description of the template description : >- Kubernetes 1.24.2 with HA control plane and high-performance networking. # Values for the openstack-cluster Helm chart values : # Specify the image and version for the cluster # These are the only required values kubernetesVersion : 1.24.2 machineImageId : \"{{ community_images_image_ids.kube_1_24 }}\" # Use the portal-internal network as the main cluster network clusterNetworking : internalNetwork : networkFilter : tags : portal-internal # Configure an extra SR-IOV port on worker nodes using an SR-IOV capable network nodeGroupDefaults : machineNetworking : ports : - {} - network : tags : sriov-vlan securityGroups : [] vnicType : direct Access control See Access control for more details on the access control annotations.","title":"Custom cluster templates"},{"location":"configuration/10-kubernetes-clusters/#harbor-registry","text":"azimuth-ops is able to manage a Harbor registry as part of an Azimuth installation. This registry is primarily used as a proxy cache to limit the number of times that images that are pulled directly from the internet. If enabled, the Harbor registry will be made available at registry.<ingress base domain> , e.g. registry.azimuth.example.org and will be configured for tenant clusters by default. By default, a single proxy cache is defined for Docker Hub in order to mitigate rate limiting . Tip If you have a Docker Hub mirror available, you can configure Kubernetes clusters to use it instead of deploying Harbor. In this case, you should disable Harbor by setting: harbor_enabled : no Harbor is enabled by default, and requires two secrets to be set: environments/my-site/inventory/group_vars/all/secrets.yml # The admin password for Harbor harbor_admin_password : \"<secure password>\" # The secret key for Harbor # This MUST be exactly 16 alphanumeric characters harbor_secret_key : \"<secure secret key>\" Danger These values should be kept secret. If you want to keep them in Git - which is recommended - then they must be encrypted .","title":"Harbor registry"},{"location":"configuration/10-kubernetes-clusters/#disabling-harbor","text":"The Harbor registry can be disabled entirely: environments/my-site/inventory/group_vars/all/variables.yml harbor_enabled : no","title":"Disabling Harbor"},{"location":"configuration/10-kubernetes-clusters/#additional-proxy-caches","text":"By default, only Docker Hub has a proxy cache. Additional proxy caches can be configured for other registries, if desired: environments/my-site/inventory/group_vars/all/variables.yml harbor_proxy_cache_extra_projects : quay.io : # The name of the project in Harbor name : quay-public # The type of the upstream registry, e.g.: # aws-ecr # azure-acr # docker-hub # docker-registry # gitlab # google-gcr # harbor # jfrog-artifactory # quay-io type : quay-io # The endpoint URL for the registry url : https://quay.io Warning Defining authentication for the upstream registries is not currently supported, i.e. only public repositories can be proxied.","title":"Additional proxy caches"},{"location":"configuration/11-kubernetes-apps/","text":"Kubernetes apps Azimuth allows operators to provide a catalog of applications (apps) that users are able to install onto their Kubernetes clusters via the Azimuth user interface. Multiple apps can be installed on the same Kubernetes cluster, and each app gets its own namespace. A Kubernetes app in Azimuth essentially consists of a Helm chart . Azimuth manages specially annotated instances of the HelmRelease resource from the Cluster API addon provider to install and upgrade apps on the target cluster. These apps can be integrated with Zenith to expose services to the user without requiring the use of LoadBalancer services or Kubernetes ingress . The available apps and the available versions of those apps are determined by instances of a custom resource provided by the Azimuth Kubernetes operator - apptemplates.azimuth.stackhpc.com - which references a chart in a Helm chart repository . Azimuth uses the chart metadata to generate the user interface for the app template, and the values schema for the chart to generate a form for collecting input from the user. Azimuth also renders the output of the NOTES.txt file in the user interface, so this can be used to describe how to consume the application. Warning If the chart does not have a values schema, the generated form will be blank and the chart will be deployed with the default values. Tip In addition, a file called azimuth-ui.schema.yaml can be included to apply some small customisations to the generated form, like selecting different controls. See the azimuth-charts for examples. Default app templates Azimuth comes with the following app templates enabled by default: jupyterhub Allows the user to deploy JupyterHub on their clusters. JupyterHub provides a multi-user environment for using Jupyter notebooks where each user gets their own dynamically-provisioned notebook server and storage. The Jupyter notebook interface is exposed using Zenith . daskhub A JupyterHub instance with Dask integration. Dask is a library that aims to simplify the process of scaling data-intensive Python applications, such as those using Numpy or pandas . DaskHub installs Dask Gateway alongside JupyterHub and configures them so that they integrate seamlessly. This allows users to easily create Dask clusters in their notebooks that scale out by creating pods on the underlying Kubernetes cluster. As with jupyterhub above, the notebook interface is exposed using Zenith. binderhub A JupyterHub instance with Binder integration. BinderHub allows you to create custom computing environments that can be shared and used by many remote users. As with jupyterhub above, the notebook interface is exposed using Zenith. kubeflow Allows users to deploy the Kubeflow machine learning toolkit on their clusters. Kubeflow provides an interface for easily accessing best-of-breed machine learning systems using Jupyter notebooks and TensorFlow . huggingface-llm A generative AI chatbot service backed by a HuggingFace large language model. A convenient web interface is exposed via Zenith and the backend API is directly accessible to other applications running on the same Kubernetes cluster for programmatic use cases. For further details, see this blog post . These can be disabled by setting the following variables: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_app_templates_jupyterhub_enabled : false azimuth_capi_operator_app_templates_daskhub_enabled : false azimuth_capi_operator_app_templates_binderhub_enabled : false azimuth_capi_operator_app_templates_kubeflow_enabled : false azimuth_capi_operator_app_templates_huggingface_llm_enabled : false Custom app templates If you have Helm charts that you want to make available as apps, you can define them as follows: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_app_templates_extra : # The key is the name of the app template my-custom-app : # Access control annotations annotations : {} # The cluster template specification spec : # This is the only required field, and determines the chart that is used chart : # The chart repository containing the chart repo : https://my.company.org/helm-charts # The name of the chart that the template is for name : my-custom-app Access control See Access control for more details on the access control annotations. By default, Azimuth will use the last 5 stable versions of the chart (i.e. versions without a prerelease part, see semver.org ) and the name , icon and description from the Chart.yaml file in the chart to build the user interface. A chart annotation is also supported to define the human-readable label: my-chart/Chart.yaml annotations : azimuth.stackhpc.com/label : My Custom App This behaviour can be customised for each app template using the following optional fields: label The human-readable label for the app template, instead of the annotation from Chart.yaml . logo The URL of the logo for the app template, instead of the icon from Chart.yaml . description A short description of the app template, instead of the description from Chart.yaml . versionRange Default: >=0.0.0 (stable versions only). The range of chart versions to consider. Must be a comma-separated list of constraints, where each constraint is an operator followed by a SemVer version . The supported operators are == , != , > , >= , < and <= . Prerelease versions are only considered if the lower bound includes a prerelease part. If no lower bound is given, an implicit lower bound of >=0.0.0 is used. Some examples of valid ranges: >=0.0.0-0 - all versions, including prerelease versions like 1.0.0-alpha.1 < 2.0.0 - all stable versions matching 0.X.Y and 1.X.Y (implicit lower bound) >=1.0.0, < 3.0.0,!=2.1.5 - all stable versions matching 1.X.Y or 2.X.Y except 2.1.5 (useful for excluding specific versions with known issues) keepVersions Default: 5. The number of versions to keep. This is used to limit the size of the AppTemplate resource, because etcd has limits on the maximum size of objects (usually approx. 1MB). This will need to be smaller if the chart has a large values schema. syncFrequency Default: 86400 (24 hours). The number of seconds to wait before checking for new versions of the chart. defaultValues Default: {} . Default values for deployments of the app, on top of the chart defaults. Zenith integration When Zenith is enabled , every Kubernetes cluster created by Azimuth has an instance of the Zenith operator watching it. This operator makes two Kubernetes custom resources available that can be used to expose a Kubernetes service to users without using type: NodePort , type: LoadBalancer or Kubernetes ingress : reservations.zenith.stackhpc.com Represents the reservation of a Zenith domain, and results in the generation of a Kubernetes secret containing an SSH keypair associated with the allocated domain. clients.zenith.stackhpc.com Defines a Zenith client, pointing at a Zenith reservation and upstream Kubernetes service. In addition, services can benefit from the project-level authentication and authorization that is performed by Zenith at it's proxy to prevent unauthorised users from accessing the service. The Zenith services associated with an app are monitored and made available in the Azimuth user interface, making the apps easy to use. Azimuth supports the following annotations on the reservation to determine how the service is rendered in the user interface: annotations : azimuth.stackhpc.com/service-label : \"My Fancy Service\" azimuth.stackhpc.com/service-icon-url : https://my.company.org/images/my-fancy-service-icon.png It is possible to add Zenith integration to an existing chart by creating a new parent chart with the existing chart as a dependency . You can define templates for the Zenith resources in the parent chart, pointing at services created by the child chart. This is the approach taken by the azimuth-charts that provide the default jupyterhub and daskhub apps.","title":"Kubernetes apps"},{"location":"configuration/11-kubernetes-apps/#kubernetes-apps","text":"Azimuth allows operators to provide a catalog of applications (apps) that users are able to install onto their Kubernetes clusters via the Azimuth user interface. Multiple apps can be installed on the same Kubernetes cluster, and each app gets its own namespace. A Kubernetes app in Azimuth essentially consists of a Helm chart . Azimuth manages specially annotated instances of the HelmRelease resource from the Cluster API addon provider to install and upgrade apps on the target cluster. These apps can be integrated with Zenith to expose services to the user without requiring the use of LoadBalancer services or Kubernetes ingress . The available apps and the available versions of those apps are determined by instances of a custom resource provided by the Azimuth Kubernetes operator - apptemplates.azimuth.stackhpc.com - which references a chart in a Helm chart repository . Azimuth uses the chart metadata to generate the user interface for the app template, and the values schema for the chart to generate a form for collecting input from the user. Azimuth also renders the output of the NOTES.txt file in the user interface, so this can be used to describe how to consume the application. Warning If the chart does not have a values schema, the generated form will be blank and the chart will be deployed with the default values. Tip In addition, a file called azimuth-ui.schema.yaml can be included to apply some small customisations to the generated form, like selecting different controls. See the azimuth-charts for examples.","title":"Kubernetes apps"},{"location":"configuration/11-kubernetes-apps/#default-app-templates","text":"Azimuth comes with the following app templates enabled by default: jupyterhub Allows the user to deploy JupyterHub on their clusters. JupyterHub provides a multi-user environment for using Jupyter notebooks where each user gets their own dynamically-provisioned notebook server and storage. The Jupyter notebook interface is exposed using Zenith . daskhub A JupyterHub instance with Dask integration. Dask is a library that aims to simplify the process of scaling data-intensive Python applications, such as those using Numpy or pandas . DaskHub installs Dask Gateway alongside JupyterHub and configures them so that they integrate seamlessly. This allows users to easily create Dask clusters in their notebooks that scale out by creating pods on the underlying Kubernetes cluster. As with jupyterhub above, the notebook interface is exposed using Zenith. binderhub A JupyterHub instance with Binder integration. BinderHub allows you to create custom computing environments that can be shared and used by many remote users. As with jupyterhub above, the notebook interface is exposed using Zenith. kubeflow Allows users to deploy the Kubeflow machine learning toolkit on their clusters. Kubeflow provides an interface for easily accessing best-of-breed machine learning systems using Jupyter notebooks and TensorFlow . huggingface-llm A generative AI chatbot service backed by a HuggingFace large language model. A convenient web interface is exposed via Zenith and the backend API is directly accessible to other applications running on the same Kubernetes cluster for programmatic use cases. For further details, see this blog post . These can be disabled by setting the following variables: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_app_templates_jupyterhub_enabled : false azimuth_capi_operator_app_templates_daskhub_enabled : false azimuth_capi_operator_app_templates_binderhub_enabled : false azimuth_capi_operator_app_templates_kubeflow_enabled : false azimuth_capi_operator_app_templates_huggingface_llm_enabled : false","title":"Default app templates"},{"location":"configuration/11-kubernetes-apps/#custom-app-templates","text":"If you have Helm charts that you want to make available as apps, you can define them as follows: environments/my-site/inventory/group_vars/all/variables.yml azimuth_capi_operator_app_templates_extra : # The key is the name of the app template my-custom-app : # Access control annotations annotations : {} # The cluster template specification spec : # This is the only required field, and determines the chart that is used chart : # The chart repository containing the chart repo : https://my.company.org/helm-charts # The name of the chart that the template is for name : my-custom-app Access control See Access control for more details on the access control annotations. By default, Azimuth will use the last 5 stable versions of the chart (i.e. versions without a prerelease part, see semver.org ) and the name , icon and description from the Chart.yaml file in the chart to build the user interface. A chart annotation is also supported to define the human-readable label: my-chart/Chart.yaml annotations : azimuth.stackhpc.com/label : My Custom App This behaviour can be customised for each app template using the following optional fields: label The human-readable label for the app template, instead of the annotation from Chart.yaml . logo The URL of the logo for the app template, instead of the icon from Chart.yaml . description A short description of the app template, instead of the description from Chart.yaml . versionRange Default: >=0.0.0 (stable versions only). The range of chart versions to consider. Must be a comma-separated list of constraints, where each constraint is an operator followed by a SemVer version . The supported operators are == , != , > , >= , < and <= . Prerelease versions are only considered if the lower bound includes a prerelease part. If no lower bound is given, an implicit lower bound of >=0.0.0 is used. Some examples of valid ranges: >=0.0.0-0 - all versions, including prerelease versions like 1.0.0-alpha.1 < 2.0.0 - all stable versions matching 0.X.Y and 1.X.Y (implicit lower bound) >=1.0.0, < 3.0.0,!=2.1.5 - all stable versions matching 1.X.Y or 2.X.Y except 2.1.5 (useful for excluding specific versions with known issues) keepVersions Default: 5. The number of versions to keep. This is used to limit the size of the AppTemplate resource, because etcd has limits on the maximum size of objects (usually approx. 1MB). This will need to be smaller if the chart has a large values schema. syncFrequency Default: 86400 (24 hours). The number of seconds to wait before checking for new versions of the chart. defaultValues Default: {} . Default values for deployments of the app, on top of the chart defaults.","title":"Custom app templates"},{"location":"configuration/11-kubernetes-apps/#zenith-integration","text":"When Zenith is enabled , every Kubernetes cluster created by Azimuth has an instance of the Zenith operator watching it. This operator makes two Kubernetes custom resources available that can be used to expose a Kubernetes service to users without using type: NodePort , type: LoadBalancer or Kubernetes ingress : reservations.zenith.stackhpc.com Represents the reservation of a Zenith domain, and results in the generation of a Kubernetes secret containing an SSH keypair associated with the allocated domain. clients.zenith.stackhpc.com Defines a Zenith client, pointing at a Zenith reservation and upstream Kubernetes service. In addition, services can benefit from the project-level authentication and authorization that is performed by Zenith at it's proxy to prevent unauthorised users from accessing the service. The Zenith services associated with an app are monitored and made available in the Azimuth user interface, making the apps easy to use. Azimuth supports the following annotations on the reservation to determine how the service is rendered in the user interface: annotations : azimuth.stackhpc.com/service-label : \"My Fancy Service\" azimuth.stackhpc.com/service-icon-url : https://my.company.org/images/my-fancy-service-icon.png It is possible to add Zenith integration to an existing chart by creating a new parent chart with the existing chart as a dependency . You can define templates for the Zenith resources in the parent chart, pointing at services created by the child chart. This is the approach taken by the azimuth-charts that provide the default jupyterhub and daskhub apps.","title":"Zenith integration"},{"location":"configuration/12-caas/","text":"Cluster-as-a-Service (CaaS) Cluster-as-a-Service (CaaS) in Azimuth allows self-service platforms to be provided to users that are deployed and configured using a combination of Ansible , OpenTofu and Packer , stored in a git repository. CaaS support in Azimuth is implemented by the Azimuth CaaS operator . The operator executes Ansible playbooks using ansible-runner in response to Azimuth creating and modifying instances of the custom resources that it exposes: clustertypes.caas.azimuth.stackhpc.com A cluster type represents an available appliance, e.g. \"workstation\" or \"Slurm cluster\". This CRD defines the git repository, version and playbook that will be used to deploy clusters of the specified type, along with metadata for generating the UI and any global variable such as image UUIDs. clusters.caas.azimuth.stackhpc.com A cluster represents the combination of a cluster type with values collected from the user. The CaaS operator tracks the status of the ansible-runner executions for the cluster and reports it on the CRD for Azimuth to consume. Disabling CaaS CaaS support is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_clusters_enabled : no Removing legacy AWX components If you are migrating from an installation using the previous AWX implementation then Azimuth itself will be reconfigured to use the CaaS CRD, but by default the AWX installation will be left untouched in order to allow any legacy apps to be cleaned up. To remove AWX components, an additional variable must be set when the provision playbook is executed: ansible-playbook stackhpc.azimuth_ops.provision -e awx_purge = yes StackHPC Appliances By default, three appliances maintained by StackHPC are made available - the Slurm appliance , the Linux Workstation appliance and the repo2docker appliance . Slurm appliance The Slurm appliance allows users to deploy Slurm clusters for running batch workloads. The clusters include the Open OnDemand web interface and a monitoring stack with web dashboards, both of which are exposed using Zenith. To disable the Slurm appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_slurm_appliance_enabled : no Linux Workstation appliance The Linux Workstation appliance allows users to provision a workstation that is accessible via a web-browser using Apache Guacamole . Guacamole provides a web-based virtual desktop and a console, and is exposed using Zenith. A simple monitoring stack is also available, exposed via Zenith. To disable the Linux Workstation appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_workstation_enabled : no repo2docker appliance The repo2docker appliance allows users to deploy a Jupyter Notebook server, exposed via Zenith, from a repo2docker compliant repository. A simple monitoring stack is also available, exposed via Zenith. To disable the repo2docker appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_repo2docker_enabled : no R-studio appliance The R-studio appliance allows users to deploy an R-studio server instance running on a cloud VM. A simple monitoring stack is also provided, with both the R-studio and monitoring services exposed via Zenith. To disable the R-studio appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_rstudio_enabled : no Custom appliances It is possible to make custom appliances available in the Azimuth interface for users to deploy. For more information on building a CaaS-compatible appliance, please see the sample appliance . Custom appliances can be easily specified in your Azimuth configuration. For example, the following will configure the sample appliance as an available cluster type: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_cluster_templates_overrides : sample-appliance : # Access control annotations annotations : {} # The cluster type specification spec : # The git URL of the appliance gitUrl : https://github.com/stackhpc/azimuth-sample-appliance.git # The branch, tag or commit id to use # For production, it is recommended to use a fixed tag or commit ID gitVersion : main # The name of the playbook to use playbook : sample-appliance.yml # The URL of the metadata file uiMetaUrl : https://raw.githubusercontent.com/stackhpc/azimuth-sample-appliance/main/ui-meta/sample-appliance.yaml # Dict of extra variables for the appliance extraVars : # Use the ID of an Ubuntu 20.04 image that we asked azimuth-ops to upload cluster_image : \"{{ community_images_image_ids.ubuntu_2004_20220712 }}\" Access control See Access control for more details on the access control annotations.","title":"Cluster-as-a-Service (CaaS)"},{"location":"configuration/12-caas/#cluster-as-a-service-caas","text":"Cluster-as-a-Service (CaaS) in Azimuth allows self-service platforms to be provided to users that are deployed and configured using a combination of Ansible , OpenTofu and Packer , stored in a git repository. CaaS support in Azimuth is implemented by the Azimuth CaaS operator . The operator executes Ansible playbooks using ansible-runner in response to Azimuth creating and modifying instances of the custom resources that it exposes: clustertypes.caas.azimuth.stackhpc.com A cluster type represents an available appliance, e.g. \"workstation\" or \"Slurm cluster\". This CRD defines the git repository, version and playbook that will be used to deploy clusters of the specified type, along with metadata for generating the UI and any global variable such as image UUIDs. clusters.caas.azimuth.stackhpc.com A cluster represents the combination of a cluster type with values collected from the user. The CaaS operator tracks the status of the ansible-runner executions for the cluster and reports it on the CRD for Azimuth to consume.","title":"Cluster-as-a-Service (CaaS)"},{"location":"configuration/12-caas/#disabling-caas","text":"CaaS support is enabled by default in the reference configuration. To disable it, just set: environments/my-site/inventory/group_vars/all/variables.yml azimuth_clusters_enabled : no","title":"Disabling CaaS"},{"location":"configuration/12-caas/#removing-legacy-awx-components","text":"If you are migrating from an installation using the previous AWX implementation then Azimuth itself will be reconfigured to use the CaaS CRD, but by default the AWX installation will be left untouched in order to allow any legacy apps to be cleaned up. To remove AWX components, an additional variable must be set when the provision playbook is executed: ansible-playbook stackhpc.azimuth_ops.provision -e awx_purge = yes","title":"Removing legacy AWX components"},{"location":"configuration/12-caas/#stackhpc-appliances","text":"By default, three appliances maintained by StackHPC are made available - the Slurm appliance , the Linux Workstation appliance and the repo2docker appliance .","title":"StackHPC Appliances"},{"location":"configuration/12-caas/#slurm-appliance","text":"The Slurm appliance allows users to deploy Slurm clusters for running batch workloads. The clusters include the Open OnDemand web interface and a monitoring stack with web dashboards, both of which are exposed using Zenith. To disable the Slurm appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_slurm_appliance_enabled : no","title":"Slurm appliance"},{"location":"configuration/12-caas/#linux-workstation-appliance","text":"The Linux Workstation appliance allows users to provision a workstation that is accessible via a web-browser using Apache Guacamole . Guacamole provides a web-based virtual desktop and a console, and is exposed using Zenith. A simple monitoring stack is also available, exposed via Zenith. To disable the Linux Workstation appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_workstation_enabled : no","title":"Linux Workstation appliance"},{"location":"configuration/12-caas/#repo2docker-appliance","text":"The repo2docker appliance allows users to deploy a Jupyter Notebook server, exposed via Zenith, from a repo2docker compliant repository. A simple monitoring stack is also available, exposed via Zenith. To disable the repo2docker appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_repo2docker_enabled : no","title":"repo2docker appliance"},{"location":"configuration/12-caas/#r-studio-appliance","text":"The R-studio appliance allows users to deploy an R-studio server instance running on a cloud VM. A simple monitoring stack is also provided, with both the R-studio and monitoring services exposed via Zenith. To disable the R-studio appliance, use the following: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_stackhpc_rstudio_enabled : no","title":"R-studio appliance"},{"location":"configuration/12-caas/#custom-appliances","text":"It is possible to make custom appliances available in the Azimuth interface for users to deploy. For more information on building a CaaS-compatible appliance, please see the sample appliance . Custom appliances can be easily specified in your Azimuth configuration. For example, the following will configure the sample appliance as an available cluster type: environments/my-site/inventory/group_vars/all/variables.yml azimuth_caas_cluster_templates_overrides : sample-appliance : # Access control annotations annotations : {} # The cluster type specification spec : # The git URL of the appliance gitUrl : https://github.com/stackhpc/azimuth-sample-appliance.git # The branch, tag or commit id to use # For production, it is recommended to use a fixed tag or commit ID gitVersion : main # The name of the playbook to use playbook : sample-appliance.yml # The URL of the metadata file uiMetaUrl : https://raw.githubusercontent.com/stackhpc/azimuth-sample-appliance/main/ui-meta/sample-appliance.yaml # Dict of extra variables for the appliance extraVars : # Use the ID of an Ubuntu 20.04 image that we asked azimuth-ops to upload cluster_image : \"{{ community_images_image_ids.ubuntu_2004_20220712 }}\" Access control See Access control for more details on the access control annotations.","title":"Custom appliances"},{"location":"configuration/13-access-control/","text":"Access control Azimuth allows access to platform types to be restricted on a per-platform type and per-tenant basis, allowing different platform types to be made available to different tenants as required. Kubernetes cluster templates Kubernetes app templates CaaS cluster types By default, all of the platform types are available to all tenants. Warning Any restrictions that are applied after platforms have already been created do not result in existing platforms being deleted. However the creation of new platforms will be restricted to the available platform types, and platforms deployed using platform types that are subsequently restricted will be limited to the delete action only. Annotations Access control is implemented using annotations that are applied to instances of the clustertemplates.azimuth.stackhpc.com , apptemplates.azimuth.stackhpc.com and clustertypes.caas.azimuth.stackhpc.com resources for Kubernetes cluster templates, Kubernetes apps and CaaS cluster types respectively. The annotations are the same for all platform types, and the following annotations are respected, in order of precedence: acl.azimuth.stackhpc.com/deny-list A comma-separated list of tenancy IDs that are not allowed to use the platform type. acl.azimuth.stackhpc.com/allow-list A comma-separated list of tenancy IDs that are allowed to use the platform type. acl.azimuth.stackhpc.com/deny-regex A regex where matching tenancy names are not allowed to use the platform type. acl.azimuth.stackhpc.com/allow-regex A regex where matching tenancy names are allowed to use the platform type. In particular, the precedence order means that, for example: A tenancy whose name matches the allow-regex but whose ID is in the deny-list is not able to use the platform type. Similarly, a tenancy whose name matches the deny-regex but whose ID is in the allow-list is allowed to use the platform type. A tenancy whose ID is in both the allow-list and deny-list is not allowed to use the platform type. No annotations means allow If no access control annotations are present, then that platform type is available to all tenants. Deny-by-default when allow annotations are present The presence of at least one allow annotation triggers a deny-by-default policy, where tenants not matching an allow annotation are denied. Built-in platform types azimuth-ops supports a number of variables that can be used to apply access controls to the built-in platform types. The following variables allow default access controls to be set for all built-in platform types : environments/my-site/inventory/group_vars/all/variables.yml # List of denied tenancy IDs platforms_tenancy_deny_list : - \"<id1>\" - \"<id2>\" # List of allowed tenancy IDs platforms_tenancy_allow_list : - \"<id3>\" - \"<id4>\" # Regex pattern to deny tenancies by name platforms_tenancy_deny_regex : \"dev$\" # Deny any tenancy whose name ends with 'dev' # Regex pattern to allow tenancies by name platforms_tenancy_allow_regex : \".*\" # Allow all tenancies by default These can be overridden for specific platform types if required: environments/my-site/inventory/group_vars/all/variables.yml # The following apply to all Kubernetes cluster templates azimuth_capi_operator_cluster_template_tenancy_deny_list : azimuth_capi_operator_cluster_template_tenancy_allow_list : azimuth_capi_operator_cluster_template_tenancy_deny_regex : azimuth_capi_operator_cluster_template_tenancy_allow_regex : # Each Kubernetes app has specific variables # Replace 'jupyterhub' with the required app template azimuth_capi_operator_app_templates_jupyterhub_tenancy_deny_list : azimuth_capi_operator_app_templates_jupyterhub_tenancy_allow_list : azimuth_capi_operator_app_templates_jupyterhub_tenancy_deny_regex : azimuth_capi_operator_app_templates_jupyterhub_tenancy_allow_regex : # Each CaaS cluster type has specific variables # Replace 'slurm' with the required cluster type azimuth_caas_stackhpc_slurm_appliance_tenancy_deny_list : azimuth_caas_stackhpc_slurm_appliance_tenancy_allow_list : azimuth_caas_stackhpc_slurm_appliance_tenancy_deny_regex : azimuth_caas_stackhpc_slurm_appliance_tenancy_allow_regex :","title":"Access control"},{"location":"configuration/13-access-control/#access-control","text":"Azimuth allows access to platform types to be restricted on a per-platform type and per-tenant basis, allowing different platform types to be made available to different tenants as required. Kubernetes cluster templates Kubernetes app templates CaaS cluster types By default, all of the platform types are available to all tenants. Warning Any restrictions that are applied after platforms have already been created do not result in existing platforms being deleted. However the creation of new platforms will be restricted to the available platform types, and platforms deployed using platform types that are subsequently restricted will be limited to the delete action only.","title":"Access control"},{"location":"configuration/13-access-control/#annotations","text":"Access control is implemented using annotations that are applied to instances of the clustertemplates.azimuth.stackhpc.com , apptemplates.azimuth.stackhpc.com and clustertypes.caas.azimuth.stackhpc.com resources for Kubernetes cluster templates, Kubernetes apps and CaaS cluster types respectively. The annotations are the same for all platform types, and the following annotations are respected, in order of precedence: acl.azimuth.stackhpc.com/deny-list A comma-separated list of tenancy IDs that are not allowed to use the platform type. acl.azimuth.stackhpc.com/allow-list A comma-separated list of tenancy IDs that are allowed to use the platform type. acl.azimuth.stackhpc.com/deny-regex A regex where matching tenancy names are not allowed to use the platform type. acl.azimuth.stackhpc.com/allow-regex A regex where matching tenancy names are allowed to use the platform type. In particular, the precedence order means that, for example: A tenancy whose name matches the allow-regex but whose ID is in the deny-list is not able to use the platform type. Similarly, a tenancy whose name matches the deny-regex but whose ID is in the allow-list is allowed to use the platform type. A tenancy whose ID is in both the allow-list and deny-list is not allowed to use the platform type. No annotations means allow If no access control annotations are present, then that platform type is available to all tenants. Deny-by-default when allow annotations are present The presence of at least one allow annotation triggers a deny-by-default policy, where tenants not matching an allow annotation are denied.","title":"Annotations"},{"location":"configuration/13-access-control/#built-in-platform-types","text":"azimuth-ops supports a number of variables that can be used to apply access controls to the built-in platform types. The following variables allow default access controls to be set for all built-in platform types : environments/my-site/inventory/group_vars/all/variables.yml # List of denied tenancy IDs platforms_tenancy_deny_list : - \"<id1>\" - \"<id2>\" # List of allowed tenancy IDs platforms_tenancy_allow_list : - \"<id3>\" - \"<id4>\" # Regex pattern to deny tenancies by name platforms_tenancy_deny_regex : \"dev$\" # Deny any tenancy whose name ends with 'dev' # Regex pattern to allow tenancies by name platforms_tenancy_allow_regex : \".*\" # Allow all tenancies by default These can be overridden for specific platform types if required: environments/my-site/inventory/group_vars/all/variables.yml # The following apply to all Kubernetes cluster templates azimuth_capi_operator_cluster_template_tenancy_deny_list : azimuth_capi_operator_cluster_template_tenancy_allow_list : azimuth_capi_operator_cluster_template_tenancy_deny_regex : azimuth_capi_operator_cluster_template_tenancy_allow_regex : # Each Kubernetes app has specific variables # Replace 'jupyterhub' with the required app template azimuth_capi_operator_app_templates_jupyterhub_tenancy_deny_list : azimuth_capi_operator_app_templates_jupyterhub_tenancy_allow_list : azimuth_capi_operator_app_templates_jupyterhub_tenancy_deny_regex : azimuth_capi_operator_app_templates_jupyterhub_tenancy_allow_regex : # Each CaaS cluster type has specific variables # Replace 'slurm' with the required cluster type azimuth_caas_stackhpc_slurm_appliance_tenancy_deny_list : azimuth_caas_stackhpc_slurm_appliance_tenancy_allow_list : azimuth_caas_stackhpc_slurm_appliance_tenancy_deny_regex : azimuth_caas_stackhpc_slurm_appliance_tenancy_allow_regex :","title":"Built-in platform types"},{"location":"configuration/14-monitoring/","text":"Monitoring and alerting Azimuth installations come with a monitoring and alerting stack that uses Prometheus to collect metrics on various components of the Kubernetes cluster and the Azimuth components running on it, Alertmanager to produce alerts based on those metrics and Grafana to visualise the metrics using a curated set of dashboards. HA installations also include a log aggregation stack using Loki and Promtail that collects logs from all the pods running on the cluster and the systemd services on each cluster node. These logs are available in a dashboard in Grafana, where they can be filtered and searched. In addition to the monitoring and alerting stack, several additional dashboards are installed: The Kubernetes dashboard for browsing the current state of Kubernetes resources. The Helm dashboard for browsing the current state of Helm releases. The Consul UI for browsing the Consul state (used by Cluster-as-a-Service and Zenith). The ARA Records Ansible (ARA) web interface for browsing the Ansible playbook runs that have been recorded for operations on Cluster-as-a-Service appliances. All the dashboards that access Kubernetes resources are configured to be read-only. Accessing web interfaces The monitoring and alerting web dashboards are exposed as subdomains under the ingress_base_domain : grafana for the Grafana dashboards prometheus for the Prometheus web interface alertmanager for the Alertmanager web interface consul for the Consul UI ara for the ARA web interface helm for the Helm dashboard kubernetes for the Kubernetes dashboard The dashboards are protected by a username and password (using HTTP Basic Auth ). The username is admin and a strong password must be set in your configuration: environments/my-site/inventory/group_vars/all/secrets.yml admin_dashboard_ingress_basic_auth_password : \"<secure password>\" Sensitive information The dashboards allow read-only access to the internals of your Azimuth installation. As such you should ensure that a strong password is used, and take care when sharing it. Danger This password should be kept secret. If you want to keep the password in Git - which is recommended - then it must be encrypted . Persistence and retention Note Persistence is only configured for HA deployments. In order for metrics, alert state (e.g. silences) and logs to persist across pod restarts, we must configure Prometheus, Alertmanager and Loki to use persistent volumes to store their data. This is configured by default in an Azimuth HA installation, but you may wish to tweak the retention periods and/or volume sizes based on your requirements and/or observed usage. The following variables, shown with their default values, control the retention periods and volume sizes for Alertmanager, Prometheus and Loki: environments/my-site/inventory/group_vars/all/variables.yml # Alertmanager retention and volume size capi_cluster_addons_monitoring_alertmanager_retention : 168h capi_cluster_addons_monitoring_alertmanager_volume_size : 10Gi # Prometheus retention and volume size capi_cluster_addons_monitoring_prometheus_retention : 90d capi_cluster_addons_monitoring_prometheus_volume_size : 10Gi # Loki retention and volume size capi_cluster_addons_monitoring_loki_retention : 744h capi_cluster_addons_monitoring_loki_volume_size : 10Gi Danger Volumes can only be increased in size. Any attempt to reduce the size of a volume will be rejected. Slack alerts If your organisation uses Slack , Alertmanager can be configured to send alerts to a Slack channel using an Incoming Webhook . To enable Slack alerts, you must first create a webhook . This should result in a URL of the form: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX This URL should be placed in the following variable to allow Azimuth's Alertmanager to send alerts to Slack: environments/my-site/inventory/group_vars/all/secrets.yml alertmanager_config_slack_webhook_url : https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX Danger The webhook URL should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Monitoring and alerting"},{"location":"configuration/14-monitoring/#monitoring-and-alerting","text":"Azimuth installations come with a monitoring and alerting stack that uses Prometheus to collect metrics on various components of the Kubernetes cluster and the Azimuth components running on it, Alertmanager to produce alerts based on those metrics and Grafana to visualise the metrics using a curated set of dashboards. HA installations also include a log aggregation stack using Loki and Promtail that collects logs from all the pods running on the cluster and the systemd services on each cluster node. These logs are available in a dashboard in Grafana, where they can be filtered and searched. In addition to the monitoring and alerting stack, several additional dashboards are installed: The Kubernetes dashboard for browsing the current state of Kubernetes resources. The Helm dashboard for browsing the current state of Helm releases. The Consul UI for browsing the Consul state (used by Cluster-as-a-Service and Zenith). The ARA Records Ansible (ARA) web interface for browsing the Ansible playbook runs that have been recorded for operations on Cluster-as-a-Service appliances. All the dashboards that access Kubernetes resources are configured to be read-only.","title":"Monitoring and alerting"},{"location":"configuration/14-monitoring/#accessing-web-interfaces","text":"The monitoring and alerting web dashboards are exposed as subdomains under the ingress_base_domain : grafana for the Grafana dashboards prometheus for the Prometheus web interface alertmanager for the Alertmanager web interface consul for the Consul UI ara for the ARA web interface helm for the Helm dashboard kubernetes for the Kubernetes dashboard The dashboards are protected by a username and password (using HTTP Basic Auth ). The username is admin and a strong password must be set in your configuration: environments/my-site/inventory/group_vars/all/secrets.yml admin_dashboard_ingress_basic_auth_password : \"<secure password>\" Sensitive information The dashboards allow read-only access to the internals of your Azimuth installation. As such you should ensure that a strong password is used, and take care when sharing it. Danger This password should be kept secret. If you want to keep the password in Git - which is recommended - then it must be encrypted .","title":"Accessing web interfaces"},{"location":"configuration/14-monitoring/#persistence-and-retention","text":"Note Persistence is only configured for HA deployments. In order for metrics, alert state (e.g. silences) and logs to persist across pod restarts, we must configure Prometheus, Alertmanager and Loki to use persistent volumes to store their data. This is configured by default in an Azimuth HA installation, but you may wish to tweak the retention periods and/or volume sizes based on your requirements and/or observed usage. The following variables, shown with their default values, control the retention periods and volume sizes for Alertmanager, Prometheus and Loki: environments/my-site/inventory/group_vars/all/variables.yml # Alertmanager retention and volume size capi_cluster_addons_monitoring_alertmanager_retention : 168h capi_cluster_addons_monitoring_alertmanager_volume_size : 10Gi # Prometheus retention and volume size capi_cluster_addons_monitoring_prometheus_retention : 90d capi_cluster_addons_monitoring_prometheus_volume_size : 10Gi # Loki retention and volume size capi_cluster_addons_monitoring_loki_retention : 744h capi_cluster_addons_monitoring_loki_volume_size : 10Gi Danger Volumes can only be increased in size. Any attempt to reduce the size of a volume will be rejected.","title":"Persistence and retention"},{"location":"configuration/14-monitoring/#slack-alerts","text":"If your organisation uses Slack , Alertmanager can be configured to send alerts to a Slack channel using an Incoming Webhook . To enable Slack alerts, you must first create a webhook . This should result in a URL of the form: https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX This URL should be placed in the following variable to allow Azimuth's Alertmanager to send alerts to Slack: environments/my-site/inventory/group_vars/all/secrets.yml alertmanager_config_slack_webhook_url : https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX Danger The webhook URL should be kept secret. If you want to keep it in Git - which is recommended - then it must be encrypted .","title":"Slack alerts"},{"location":"configuration/15-disaster-recovery/","text":"Disaster Recovery Azimuth uses Velero as a disaster recovery solution. Velero provides the ability to back up Kubernetes API resources to an object store and has a plugin-based system to enable snapshotting of a cluster's persistent volumes. Warning Backup and restore is only available for production-grade HA installations of Azimuth. The Azimuth playbooks install Velero on the HA management cluster and the Velero CLI tool on the seed node. Once configured with the appropriate credentials, the installation process will create a Schedule on the HA cluster, which triggers a daily backup at midnight and cleans up backups older which are more than 1 week old. The AWS Velero plugin is used for S3 support and the CSI plugin for volume snapshots. The CSI plugin uses Kubernetes generic support for Volume Snapshots , which is implemented for OpenStack by the Cinder CSI plugin . Configuration To enable backup and restore functionality, the following variables must be set in your environment: environments/my-site/inventory/group_vars/all/variables.yml # Enable Velero velero_enabled : true # The URL of the S3 storage endpoint velero_s3_url : <object-store-endpoint-url> # The name of the bucket to use for backups velero_bucket_name : <bucket-name> Bucket must already exist The specified bucket must already exist - neither azimuth-ops nor Velero will create it. You will also need to consult the documentation for your S3 provider to obtain S3 credentials for the bucket, and add the access key ID and secret to the following variables: environments/my-site/inventory/group_vars/all/secrets.yml # Access key ID and secret for accessing the S3 bucket velero_aws_access_key_id : <s3-access-key-id> velero_aws_secret_access_key : <s3-secret-value> Generating credentials for Keystone-integrated Ceph Object Gateway If the S3 target is Ceph Object Gateway integrated with Keystone , a common configuration with OpenStack clouds, S3 credentials can be generated using the following: openstack ec2 credentials create Danger The S3 credentials should be kept secret. If you want to keep them in Git - which is recommended - then they must be encrypted . Velero CLI The Velero installation process also installs the Velero CLI on the Azimuth seed node, which can be used to inspect the state of the backups: On the seed node, with the kubeconfig for the HA cluster exported # List the configured backup locations velero backup-location get # List the backups and their statuses velero backup get See velero -h for other useful commands. Restoring from a backup To restore from a backup, you must first know the name of the target backup. This can be inferred from the object names in S3 if the Velero CLI is no longer available. Once you have the name of the backup to restore, run the following command with your environment activated (similar to a provision): ansible-playbook stackhpc.azimuth_ops.restore \\ -e velero_restore_backup_name = <backup name> This will provision a new HA cluster, restore the backup onto it and then bring the installation up-to-date with your configuration. Performing ad-hoc backups In order to perform ad-hoc backups using the same config parameters as the installed backup schedule, run the following Velero CLI command from the seed node: On the seed node, with the kubeconfig for the HA cluster exported velero backup create --from-schedule default This will begin the backup process in the background. The status of this backup (and others) can be viewed with the velero backup get command shown above. Tip Ad-hoc backups will have the same time-to-live as the configured schedule backups (default = 7 days). To change this, pass the --ttl <hours> option to the velero backup create command. Modifying the backup schedule The following config options are available for modifying the regular backup schedule: environments/my-site/inventory/group_vars/all/variables.yml # Whether or not to perform scheduled backups velero_backup_schedule_enabled : true # Name for backup schedule kubernetes resource velero_backup_schedule_name : default # Schedule to use for backups (defaults to every day at midnight) # See https://en.wikipedia.org/wiki/Cron for format options velero_backup_schedule : \"0 0 * * *\" # Time-to-live for existing backups (defaults to 1 week) # See https://pkg.go.dev/time#ParseDuration for duration format options velero_backup_ttl : \"168h\" Note Setting velero_backup_schedule_enabled: false does not prevent the backup schedule from being installed - instead it sets the schedule state to paused . This allows for ad-hoc backups to still be run on demand using the configured backup parameters.","title":"Disaster Recovery"},{"location":"configuration/15-disaster-recovery/#disaster-recovery","text":"Azimuth uses Velero as a disaster recovery solution. Velero provides the ability to back up Kubernetes API resources to an object store and has a plugin-based system to enable snapshotting of a cluster's persistent volumes. Warning Backup and restore is only available for production-grade HA installations of Azimuth. The Azimuth playbooks install Velero on the HA management cluster and the Velero CLI tool on the seed node. Once configured with the appropriate credentials, the installation process will create a Schedule on the HA cluster, which triggers a daily backup at midnight and cleans up backups older which are more than 1 week old. The AWS Velero plugin is used for S3 support and the CSI plugin for volume snapshots. The CSI plugin uses Kubernetes generic support for Volume Snapshots , which is implemented for OpenStack by the Cinder CSI plugin .","title":"Disaster Recovery"},{"location":"configuration/15-disaster-recovery/#configuration","text":"To enable backup and restore functionality, the following variables must be set in your environment: environments/my-site/inventory/group_vars/all/variables.yml # Enable Velero velero_enabled : true # The URL of the S3 storage endpoint velero_s3_url : <object-store-endpoint-url> # The name of the bucket to use for backups velero_bucket_name : <bucket-name> Bucket must already exist The specified bucket must already exist - neither azimuth-ops nor Velero will create it. You will also need to consult the documentation for your S3 provider to obtain S3 credentials for the bucket, and add the access key ID and secret to the following variables: environments/my-site/inventory/group_vars/all/secrets.yml # Access key ID and secret for accessing the S3 bucket velero_aws_access_key_id : <s3-access-key-id> velero_aws_secret_access_key : <s3-secret-value> Generating credentials for Keystone-integrated Ceph Object Gateway If the S3 target is Ceph Object Gateway integrated with Keystone , a common configuration with OpenStack clouds, S3 credentials can be generated using the following: openstack ec2 credentials create Danger The S3 credentials should be kept secret. If you want to keep them in Git - which is recommended - then they must be encrypted .","title":"Configuration"},{"location":"configuration/15-disaster-recovery/#velero-cli","text":"The Velero installation process also installs the Velero CLI on the Azimuth seed node, which can be used to inspect the state of the backups: On the seed node, with the kubeconfig for the HA cluster exported # List the configured backup locations velero backup-location get # List the backups and their statuses velero backup get See velero -h for other useful commands.","title":"Velero CLI"},{"location":"configuration/15-disaster-recovery/#restoring-from-a-backup","text":"To restore from a backup, you must first know the name of the target backup. This can be inferred from the object names in S3 if the Velero CLI is no longer available. Once you have the name of the backup to restore, run the following command with your environment activated (similar to a provision): ansible-playbook stackhpc.azimuth_ops.restore \\ -e velero_restore_backup_name = <backup name> This will provision a new HA cluster, restore the backup onto it and then bring the installation up-to-date with your configuration.","title":"Restoring from a backup"},{"location":"configuration/15-disaster-recovery/#performing-ad-hoc-backups","text":"In order to perform ad-hoc backups using the same config parameters as the installed backup schedule, run the following Velero CLI command from the seed node: On the seed node, with the kubeconfig for the HA cluster exported velero backup create --from-schedule default This will begin the backup process in the background. The status of this backup (and others) can be viewed with the velero backup get command shown above. Tip Ad-hoc backups will have the same time-to-live as the configured schedule backups (default = 7 days). To change this, pass the --ttl <hours> option to the velero backup create command.","title":"Performing ad-hoc backups"},{"location":"configuration/15-disaster-recovery/#modifying-the-backup-schedule","text":"The following config options are available for modifying the regular backup schedule: environments/my-site/inventory/group_vars/all/variables.yml # Whether or not to perform scheduled backups velero_backup_schedule_enabled : true # Name for backup schedule kubernetes resource velero_backup_schedule_name : default # Schedule to use for backups (defaults to every day at midnight) # See https://en.wikipedia.org/wiki/Cron for format options velero_backup_schedule : \"0 0 * * *\" # Time-to-live for existing backups (defaults to 1 week) # See https://pkg.go.dev/time#ParseDuration for duration format options velero_backup_ttl : \"168h\" Note Setting velero_backup_schedule_enabled: false does not prevent the backup schedule from being installed - instead it sets the schedule state to paused . This allows for ad-hoc backups to still be run on demand using the configured backup parameters.","title":"Modifying the backup schedule"},{"location":"configuration/16-local-customisations/","text":"Local customisations Azimuth allows a few site-specific customisations to be made to the user interface, if required. Documentation link The Azimuth UI includes a documentation link in the navigation bar at the top of the page. By default, this link points to the generic Azimuth user documentation that covers usage of the reference appliances. However it is recommended to change this link to point at local documentation that is specific to your site, where possible. This documentation can include additional information, e.g. how to get an account to use with Azimuth, which is out-of-scope for the generic documentation. To change the documentation link, use the following variable: environments/my-site/inventory/group_vars/all/variables.yml azimuth_documentation_url : https://docs.example.org/azimuth Theming The Azimuth UI is built using the Bootstrap frontend toolkit , which provides a grid system and several built-in components. Bootstrap is built to be customisable - please consult the Bootstrap documentation for more information on how to do this. Several websites also provide free and paid themes for Bootstrap - by default, Azimuth uses the Pulse theme from the Bootswatch project . Replacing the Bootstrap theme It is possible to replace the Bootstrap theme completely by pointing to a different compiled CSS file. For example, the following configuration tells Azimuth to use the Zephyr theme from Bootswatch : environments/my-site/inventory/group_vars/all/variables.yml azimuth_theme_bootstrap_css_url : https://bootswatch.com/5/zephyr/bootstrap.css Tip In order for the theming changes to take effect you may need to do a hard refresh of the page due to the aggressive nature of CSS caching. Mac: \u21e7 Shift + \u2318 Command + R Windows: ctrl + \u21e7 Shift + R / ctrl + F5 Injecting custom CSS In addition to replacing the entire theme, Azimuth also allows custom CSS to be injected. This can be useful for applying small tweaks, or making modifications to the Azimuth UI that are not part of your chosen theme. In particular, custom CSS can be used to add a logo to the navigation bar. For example, the following snippet adds the Azimuth logo to the navigation bar instead of the cloud label: environments/my-site/inventory/group_vars/all/variables.yml azimuth_theme_custom_css : | .navbar-brand { background-size: auto 100%; background-repeat: no-repeat; text-indent: -9999px; background-image: url('https://raw.githubusercontent.com/stackhpc/azimuth/master/branding/azimuth-logo-white-text.png'); height: 60px; width: 220px; } Tip The image must already be available somewhere on the internet - Azimuth does not currently have support for hosting the logo itself. The height and width should be adjusted to match the aspect ratio of your logo and the desired size in the Azimuth UI. Warning If you are using the default Pulse theme, make sure to include the following at the top of your custom CSS: @ import url (/ pulse-overrides . css ) ; This is because Azimuth has some Pulse-specific tweaks that you will need to keep. For more details, see the CSS file , which has comments indicating why these are necessary.","title":"Local customisations"},{"location":"configuration/16-local-customisations/#local-customisations","text":"Azimuth allows a few site-specific customisations to be made to the user interface, if required.","title":"Local customisations"},{"location":"configuration/16-local-customisations/#documentation-link","text":"The Azimuth UI includes a documentation link in the navigation bar at the top of the page. By default, this link points to the generic Azimuth user documentation that covers usage of the reference appliances. However it is recommended to change this link to point at local documentation that is specific to your site, where possible. This documentation can include additional information, e.g. how to get an account to use with Azimuth, which is out-of-scope for the generic documentation. To change the documentation link, use the following variable: environments/my-site/inventory/group_vars/all/variables.yml azimuth_documentation_url : https://docs.example.org/azimuth","title":"Documentation link"},{"location":"configuration/16-local-customisations/#theming","text":"The Azimuth UI is built using the Bootstrap frontend toolkit , which provides a grid system and several built-in components. Bootstrap is built to be customisable - please consult the Bootstrap documentation for more information on how to do this. Several websites also provide free and paid themes for Bootstrap - by default, Azimuth uses the Pulse theme from the Bootswatch project .","title":"Theming"},{"location":"configuration/16-local-customisations/#replacing-the-bootstrap-theme","text":"It is possible to replace the Bootstrap theme completely by pointing to a different compiled CSS file. For example, the following configuration tells Azimuth to use the Zephyr theme from Bootswatch : environments/my-site/inventory/group_vars/all/variables.yml azimuth_theme_bootstrap_css_url : https://bootswatch.com/5/zephyr/bootstrap.css Tip In order for the theming changes to take effect you may need to do a hard refresh of the page due to the aggressive nature of CSS caching. Mac: \u21e7 Shift + \u2318 Command + R Windows: ctrl + \u21e7 Shift + R / ctrl + F5","title":"Replacing the Bootstrap theme"},{"location":"configuration/16-local-customisations/#injecting-custom-css","text":"In addition to replacing the entire theme, Azimuth also allows custom CSS to be injected. This can be useful for applying small tweaks, or making modifications to the Azimuth UI that are not part of your chosen theme. In particular, custom CSS can be used to add a logo to the navigation bar. For example, the following snippet adds the Azimuth logo to the navigation bar instead of the cloud label: environments/my-site/inventory/group_vars/all/variables.yml azimuth_theme_custom_css : | .navbar-brand { background-size: auto 100%; background-repeat: no-repeat; text-indent: -9999px; background-image: url('https://raw.githubusercontent.com/stackhpc/azimuth/master/branding/azimuth-logo-white-text.png'); height: 60px; width: 220px; } Tip The image must already be available somewhere on the internet - Azimuth does not currently have support for hosting the logo itself. The height and width should be adjusted to match the aspect ratio of your logo and the desired size in the Azimuth UI. Warning If you are using the default Pulse theme, make sure to include the following at the top of your custom CSS: @ import url (/ pulse-overrides . css ) ; This is because Azimuth has some Pulse-specific tweaks that you will need to keep. For more details, see the CSS file , which has comments indicating why these are necessary.","title":"Injecting custom CSS"},{"location":"debugging/","text":"Debugging Azimuth This section provides tips on debugging some common issues with the components of an Azimuth installation. All of these debugging tips assume that you have activated the environment that you want to debug using: source ./bin/activate my-site","title":"Debugging Azimuth"},{"location":"debugging/#debugging-azimuth","text":"This section provides tips on debugging some common issues with the components of an Azimuth installation. All of these debugging tips assume that you have activated the environment that you want to debug using: source ./bin/activate my-site","title":"Debugging Azimuth"},{"location":"debugging/access-ha/","text":"Accessing the HA cluster HA clusters are provisioned using Cluster API on the K3s node. The Kubernetes API of the HA cluster is not accessible to the internet, so the K3s node is used to access it. On the K3s node, a kubeconfig file for the HA cluster is created in the $HOME directory of the ubuntu user. You can activate this kubeconfig by setting the KUBECONFIG environment variable, which allows you to access the HA cluster using kubectl : $ ./bin/seed-ssh ubuntu@azimuth-staging-seed:~$ export KUBECONFIG=./kubeconfig-azimuth-staging.yaml ubuntu@azimuth-staging-seed:~$ kubectl get po -n azimuth NAME READY STATUS RESTARTS AGE azimuth-api-6847fcd6c8-746pc 1/1 Running 0 55m azimuth-caas-operator-ara-79bcd7c5dd-k6zf5 1/1 Running 0 55m azimuth-caas-operator-7d55dddb5b-pm69d 1/1 Running 0 55m azimuth-capi-operator-metrics-57bbdfd8f4-9rqx4 1/1 Running 0 55m azimuth-capi-operator-reloader-78f879c866-k4r5w 1/1 Running 0 55m azimuth-capi-operator-789b5c8f44-2g4wx 1/1 Running 0 55m azimuth-identity-operator-778c87548b-dkcpn 1/1 Running 0 55m azimuth-ui-fc556-cjwq2 1/1 Running 0 55m consul-client-8zbnx 1/1 Running 0 55m consul-client-kf8fl 1/1 Running 0 55m consul-client-n4w2x 1/1 Running 0 55m consul-exporter-prometheus-consul-exporter-74bdc5dbc7-l7cb7 1/1 Running 0 55m consul-server-0 1/1 Running 0 55m consul-server-1 1/1 Running 0 55m consul-server-2 1/1 Running 0 55m zenith-registrar-86df769979-z2cgb 1/1 Running 0 55m zenith-sshd-768794b88b-48rcf 1/1 Running 0 55m zenith-sync-8f55f978c-v6czg 1/1 Running 0 55m","title":"Accessing the HA cluster"},{"location":"debugging/access-ha/#accessing-the-ha-cluster","text":"HA clusters are provisioned using Cluster API on the K3s node. The Kubernetes API of the HA cluster is not accessible to the internet, so the K3s node is used to access it. On the K3s node, a kubeconfig file for the HA cluster is created in the $HOME directory of the ubuntu user. You can activate this kubeconfig by setting the KUBECONFIG environment variable, which allows you to access the HA cluster using kubectl : $ ./bin/seed-ssh ubuntu@azimuth-staging-seed:~$ export KUBECONFIG=./kubeconfig-azimuth-staging.yaml ubuntu@azimuth-staging-seed:~$ kubectl get po -n azimuth NAME READY STATUS RESTARTS AGE azimuth-api-6847fcd6c8-746pc 1/1 Running 0 55m azimuth-caas-operator-ara-79bcd7c5dd-k6zf5 1/1 Running 0 55m azimuth-caas-operator-7d55dddb5b-pm69d 1/1 Running 0 55m azimuth-capi-operator-metrics-57bbdfd8f4-9rqx4 1/1 Running 0 55m azimuth-capi-operator-reloader-78f879c866-k4r5w 1/1 Running 0 55m azimuth-capi-operator-789b5c8f44-2g4wx 1/1 Running 0 55m azimuth-identity-operator-778c87548b-dkcpn 1/1 Running 0 55m azimuth-ui-fc556-cjwq2 1/1 Running 0 55m consul-client-8zbnx 1/1 Running 0 55m consul-client-kf8fl 1/1 Running 0 55m consul-client-n4w2x 1/1 Running 0 55m consul-exporter-prometheus-consul-exporter-74bdc5dbc7-l7cb7 1/1 Running 0 55m consul-server-0 1/1 Running 0 55m consul-server-1 1/1 Running 0 55m consul-server-2 1/1 Running 0 55m zenith-registrar-86df769979-z2cgb 1/1 Running 0 55m zenith-sshd-768794b88b-48rcf 1/1 Running 0 55m zenith-sync-8f55f978c-v6czg 1/1 Running 0 55m","title":"Accessing the HA cluster"},{"location":"debugging/access-k3s/","text":"Accessing the K3s cluster Both the single node and high-availability (HA) deployment methods have a K3s node that is provisioned using Terraform. In the single node case, this is the cluster that actually hosts Azimuth and all its dependencies. In the HA case, this cluster is configured as a Cluster API management cluster for the HA cluster that actually runs Azimuth. In both cases, the K3s node is deployed using Terraform and the IP address and SSH key for accessing the node are in the Terraform state for the environment. The azimuth-config repository contains a utility script - seed-ssh - that will extract these details from the Terraform state for the active environment and use them to execute an SSH command to access the provisioned node. Accessing an environment deployed using automation If you are using seed-ssh to access an environment that is deployed using automation , you still need to make sure that the Python and Ansible dependencies are installed in order for the script to work: # Ensure that the Python venv is set up ./bin/ensure-venv # Activate the target environment source ./bin/activate my-site # Install Ansible dependencies ansible-galaxy install -f -r requirements.yml # Execute the seed-ssh script ./bin/seed-ssh Once on the node, you can use kubectl to inspect the state of the Kubernetes cluster. It is already configured with the correct kubeconfig file: $ ./bin/seed-ssh ubuntu@azimuth-staging-seed:~$ kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7796b77cd4-mk9gw 1/1 Running 0 2d kube-system metrics-server-ff9dbcb6c-nbpjt 1/1 Running 0 2d cert-manager cert-manager-webhook-5fd7d458f7-scwd8 1/1 Running 0 2d kube-system local-path-provisioner-84bb864455-r4vd2 1/1 Running 0 2d cert-manager cert-manager-66b6d6bf59-vzqrq 1/1 Running 0 2d capi-system capi-controller-manager-7f45d4b75b-47cf4 1/1 Running 0 2d capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-5f5d7fb49-684n9 1/1 Running 0 2d cert-manager cert-manager-cainjector-856d4df858-bgs4d 1/1 Running 0 2d capo-system capo-controller-manager-5c9748574f-vnbzp 1/1 Running 0 2d capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-c444455b5-zn4qw 1/1 Running 0 2d default azimuth-staging-addons-cloud-config-install-9eeff--1-694x4 0/1 Completed 0 2d default azimuth-staging-addons-cni-calico-install-0d58f--1-cjwzx 0/1 Completed 0 2d default azimuth-staging-addons-ccm-openstack-install-bc2b0--1-jlb4b 0/1 Completed 0 2d default azimuth-staging-addons-prometheus-operator-crds-instal--1-sn66s 0/1 Completed 0 2d default azimuth-staging-addons-metrics-server-install-45390--1-rr5m6 0/1 Completed 0 2d default azimuth-staging-addons-csi-cinder-install-53888--1-cg9gq 0/1 Completed 0 2d default azimuth-staging-addons-ingress-nginx-install-478f3--1-p4hwb 0/1 Completed 0 2d default azimuth-staging-addons-loki-stack-install-c8cd1--1-5z2wc 0/1 Completed 0 2d default azimuth-staging-addons-kube-prometheus-stack-install-4--1-xgmvd 0/1 Completed 0 2d default azimuth-staging-autoscaler-66cc55487c-qfpgn 1/1 Running 0 2d","title":"Accessing the K3s cluster"},{"location":"debugging/access-k3s/#accessing-the-k3s-cluster","text":"Both the single node and high-availability (HA) deployment methods have a K3s node that is provisioned using Terraform. In the single node case, this is the cluster that actually hosts Azimuth and all its dependencies. In the HA case, this cluster is configured as a Cluster API management cluster for the HA cluster that actually runs Azimuth. In both cases, the K3s node is deployed using Terraform and the IP address and SSH key for accessing the node are in the Terraform state for the environment. The azimuth-config repository contains a utility script - seed-ssh - that will extract these details from the Terraform state for the active environment and use them to execute an SSH command to access the provisioned node. Accessing an environment deployed using automation If you are using seed-ssh to access an environment that is deployed using automation , you still need to make sure that the Python and Ansible dependencies are installed in order for the script to work: # Ensure that the Python venv is set up ./bin/ensure-venv # Activate the target environment source ./bin/activate my-site # Install Ansible dependencies ansible-galaxy install -f -r requirements.yml # Execute the seed-ssh script ./bin/seed-ssh Once on the node, you can use kubectl to inspect the state of the Kubernetes cluster. It is already configured with the correct kubeconfig file: $ ./bin/seed-ssh ubuntu@azimuth-staging-seed:~$ kubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-7796b77cd4-mk9gw 1/1 Running 0 2d kube-system metrics-server-ff9dbcb6c-nbpjt 1/1 Running 0 2d cert-manager cert-manager-webhook-5fd7d458f7-scwd8 1/1 Running 0 2d kube-system local-path-provisioner-84bb864455-r4vd2 1/1 Running 0 2d cert-manager cert-manager-66b6d6bf59-vzqrq 1/1 Running 0 2d capi-system capi-controller-manager-7f45d4b75b-47cf4 1/1 Running 0 2d capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-5f5d7fb49-684n9 1/1 Running 0 2d cert-manager cert-manager-cainjector-856d4df858-bgs4d 1/1 Running 0 2d capo-system capo-controller-manager-5c9748574f-vnbzp 1/1 Running 0 2d capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-c444455b5-zn4qw 1/1 Running 0 2d default azimuth-staging-addons-cloud-config-install-9eeff--1-694x4 0/1 Completed 0 2d default azimuth-staging-addons-cni-calico-install-0d58f--1-cjwzx 0/1 Completed 0 2d default azimuth-staging-addons-ccm-openstack-install-bc2b0--1-jlb4b 0/1 Completed 0 2d default azimuth-staging-addons-prometheus-operator-crds-instal--1-sn66s 0/1 Completed 0 2d default azimuth-staging-addons-metrics-server-install-45390--1-rr5m6 0/1 Completed 0 2d default azimuth-staging-addons-csi-cinder-install-53888--1-cg9gq 0/1 Completed 0 2d default azimuth-staging-addons-ingress-nginx-install-478f3--1-p4hwb 0/1 Completed 0 2d default azimuth-staging-addons-loki-stack-install-c8cd1--1-5z2wc 0/1 Completed 0 2d default azimuth-staging-addons-kube-prometheus-stack-install-4--1-xgmvd 0/1 Completed 0 2d default azimuth-staging-autoscaler-66cc55487c-qfpgn 1/1 Running 0 2d","title":"Accessing the K3s cluster"},{"location":"debugging/access-monitoring/","text":"Accessing the monitoring As discussed in Monitoring and alerting , the monitoring dashboards are exposed as subdomains under ingress_base_domain and protected by a username and password. Grafana Grafana is accessed as grafana.<ingress base domain> , e.g. grafana.azimuth.example.org , and can be used to access various dashboards showing the health of the Azimuth installation and its underlying Kubernetes cluster. For example, there are dashboards for resource usage, network traffic, etcd, tenant Kubernetes and CaaS clusters, Zenith services, pod logs and systemd logs. Prometheus Prometheus is accessed as prometheus.<ingress base domain> , and can be used to browse the configured alerts and see which are firing or pending. It can also be used to make ad-hoc queries of the metrics for the installation. Alertmanager Alertmanager is accessed as alertmanager.<ingress base domain> , and can be used to manage the firing alerts and configure silences if required. Kubernetes dashboard The Kubernetes dashboard is accessed as kubernetes.<ingress base domain> , and can be used to browse the current state of Kubernetes resources in the cluster. This includes streaming the logs of current pods. Helm dashboard The Helm dashboard is accessed as helm.<ingress base domain> , and can be used to browse the current state of the Helm releases on the cluster. The dashboard does also attempt to infer the health of the resources deployed by Helm, however this does sometimes report false-positives.","title":"Accessing the monitoring"},{"location":"debugging/access-monitoring/#accessing-the-monitoring","text":"As discussed in Monitoring and alerting , the monitoring dashboards are exposed as subdomains under ingress_base_domain and protected by a username and password.","title":"Accessing the monitoring"},{"location":"debugging/access-monitoring/#grafana","text":"Grafana is accessed as grafana.<ingress base domain> , e.g. grafana.azimuth.example.org , and can be used to access various dashboards showing the health of the Azimuth installation and its underlying Kubernetes cluster. For example, there are dashboards for resource usage, network traffic, etcd, tenant Kubernetes and CaaS clusters, Zenith services, pod logs and systemd logs.","title":"Grafana"},{"location":"debugging/access-monitoring/#prometheus","text":"Prometheus is accessed as prometheus.<ingress base domain> , and can be used to browse the configured alerts and see which are firing or pending. It can also be used to make ad-hoc queries of the metrics for the installation.","title":"Prometheus"},{"location":"debugging/access-monitoring/#alertmanager","text":"Alertmanager is accessed as alertmanager.<ingress base domain> , and can be used to manage the firing alerts and configure silences if required.","title":"Alertmanager"},{"location":"debugging/access-monitoring/#kubernetes-dashboard","text":"The Kubernetes dashboard is accessed as kubernetes.<ingress base domain> , and can be used to browse the current state of Kubernetes resources in the cluster. This includes streaming the logs of current pods.","title":"Kubernetes dashboard"},{"location":"debugging/access-monitoring/#helm-dashboard","text":"The Helm dashboard is accessed as helm.<ingress base domain> , and can be used to browse the current state of the Helm releases on the cluster. The dashboard does also attempt to infer the health of the resources deployed by Helm, however this does sometimes report false-positives.","title":"Helm dashboard"},{"location":"debugging/caas/","text":"Debugging Cluster-as-a-Service As described in configuring CaaS , Azimuth uses the Azimuth CaaS operator to manage clusters. The Azimuth API creates a namespace for each project in which instances of the CaaS CRDs are created in order to create tenant clusters. These namespaces are of the form caas-<sanitized project id> . When issues occur with cluster provisioning, here are some things to try in order to locate the issue. CRDs installed and operator running First, check that the CaaS CRDs have been registered: On the K3s node, targetting the HA cluster if deployed $ kubectl get crd | grep caas clusters.caas.azimuth.stackhpc.com 2023-07-03T13:33:28Z clustertypes.caas.azimuth.stackhpc.com 2023-07-03T13:33:28Z If they do not exist, check if the azimuth-caas-operator is running: On the K3s node, targetting the HA cluster if deployed $ kubectl -n azimuth get po -l app.kubernetes.io/instance=azimuth-caas-operator NAME READY STATUS RESTARTS AGE azimuth-caas-operator-ara-79bcd7c5dd-k6zf5 1/1 Running 0 18m azimuth-caas-operator-7d55dddb5b-pm69d 1/1 Running 0 18m Cluster resource exists Next, check if the cluster resource exists in the tenant namespace: On the K3s node, targetting the HA cluster if deployed $ kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx get cluster.caas NAME AGE demo-ws 7m45s demo-slurm 7m26s Check the status of the cluster you want to debug: On the K3s node, targetting the HA cluster if deployed $ kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx describe cluster.caas demo-ws Name: demo-ws Namespace: caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx ... Check if jobs were scheduled The Azimuth CaaS operator schedules Kubernetes jobs that use ansible-runner to execute the required Ansible. If the operator is functioning properly, you should see these jobs being created: On the K3s node, targetting the HA cluster if deployed $ kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx get job NAME COMPLETIONS DURATION AGE demo-slurm-create-pwqmh 0/1 14m 14m demo-ws-create-9nx96 1/1 4m36s 14m If the jobs are not being scheduled, check the logs of the CaaS operator to see what is stopping them from being created: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deploy/azimuth-caas-operator [ -f ] If you need to restart the operator, you can use the following command: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deploy/azimuth-caas-operator If the jobs are being scheduled, you can also check the pods that are created to see if there are issues: On the K3s node, targetting the HA cluster if deployed $ kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx get po NAME READY STATUS RESTARTS AGE demo-ws-create-9nx96-m2l97 0/1 Completed 0 21m demo-slurm-create-pwqmh-jsbmk 0/1 Completed 0 20m If any of the pods are getting stuck in the init phase, check the logs of the init containers to see if there are issues checking out the appliance code or installing the Ansible dependencies: On the K3s node, targetting the HA cluster if deployed kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx logs demo-ws-create-9nx96-m2l97 [ -c [ inventory | clone ]] Check Ansible output Azimuth includes a deployment of ARA Records Ansible (ARA) that is used to record Ansible playbook executions as they are run by the CaaS operator. If the job is getting as far as starting to run Ansible, then ARA is a much easier way to debug the Ansible for an appliance than wading through the Ansible logs from the job. As discussed in Monitoring and alerting , the ARA web interface is exposed as ara.<ingress base domain> , e.g. ara.azimuth.example.org , and is protected by a username and password. Once inside, you can look at the details of the recently executed jobs, see which tasks failed and what variables were set at the time.","title":"Debugging Cluster-as-a-Service"},{"location":"debugging/caas/#debugging-cluster-as-a-service","text":"As described in configuring CaaS , Azimuth uses the Azimuth CaaS operator to manage clusters. The Azimuth API creates a namespace for each project in which instances of the CaaS CRDs are created in order to create tenant clusters. These namespaces are of the form caas-<sanitized project id> . When issues occur with cluster provisioning, here are some things to try in order to locate the issue.","title":"Debugging Cluster-as-a-Service"},{"location":"debugging/caas/#crds-installed-and-operator-running","text":"First, check that the CaaS CRDs have been registered: On the K3s node, targetting the HA cluster if deployed $ kubectl get crd | grep caas clusters.caas.azimuth.stackhpc.com 2023-07-03T13:33:28Z clustertypes.caas.azimuth.stackhpc.com 2023-07-03T13:33:28Z If they do not exist, check if the azimuth-caas-operator is running: On the K3s node, targetting the HA cluster if deployed $ kubectl -n azimuth get po -l app.kubernetes.io/instance=azimuth-caas-operator NAME READY STATUS RESTARTS AGE azimuth-caas-operator-ara-79bcd7c5dd-k6zf5 1/1 Running 0 18m azimuth-caas-operator-7d55dddb5b-pm69d 1/1 Running 0 18m","title":"CRDs installed and operator running"},{"location":"debugging/caas/#cluster-resource-exists","text":"Next, check if the cluster resource exists in the tenant namespace: On the K3s node, targetting the HA cluster if deployed $ kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx get cluster.caas NAME AGE demo-ws 7m45s demo-slurm 7m26s Check the status of the cluster you want to debug: On the K3s node, targetting the HA cluster if deployed $ kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx describe cluster.caas demo-ws Name: demo-ws Namespace: caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx ...","title":"Cluster resource exists"},{"location":"debugging/caas/#check-if-jobs-were-scheduled","text":"The Azimuth CaaS operator schedules Kubernetes jobs that use ansible-runner to execute the required Ansible. If the operator is functioning properly, you should see these jobs being created: On the K3s node, targetting the HA cluster if deployed $ kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx get job NAME COMPLETIONS DURATION AGE demo-slurm-create-pwqmh 0/1 14m 14m demo-ws-create-9nx96 1/1 4m36s 14m If the jobs are not being scheduled, check the logs of the CaaS operator to see what is stopping them from being created: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deploy/azimuth-caas-operator [ -f ] If you need to restart the operator, you can use the following command: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deploy/azimuth-caas-operator If the jobs are being scheduled, you can also check the pods that are created to see if there are issues: On the K3s node, targetting the HA cluster if deployed $ kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx get po NAME READY STATUS RESTARTS AGE demo-ws-create-9nx96-m2l97 0/1 Completed 0 21m demo-slurm-create-pwqmh-jsbmk 0/1 Completed 0 20m If any of the pods are getting stuck in the init phase, check the logs of the init containers to see if there are issues checking out the appliance code or installing the Ansible dependencies: On the K3s node, targetting the HA cluster if deployed kubectl -n caas-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx logs demo-ws-create-9nx96-m2l97 [ -c [ inventory | clone ]]","title":"Check if jobs were scheduled"},{"location":"debugging/caas/#check-ansible-output","text":"Azimuth includes a deployment of ARA Records Ansible (ARA) that is used to record Ansible playbook executions as they are run by the CaaS operator. If the job is getting as far as starting to run Ansible, then ARA is a much easier way to debug the Ansible for an appliance than wading through the Ansible logs from the job. As discussed in Monitoring and alerting , the ARA web interface is exposed as ara.<ingress base domain> , e.g. ara.azimuth.example.org , and is protected by a username and password. Once inside, you can look at the details of the recently executed jobs, see which tasks failed and what variables were set at the time.","title":"Check Ansible output"},{"location":"debugging/consul/","text":"Debugging Consul Info Consul is installed using upstream Helm charts . Documentation and bug reports supplied upstream can sometimes provide helpful troubleshooting steps for issues outside the scope of this documentation. Warning Consul cluster failures are most common during and after network interruptions or activities which cause HA cluster nodes to be created or destroyed, such as: Upgrading the HA cluster to a new Kubernetes version (which requires a new image). An auto-healing event has replaced a HA cluster member which was determined to be in a down state. Consul failures are generally confined to two failure modes: Failing leader elections This failure mode is characterised by an increase in leader election events, which may trigger Alertmanager alerts . Additionally, log messages in the following format are generated by consul-server : 2023-04-19T14:39:46.896Z [ERROR] agent.anti_entropy: failed to sync remote state: error=\"No cluster leader\" 2023-04-19T14:39:47.736Z [INFO] agent.server: New leader elected: payload=consul-server-1 2023-04-19T14:39:51.495Z [WARN] agent.server.raft: Election timeout reached, restarting election 2023-04-19T14:39:51.495Z [INFO] agent.server.raft: entering candidate state: node=\"Node at 172.16.189.31:8300 [Candidate]\" term=598 View the consul-server logs using the following command: On the K3s node, targetting the HA cluster kubectl -n azimuth logs statefulsets/consul-server To remedy the errors, issue a rolling restart the Consul cluster: On the K3s node, targetting the HA cluster kubectl -n azimuth rollout restart daemonset/consul-client kubectl -n azimuth rollout restart sts/consul-server Check consul-server logs for leader elections that fail: On the K3s node, targetting the HA cluster kubectl -n azimuth logs statefulsets/consul-server Delete any pods that fail to become leader: On the K3s node, targetting the HA cluster kubectl -n azimuth delete pod $FAILING_CONSUL_SERVER For the example logs given above, the correct command would be: On the K3s node, targetting the HA cluster kubectl -n azimuth delete pod consul-server-1 Wait for all consul pods to become ready: On the K3s node, targetting the HA cluster kubectl get -n azimuth po -l app = consul NAME READY STATUS RESTARTS AGE consul-server-0 1 /1 Running 0 12m consul-server-1 1 /1 Running 0 11m consul-server-2 1 /1 Running 0 11m consul-client-9x6k7 1 /1 Running 0 13m consul-client-9e87d 1 /1 Running 0 12m consul-client-f73c3 1 /1 Running 0 12m After removing misbehaving clients, consul-server should log no futher [ERROR] messages related to failed leader elections, and Zenith Sync should begin to reconcile services, which can be monitored as descibed in the Debugging Zenith services section . Clients registering with an existing name but new IP address This failure mode is characterised by an increase in RPC errors reported by Consul, which may trigger Alertmanager alerts . Additionally, log messages in the following format are generated by consul-server : 2023-08-04T09:04:45.998Z [ERROR] agent.client: RPC failed to server: method=Catalog.Register server=172.23.196.11:8300 error=\"rpc error making call: rpc error making call: failed inserting node: Error while renaming Node ID: \\\"deadbeef-dead-beef-dead-beefdeadbee4\\\": Node name azimuth-env1-md-0-9aacb97c-ggdnb is reserved by node deadbeef-0000-0000-0000-000000000000 with name azimuth-env1-md-0-9aacb97c-ggdnb (172.17.195.48)\" 2023-08-04T09:04:45.998Z [WARN] agent: Syncing node info failed.: error=\"rpc error making call: rpc error making call: failed inserting node: Error while renaming Node ID: \\\"deadbeef-dead-beef-dead-beefdeadbee4\\\": Node name azimuth-env1-md-0-9aacb97c-ggdnb is reserved by node deadbeef-0000-0000-0000-000000000000 with name azimuth-env1-md-0-9aacb97c-ggdnb (172.17.195.48)\" 2023-08-04T09:04:45.998Z [ERROR] agent.anti_entropy: failed to sync remote state: error=\"rpc error making call: rpc error making call: failed inserting node: Error while renaming Node ID: \\\"deadbeef-dead-beef-dead-beefdeadbee4\\\": Node name azimuth-env1-md-0-9aacb97c-ggdnb is reserved by node deadbeef-0000-0000-0000-000000000000 with name azimuth-env1-md-0-9aacb97c-ggdnb (172.17.195.48)\" View the consul-server logs using the following command: On the K3s node, targetting the HA cluster kubectl -n azimuth logs statefulsets/consul-server To remedy the errors, remove the misbehaving client from the Consul cluster. The client will reattempt registration. On the K3s node, targetting the HA cluster kubectl -n azimuth exec consul-server-0 -- curl --request PUT --data '{\"Node\":\"$NODENAME\"}' -v http://localhost:8500/v1/catalog/deregister For the example logs given above, the correct command would be: On the K3s node, targetting the HA cluster kubectl -n azimuth exec consul-server-0 -- curl --request PUT --data '{\"Node\":\"azimuth-env1-md-0-9aacb97c-ggdnb\"}' -v http://localhost:8500/v1/catalog/deregister This process may need to be repeated multiple times, for each node whose node name appears in the consul-server log. After removing misbehaving clients, consul-server should log no futher [ERROR] messages related to client RPC calls, and Zenith Sync should begin to reconcile services, which can be monitored as descibed in the Debugging Zenith services section .","title":"Debugging Consul"},{"location":"debugging/consul/#debugging-consul","text":"Info Consul is installed using upstream Helm charts . Documentation and bug reports supplied upstream can sometimes provide helpful troubleshooting steps for issues outside the scope of this documentation. Warning Consul cluster failures are most common during and after network interruptions or activities which cause HA cluster nodes to be created or destroyed, such as: Upgrading the HA cluster to a new Kubernetes version (which requires a new image). An auto-healing event has replaced a HA cluster member which was determined to be in a down state. Consul failures are generally confined to two failure modes:","title":"Debugging Consul"},{"location":"debugging/consul/#failing-leader-elections","text":"This failure mode is characterised by an increase in leader election events, which may trigger Alertmanager alerts . Additionally, log messages in the following format are generated by consul-server : 2023-04-19T14:39:46.896Z [ERROR] agent.anti_entropy: failed to sync remote state: error=\"No cluster leader\" 2023-04-19T14:39:47.736Z [INFO] agent.server: New leader elected: payload=consul-server-1 2023-04-19T14:39:51.495Z [WARN] agent.server.raft: Election timeout reached, restarting election 2023-04-19T14:39:51.495Z [INFO] agent.server.raft: entering candidate state: node=\"Node at 172.16.189.31:8300 [Candidate]\" term=598 View the consul-server logs using the following command: On the K3s node, targetting the HA cluster kubectl -n azimuth logs statefulsets/consul-server To remedy the errors, issue a rolling restart the Consul cluster: On the K3s node, targetting the HA cluster kubectl -n azimuth rollout restart daemonset/consul-client kubectl -n azimuth rollout restart sts/consul-server Check consul-server logs for leader elections that fail: On the K3s node, targetting the HA cluster kubectl -n azimuth logs statefulsets/consul-server Delete any pods that fail to become leader: On the K3s node, targetting the HA cluster kubectl -n azimuth delete pod $FAILING_CONSUL_SERVER For the example logs given above, the correct command would be: On the K3s node, targetting the HA cluster kubectl -n azimuth delete pod consul-server-1 Wait for all consul pods to become ready: On the K3s node, targetting the HA cluster kubectl get -n azimuth po -l app = consul NAME READY STATUS RESTARTS AGE consul-server-0 1 /1 Running 0 12m consul-server-1 1 /1 Running 0 11m consul-server-2 1 /1 Running 0 11m consul-client-9x6k7 1 /1 Running 0 13m consul-client-9e87d 1 /1 Running 0 12m consul-client-f73c3 1 /1 Running 0 12m After removing misbehaving clients, consul-server should log no futher [ERROR] messages related to failed leader elections, and Zenith Sync should begin to reconcile services, which can be monitored as descibed in the Debugging Zenith services section .","title":"Failing leader elections"},{"location":"debugging/consul/#clients-registering-with-an-existing-name-but-new-ip-address","text":"This failure mode is characterised by an increase in RPC errors reported by Consul, which may trigger Alertmanager alerts . Additionally, log messages in the following format are generated by consul-server : 2023-08-04T09:04:45.998Z [ERROR] agent.client: RPC failed to server: method=Catalog.Register server=172.23.196.11:8300 error=\"rpc error making call: rpc error making call: failed inserting node: Error while renaming Node ID: \\\"deadbeef-dead-beef-dead-beefdeadbee4\\\": Node name azimuth-env1-md-0-9aacb97c-ggdnb is reserved by node deadbeef-0000-0000-0000-000000000000 with name azimuth-env1-md-0-9aacb97c-ggdnb (172.17.195.48)\" 2023-08-04T09:04:45.998Z [WARN] agent: Syncing node info failed.: error=\"rpc error making call: rpc error making call: failed inserting node: Error while renaming Node ID: \\\"deadbeef-dead-beef-dead-beefdeadbee4\\\": Node name azimuth-env1-md-0-9aacb97c-ggdnb is reserved by node deadbeef-0000-0000-0000-000000000000 with name azimuth-env1-md-0-9aacb97c-ggdnb (172.17.195.48)\" 2023-08-04T09:04:45.998Z [ERROR] agent.anti_entropy: failed to sync remote state: error=\"rpc error making call: rpc error making call: failed inserting node: Error while renaming Node ID: \\\"deadbeef-dead-beef-dead-beefdeadbee4\\\": Node name azimuth-env1-md-0-9aacb97c-ggdnb is reserved by node deadbeef-0000-0000-0000-000000000000 with name azimuth-env1-md-0-9aacb97c-ggdnb (172.17.195.48)\" View the consul-server logs using the following command: On the K3s node, targetting the HA cluster kubectl -n azimuth logs statefulsets/consul-server To remedy the errors, remove the misbehaving client from the Consul cluster. The client will reattempt registration. On the K3s node, targetting the HA cluster kubectl -n azimuth exec consul-server-0 -- curl --request PUT --data '{\"Node\":\"$NODENAME\"}' -v http://localhost:8500/v1/catalog/deregister For the example logs given above, the correct command would be: On the K3s node, targetting the HA cluster kubectl -n azimuth exec consul-server-0 -- curl --request PUT --data '{\"Node\":\"azimuth-env1-md-0-9aacb97c-ggdnb\"}' -v http://localhost:8500/v1/catalog/deregister This process may need to be repeated multiple times, for each node whose node name appears in the consul-server log. After removing misbehaving clients, consul-server should log no futher [ERROR] messages related to client RPC calls, and Zenith Sync should begin to reconcile services, which can be monitored as descibed in the Debugging Zenith services section .","title":"Clients registering with an existing name but new IP address"},{"location":"debugging/kubernetes/","text":"Debugging Kubernetes As described in Configuring Kubernetes , Azimuth uses Cluster API to manage tenant Kubernetes clusters. Cluster API resources are managed by releases of the openstack-cluster Helm chart , which in turn are managed by the azimuth-capi-operator in response to changes to instances of the clusters.azimuth.stackhpc.com custom resource. These instances are created, updated and deleted in Kubernetes by the Azimuth API in response to user actions. The Azimuth API creates a namespace for each project, in which cluster resources are created. These namespaces are of the form az-<sanitized project name> . It is also important to note that the Kubernetes API servers for tenant clusters do not use Octavia load balancers like the Azimuth HA cluster. Instead, the API servers for tenant clusters are exposed via Zenith. When issues occur with cluster provisioning, here are some things to try in order to locate the issue. Cluster resource exists First, check if the cluster resource exists in the tenant namespace: On the K3s node, targetting the HA cluster if deployed $ kubectl -n az-demo get cluster NAME LABEL TEMPLATE KUBERNETES VERSION PHASE NODE COUNT AGE demo demo kube-1-24-2 1.24.2 Ready 4 11d If no cluster resource exists, check if the Kubernetes CRDs are installed: On the K3s node, targetting the HA cluster if deployed $ kubectl get crd | grep azimuth apptemplates.azimuth.stackhpc.com 2022-11-02T11:11:13Z clusters.azimuth.stackhpc.com 2022-11-02T10:53:26Z clustertemplates.azimuth.stackhpc.com 2022-11-02T10:53:26Z If they do not exist, check if the azimuth-capi-operator is running: On the K3s node, targetting the HA cluster if deployed $ kubectl -n azimuth get po -l app.kubernetes.io/instance=azimuth-capi-operator NAME READY STATUS RESTARTS AGE azimuth-capi-operator-5c65c4b598-h2thx 1/1 Running 0 10d Helm release exists The first thing that the azimuth-capi-operator does when it sees a new cluster resource is make a Helm release: On the K3s node, targetting the HA cluster if deployed $ helm -n az-demo list -a NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION demo az-demo 1 2022-07-07 13:26:22.94084961 +0000 UTC deployed openstack-cluster-0.1.0-dev.0.main.161 a0bcee5 If the Helm release does not exist, restart the azimuth-capi-operator : On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deploy/azimuth-capi-operator If the Helm release does not get created, even after a restart, check the logs of the azimuth-capi-operator for any warnings or errors: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deploy/azimuth-capi-operator [ -f ] Cluster API resource status If the Helm release is in the deployed status, the next thing to check is the state of the Cluster API resources that were created: On the K3s node, targetting the HA cluster if deployed $ kubectl -n az-demo get cluster-api NAME CLUSTER BOOTSTRAP TARGET NAMESPACE RELEASE NAME PHASE REVISION AGE manifests.addons.stackhpc.com/demo-cloud-config demo true openstack-system cloud-config Deployed 1 11d manifests.addons.stackhpc.com/demo-csi-cinder-storageclass demo true openstack-system csi-cinder-storageclass Deployed 1 11d manifests.addons.stackhpc.com/demo-kube-prometheus-stack-client demo true monitoring-system kube-prometheus-stack-client Deployed 2 11d manifests.addons.stackhpc.com/demo-kube-prometheus-stack-dashboards demo true monitoring-system kube-prometheus-stack-dashboards Deployed 1 11d manifests.addons.stackhpc.com/demo-kubernetes-dashboard-client demo true kubernetes-dashboard kubernetes-dashboard-client Deployed 2 11d manifests.addons.stackhpc.com/demo-loki-stack-dashboards demo true monitoring-system loki-stack-dashboards Deployed 1 11d NAME CLUSTER BOOTSTRAP TARGET NAMESPACE RELEASE NAME PHASE REVISION CHART NAME CHART VERSION AGE helmrelease.addons.stackhpc.com/dask-demo demo dask-demo dask-demo Deployed 1 daskhub-azimuth 0.1.0-dev.0.main.23 11d helmrelease.addons.stackhpc.com/demo-ccm-openstack demo true openstack-system ccm-openstack Deployed 1 openstack-cloud-controller-manager 1.3.0 11d helmrelease.addons.stackhpc.com/demo-cni-calico demo true tigera-operator cni-calico Deployed 1 tigera-operator v3.23.3 11d helmrelease.addons.stackhpc.com/demo-csi-cinder demo true openstack-system csi-cinder Deployed 1 openstack-cinder-csi 2.2.0 11d helmrelease.addons.stackhpc.com/demo-kube-prometheus-stack demo true monitoring-system kube-prometheus-stack Deployed 1 kube-prometheus-stack 40.1.0 11d helmrelease.addons.stackhpc.com/demo-kubernetes-dashboard demo true kubernetes-dashboard kubernetes-dashboard Deployed 1 kubernetes-dashboard 5.10.0 11d helmrelease.addons.stackhpc.com/demo-loki-stack demo true monitoring-system loki-stack Deployed 1 loki-stack 2.8.2 11d helmrelease.addons.stackhpc.com/demo-mellanox-network-operator demo true network-operator mellanox-network-operator Deployed 1 network-operator 1.3.0 11d helmrelease.addons.stackhpc.com/demo-metrics-server demo true kube-system metrics-server Deployed 1 metrics-server 3.8.2 11d helmrelease.addons.stackhpc.com/demo-node-feature-discovery demo true node-feature-discovery node-feature-discovery Deployed 1 node-feature-discovery 0.11.2 11d helmrelease.addons.stackhpc.com/demo-nvidia-gpu-operator demo true gpu-operator nvidia-gpu-operator Deployed 1 gpu-operator v1.11.1 11d NAME CLUSTER AGE kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-5kjjt demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-897r6 demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-x26zz demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-sm0-cc21616d-lddk6 demo 11d NAME AGE kubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/demo-sm0-cc21616d 11d NAME CLUSTER EXPECTEDMACHINES MAXUNHEALTHY CURRENTHEALTHY AGE machinehealthcheck.cluster.x-k8s.io/demo-control-plane demo 3 100% 3 11d machinehealthcheck.cluster.x-k8s.io/demo-sm0 demo 1 100% 1 11d NAME CLUSTER REPLICAS READY AVAILABLE AGE VERSION machineset.cluster.x-k8s.io/demo-sm0-c99fb7798 demo 1 1 1 11d v1.24.2 NAME CLUSTER REPLICAS READY UPDATED UNAVAILABLE PHASE AGE VERSION machinedeployment.cluster.x-k8s.io/demo-sm0 demo 1 1 1 0 Running 11d v1.24.2 NAME PHASE AGE VERSION cluster.cluster.x-k8s.io/demo Provisioned 11d NAME CLUSTER NODENAME PROVIDERID PHASE AGE VERSION machine.cluster.x-k8s.io/demo-control-plane-7p8zv demo demo-control-plane-7d76d0be-z6dm8 openstack:///f687f926-3cee-4550-91e5-32c2885708b0 Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-control-plane-9skvh demo demo-control-plane-7d76d0be-d2mcr openstack:///ea91f79a-8abb-4cb9-a2ea-8f772568e93c Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-control-plane-s8dhv demo demo-control-plane-7d76d0be-j64w6 openstack:///33a3a532-348a-4b93-ab19-d7d8cdb0daa4 Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-sm0-c99fb7798-qqk4j demo demo-sm0-7d76d0be-gdjwv openstack:///ef9ae59c-bf20-44e0-831f-3798d25b7a06 Running 11d v1.24.2 NAME CLUSTER INITIALIZED API SERVER AVAILABLE REPLICAS READY UPDATED UNAVAILABLE AGE VERSION kubeadmcontrolplane.controlplane.cluster.x-k8s.io/demo-control-plane demo true true 3 3 3 0 11d v1.24.2 NAME CLUSTER READY NETWORK SUBNET BASTION IP openstackcluster.infrastructure.cluster.x-k8s.io/demo demo true 4b6b2722-ee5b-40ec-8e52-a6610e14cc51 73e22c49-10b8-4763-af2f-4c0cce007c82 NAME CLUSTER INSTANCESTATE READY PROVIDERID MACHINE openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-d2mcr demo ACTIVE true openstack:///ea91f79a-8abb-4cb9-a2ea-8f772568e93c demo-control-plane-9skvh openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-j64w6 demo ACTIVE true openstack:///33a3a532-348a-4b93-ab19-d7d8cdb0daa4 demo-control-plane-s8dhv openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-z6dm8 demo ACTIVE true openstack:///f687f926-3cee-4550-91e5-32c2885708b0 demo-control-plane-7p8zv openstackmachine.infrastructure.cluster.x-k8s.io/demo-sm0-7d76d0be-gdjwv demo ACTIVE true openstack:///ef9ae59c-bf20-44e0-831f-3798d25b7a06 demo-sm0-c99fb7798-qqk4j NAME AGE openstackmachinetemplate.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be 11d openstackmachinetemplate.infrastructure.cluster.x-k8s.io/demo-sm0-7d76d0be 11d The cluster.cluster.x-k8s.io resource should be Provisioned , the machine.cluster.x-k8s.io resources should be Running with an associated NODENAME , the openstackmachine.infrastructure.cluster.x-k8s.io resources should be ACTIVE and the {manifests,helmrelease}.addons.stackhpc.com resources should all be Deployed . If this is not the case, first check the interactive console of the cluster nodes in Horizon to see if the nodes had any problems joining the cluster. Also check to see if the Zenith service for the API server was created correctly - once all control plane nodes have registered correctly the Endpoints resource for the service should have an entry for each control plane node (usually three). If these all look OK, check the logs of the Cluster API providers for any errors: On the K3s node, targetting the HA cluster if deployed kubectl -n capi-system logs deploy/capi-controller-manager kubectl -n capi-kubeadm-bootstrap-system logs deploy/capi-kubeadm-bootstrap-controller-manager kubectl -n capi-kubeadm-control-plane-system logs deploy/capi-kubeadm-control-plane-controller-manager kubectl -n capo-system logs deploy/capo-controller-manager kubectl -n capi-addon-system logs deploy/cluster-api-addon-provider Accessing tenant clusters The kubeconfigs for all tenant clusters are stored as secrets. First, you need to find the name and namespace of the cluster you want to debug. This can be seen from the list of clusters: On the K3s node, targetting the HA cluster if deployed $ kubectl get cluster -A Then, you can retrieve and decode the kubeconfig with the following: On the K3s node, targetting the HA cluster if deployed $ kubectl -n <namespace> get secret <clustername>-kubeconfig -o json | \\ jq -r '.data.value' | \\ base64 -d \\ > kubeconfig-tenant.yaml This can now be used by exporting the path to this file: On the K3s node, targetting the HA cluster if deployed $ export KUBECONFIG=kubeconfig-tenant.yaml Zenith service issues Zenith services are enabled on Kubernetes clusters using the Zenith operator . Each tenant Kubernetes cluster gets an instance of the operator that runs on the Azimuth cluster, where it can reach the Zenith registrar to allocate subdomains, but watches the tenant cluster for instances of the reservation.zenith.stackhpc.com and client.zenith.stackhpc.com custom resources. By creating instances of these resources in the tenant cluster, Kubernetes Services in the target cluster can be exposed via Zenith. If Zenith services are not becoming available for Kubernetes cluster services, first follow the procedure for debugging a Zenith service , including checking that the clients were created correctly and that the pods are running: Targetting the tenant cluster $ kubectl get zenith -A NAMESPACE NAME PHASE UPSTREAM SERVICE MITM ENABLED MITM AUTH AGE kubernetes-dashboard client.zenith.stackhpc.com/kubernetes-dashboard Available kubernetes-dashboard true ServiceAccount 4d19h monitoring-system client.zenith.stackhpc.com/kube-prometheus-stack Available kube-prometheus-stack-grafana true Basic 4d19h NAMESPACE NAME SECRET PHASE FQDN AGE kubernetes-dashboard reservation.zenith.stackhpc.com/kubernetes-dashboard kubernetes-dashboard-zenith-credential Ready mwqgcdrk77nva18uzcct3g7jlo7obi7zlbcgemuhk6nhk.azimuth.example.org 4d19h monitoring-system reservation.zenith.stackhpc.com/kube-prometheus-stack kube-prometheus-stack-zenith-credential Ready zovdsnnesww2hiw074mvufvcfgczfbd2yhmuhsf3p59xa.azimuth.example.org 4d19h $ kubectl get deploy,po -A -l app.kubernetes.io/managed-by=zenith-operator NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-dashboard deployment.apps/kubernetes-dashboard-zenith-client 1/1 1 1 4d19h monitoring-system deployment.apps/kube-prometheus-stack-zenith-client 1/1 1 1 4d19h NAMESPACE NAME READY STATUS RESTARTS AGE kubernetes-dashboard pod/kubernetes-dashboard-zenith-client-86c5fd9bd-2jfdb 2/2 Running 0 4d19h monitoring-system pod/kube-prometheus-stack-zenith-client-b9986579d-qgp82 2/2 Running 0 9h Tip The kubeconfig for a tenant cluster is available in a secret in the tenant namespace: On the K3s node, targetting the HA cluster if deployed $ kubectl -n az-demo get secret | grep kubeconfig demo-kubeconfig cluster.x-k8s.io/secret 1 11d If everything looks OK, try restarting the Zenith operator for the cluster: On the K3s node, targetting the HA cluster if deployed $ kubectl -n az-demo rollout restart deploy/demo-zenith-operator","title":"Debugging Kubernetes"},{"location":"debugging/kubernetes/#debugging-kubernetes","text":"As described in Configuring Kubernetes , Azimuth uses Cluster API to manage tenant Kubernetes clusters. Cluster API resources are managed by releases of the openstack-cluster Helm chart , which in turn are managed by the azimuth-capi-operator in response to changes to instances of the clusters.azimuth.stackhpc.com custom resource. These instances are created, updated and deleted in Kubernetes by the Azimuth API in response to user actions. The Azimuth API creates a namespace for each project, in which cluster resources are created. These namespaces are of the form az-<sanitized project name> . It is also important to note that the Kubernetes API servers for tenant clusters do not use Octavia load balancers like the Azimuth HA cluster. Instead, the API servers for tenant clusters are exposed via Zenith. When issues occur with cluster provisioning, here are some things to try in order to locate the issue.","title":"Debugging Kubernetes"},{"location":"debugging/kubernetes/#cluster-resource-exists","text":"First, check if the cluster resource exists in the tenant namespace: On the K3s node, targetting the HA cluster if deployed $ kubectl -n az-demo get cluster NAME LABEL TEMPLATE KUBERNETES VERSION PHASE NODE COUNT AGE demo demo kube-1-24-2 1.24.2 Ready 4 11d If no cluster resource exists, check if the Kubernetes CRDs are installed: On the K3s node, targetting the HA cluster if deployed $ kubectl get crd | grep azimuth apptemplates.azimuth.stackhpc.com 2022-11-02T11:11:13Z clusters.azimuth.stackhpc.com 2022-11-02T10:53:26Z clustertemplates.azimuth.stackhpc.com 2022-11-02T10:53:26Z If they do not exist, check if the azimuth-capi-operator is running: On the K3s node, targetting the HA cluster if deployed $ kubectl -n azimuth get po -l app.kubernetes.io/instance=azimuth-capi-operator NAME READY STATUS RESTARTS AGE azimuth-capi-operator-5c65c4b598-h2thx 1/1 Running 0 10d","title":"Cluster resource exists"},{"location":"debugging/kubernetes/#helm-release-exists","text":"The first thing that the azimuth-capi-operator does when it sees a new cluster resource is make a Helm release: On the K3s node, targetting the HA cluster if deployed $ helm -n az-demo list -a NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION demo az-demo 1 2022-07-07 13:26:22.94084961 +0000 UTC deployed openstack-cluster-0.1.0-dev.0.main.161 a0bcee5 If the Helm release does not exist, restart the azimuth-capi-operator : On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deploy/azimuth-capi-operator If the Helm release does not get created, even after a restart, check the logs of the azimuth-capi-operator for any warnings or errors: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deploy/azimuth-capi-operator [ -f ]","title":"Helm release exists"},{"location":"debugging/kubernetes/#cluster-api-resource-status","text":"If the Helm release is in the deployed status, the next thing to check is the state of the Cluster API resources that were created: On the K3s node, targetting the HA cluster if deployed $ kubectl -n az-demo get cluster-api NAME CLUSTER BOOTSTRAP TARGET NAMESPACE RELEASE NAME PHASE REVISION AGE manifests.addons.stackhpc.com/demo-cloud-config demo true openstack-system cloud-config Deployed 1 11d manifests.addons.stackhpc.com/demo-csi-cinder-storageclass demo true openstack-system csi-cinder-storageclass Deployed 1 11d manifests.addons.stackhpc.com/demo-kube-prometheus-stack-client demo true monitoring-system kube-prometheus-stack-client Deployed 2 11d manifests.addons.stackhpc.com/demo-kube-prometheus-stack-dashboards demo true monitoring-system kube-prometheus-stack-dashboards Deployed 1 11d manifests.addons.stackhpc.com/demo-kubernetes-dashboard-client demo true kubernetes-dashboard kubernetes-dashboard-client Deployed 2 11d manifests.addons.stackhpc.com/demo-loki-stack-dashboards demo true monitoring-system loki-stack-dashboards Deployed 1 11d NAME CLUSTER BOOTSTRAP TARGET NAMESPACE RELEASE NAME PHASE REVISION CHART NAME CHART VERSION AGE helmrelease.addons.stackhpc.com/dask-demo demo dask-demo dask-demo Deployed 1 daskhub-azimuth 0.1.0-dev.0.main.23 11d helmrelease.addons.stackhpc.com/demo-ccm-openstack demo true openstack-system ccm-openstack Deployed 1 openstack-cloud-controller-manager 1.3.0 11d helmrelease.addons.stackhpc.com/demo-cni-calico demo true tigera-operator cni-calico Deployed 1 tigera-operator v3.23.3 11d helmrelease.addons.stackhpc.com/demo-csi-cinder demo true openstack-system csi-cinder Deployed 1 openstack-cinder-csi 2.2.0 11d helmrelease.addons.stackhpc.com/demo-kube-prometheus-stack demo true monitoring-system kube-prometheus-stack Deployed 1 kube-prometheus-stack 40.1.0 11d helmrelease.addons.stackhpc.com/demo-kubernetes-dashboard demo true kubernetes-dashboard kubernetes-dashboard Deployed 1 kubernetes-dashboard 5.10.0 11d helmrelease.addons.stackhpc.com/demo-loki-stack demo true monitoring-system loki-stack Deployed 1 loki-stack 2.8.2 11d helmrelease.addons.stackhpc.com/demo-mellanox-network-operator demo true network-operator mellanox-network-operator Deployed 1 network-operator 1.3.0 11d helmrelease.addons.stackhpc.com/demo-metrics-server demo true kube-system metrics-server Deployed 1 metrics-server 3.8.2 11d helmrelease.addons.stackhpc.com/demo-node-feature-discovery demo true node-feature-discovery node-feature-discovery Deployed 1 node-feature-discovery 0.11.2 11d helmrelease.addons.stackhpc.com/demo-nvidia-gpu-operator demo true gpu-operator nvidia-gpu-operator Deployed 1 gpu-operator v1.11.1 11d NAME CLUSTER AGE kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-5kjjt demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-897r6 demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-control-plane-x26zz demo 11d kubeadmconfig.bootstrap.cluster.x-k8s.io/demo-sm0-cc21616d-lddk6 demo 11d NAME AGE kubeadmconfigtemplate.bootstrap.cluster.x-k8s.io/demo-sm0-cc21616d 11d NAME CLUSTER EXPECTEDMACHINES MAXUNHEALTHY CURRENTHEALTHY AGE machinehealthcheck.cluster.x-k8s.io/demo-control-plane demo 3 100% 3 11d machinehealthcheck.cluster.x-k8s.io/demo-sm0 demo 1 100% 1 11d NAME CLUSTER REPLICAS READY AVAILABLE AGE VERSION machineset.cluster.x-k8s.io/demo-sm0-c99fb7798 demo 1 1 1 11d v1.24.2 NAME CLUSTER REPLICAS READY UPDATED UNAVAILABLE PHASE AGE VERSION machinedeployment.cluster.x-k8s.io/demo-sm0 demo 1 1 1 0 Running 11d v1.24.2 NAME PHASE AGE VERSION cluster.cluster.x-k8s.io/demo Provisioned 11d NAME CLUSTER NODENAME PROVIDERID PHASE AGE VERSION machine.cluster.x-k8s.io/demo-control-plane-7p8zv demo demo-control-plane-7d76d0be-z6dm8 openstack:///f687f926-3cee-4550-91e5-32c2885708b0 Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-control-plane-9skvh demo demo-control-plane-7d76d0be-d2mcr openstack:///ea91f79a-8abb-4cb9-a2ea-8f772568e93c Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-control-plane-s8dhv demo demo-control-plane-7d76d0be-j64w6 openstack:///33a3a532-348a-4b93-ab19-d7d8cdb0daa4 Running 11d v1.24.2 machine.cluster.x-k8s.io/demo-sm0-c99fb7798-qqk4j demo demo-sm0-7d76d0be-gdjwv openstack:///ef9ae59c-bf20-44e0-831f-3798d25b7a06 Running 11d v1.24.2 NAME CLUSTER INITIALIZED API SERVER AVAILABLE REPLICAS READY UPDATED UNAVAILABLE AGE VERSION kubeadmcontrolplane.controlplane.cluster.x-k8s.io/demo-control-plane demo true true 3 3 3 0 11d v1.24.2 NAME CLUSTER READY NETWORK SUBNET BASTION IP openstackcluster.infrastructure.cluster.x-k8s.io/demo demo true 4b6b2722-ee5b-40ec-8e52-a6610e14cc51 73e22c49-10b8-4763-af2f-4c0cce007c82 NAME CLUSTER INSTANCESTATE READY PROVIDERID MACHINE openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-d2mcr demo ACTIVE true openstack:///ea91f79a-8abb-4cb9-a2ea-8f772568e93c demo-control-plane-9skvh openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-j64w6 demo ACTIVE true openstack:///33a3a532-348a-4b93-ab19-d7d8cdb0daa4 demo-control-plane-s8dhv openstackmachine.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be-z6dm8 demo ACTIVE true openstack:///f687f926-3cee-4550-91e5-32c2885708b0 demo-control-plane-7p8zv openstackmachine.infrastructure.cluster.x-k8s.io/demo-sm0-7d76d0be-gdjwv demo ACTIVE true openstack:///ef9ae59c-bf20-44e0-831f-3798d25b7a06 demo-sm0-c99fb7798-qqk4j NAME AGE openstackmachinetemplate.infrastructure.cluster.x-k8s.io/demo-control-plane-7d76d0be 11d openstackmachinetemplate.infrastructure.cluster.x-k8s.io/demo-sm0-7d76d0be 11d The cluster.cluster.x-k8s.io resource should be Provisioned , the machine.cluster.x-k8s.io resources should be Running with an associated NODENAME , the openstackmachine.infrastructure.cluster.x-k8s.io resources should be ACTIVE and the {manifests,helmrelease}.addons.stackhpc.com resources should all be Deployed . If this is not the case, first check the interactive console of the cluster nodes in Horizon to see if the nodes had any problems joining the cluster. Also check to see if the Zenith service for the API server was created correctly - once all control plane nodes have registered correctly the Endpoints resource for the service should have an entry for each control plane node (usually three). If these all look OK, check the logs of the Cluster API providers for any errors: On the K3s node, targetting the HA cluster if deployed kubectl -n capi-system logs deploy/capi-controller-manager kubectl -n capi-kubeadm-bootstrap-system logs deploy/capi-kubeadm-bootstrap-controller-manager kubectl -n capi-kubeadm-control-plane-system logs deploy/capi-kubeadm-control-plane-controller-manager kubectl -n capo-system logs deploy/capo-controller-manager kubectl -n capi-addon-system logs deploy/cluster-api-addon-provider","title":"Cluster API resource status"},{"location":"debugging/kubernetes/#accessing-tenant-clusters","text":"The kubeconfigs for all tenant clusters are stored as secrets. First, you need to find the name and namespace of the cluster you want to debug. This can be seen from the list of clusters: On the K3s node, targetting the HA cluster if deployed $ kubectl get cluster -A Then, you can retrieve and decode the kubeconfig with the following: On the K3s node, targetting the HA cluster if deployed $ kubectl -n <namespace> get secret <clustername>-kubeconfig -o json | \\ jq -r '.data.value' | \\ base64 -d \\ > kubeconfig-tenant.yaml This can now be used by exporting the path to this file: On the K3s node, targetting the HA cluster if deployed $ export KUBECONFIG=kubeconfig-tenant.yaml","title":"Accessing tenant clusters"},{"location":"debugging/kubernetes/#zenith-service-issues","text":"Zenith services are enabled on Kubernetes clusters using the Zenith operator . Each tenant Kubernetes cluster gets an instance of the operator that runs on the Azimuth cluster, where it can reach the Zenith registrar to allocate subdomains, but watches the tenant cluster for instances of the reservation.zenith.stackhpc.com and client.zenith.stackhpc.com custom resources. By creating instances of these resources in the tenant cluster, Kubernetes Services in the target cluster can be exposed via Zenith. If Zenith services are not becoming available for Kubernetes cluster services, first follow the procedure for debugging a Zenith service , including checking that the clients were created correctly and that the pods are running: Targetting the tenant cluster $ kubectl get zenith -A NAMESPACE NAME PHASE UPSTREAM SERVICE MITM ENABLED MITM AUTH AGE kubernetes-dashboard client.zenith.stackhpc.com/kubernetes-dashboard Available kubernetes-dashboard true ServiceAccount 4d19h monitoring-system client.zenith.stackhpc.com/kube-prometheus-stack Available kube-prometheus-stack-grafana true Basic 4d19h NAMESPACE NAME SECRET PHASE FQDN AGE kubernetes-dashboard reservation.zenith.stackhpc.com/kubernetes-dashboard kubernetes-dashboard-zenith-credential Ready mwqgcdrk77nva18uzcct3g7jlo7obi7zlbcgemuhk6nhk.azimuth.example.org 4d19h monitoring-system reservation.zenith.stackhpc.com/kube-prometheus-stack kube-prometheus-stack-zenith-credential Ready zovdsnnesww2hiw074mvufvcfgczfbd2yhmuhsf3p59xa.azimuth.example.org 4d19h $ kubectl get deploy,po -A -l app.kubernetes.io/managed-by=zenith-operator NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-dashboard deployment.apps/kubernetes-dashboard-zenith-client 1/1 1 1 4d19h monitoring-system deployment.apps/kube-prometheus-stack-zenith-client 1/1 1 1 4d19h NAMESPACE NAME READY STATUS RESTARTS AGE kubernetes-dashboard pod/kubernetes-dashboard-zenith-client-86c5fd9bd-2jfdb 2/2 Running 0 4d19h monitoring-system pod/kube-prometheus-stack-zenith-client-b9986579d-qgp82 2/2 Running 0 9h Tip The kubeconfig for a tenant cluster is available in a secret in the tenant namespace: On the K3s node, targetting the HA cluster if deployed $ kubectl -n az-demo get secret | grep kubeconfig demo-kubeconfig cluster.x-k8s.io/secret 1 11d If everything looks OK, try restarting the Zenith operator for the cluster: On the K3s node, targetting the HA cluster if deployed $ kubectl -n az-demo rollout restart deploy/demo-zenith-operator","title":"Zenith service issues"},{"location":"debugging/zenith-services/","text":"Debugging Zenith services If a Zenith service does not become available, the most common causes are: Client not connecting to SSHD The first thing that happens to connect a Zenith service is that the Zenith client must connect to the Zenith SSHD. To see if a Zenith client is connecting, check for the Zenith subdomain in the logs of the SSHD server: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deploy/zenith-server-sshd [ -f ] If there are no logs for the target Zenith subdomain, this usually indicates a problem with the client. Check the logs for the client and restart it if necessary. If problems persist, try restarting the Zenith SSHD: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/zenith-server-sshd Client not appearing in Zenith CRDs The components of Zenith communicate using three Kubernetes CRDs : services.zenith.stackhpc.com A reserved domain and associated SSH public key. endpoints.zenith.stackhpc.com The current endpoints for a Zenith service. This resource is updated to add the address, port and configuration of the Zenith SSH tunnel as the SSH tunnel is created. leases.zenith.stackhpc.com Heartbeat information for an individual SSH tunnel. Each Zenith SSH tunnel has its own lease resource that is regularly updated with a heartbeat. If a Zenith service is not functioning as expected, check the state of the CRDs for that service. First, check that the service exists and has an SSH key associated: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get services.zenith NAME FINGERPRINT AGE igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn WLo15SbKRadA5q1WIn6dToWT4Q+j05rZ5T+Zc/so4M0 13m sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 G6sdXwUfvdlosCB2yi40TEf5//ie2bgCxytrig4xpTA 13m Next, check that there is at least one lease for the service and verify that it is being regularly renewed: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get leases.zenith NAME RENEWED TTL REAP AFTER AGE sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-fn75d 7s 20 120 13m igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-7tnqm 5s 20 120 14m Finally, check that the endpoint is registered correctly in the endpoints resource: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get endpoints.zenith igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn -o yaml apiVersion: zenith.stackhpc.com/v1alpha1 kind: Endpoints metadata: creationTimestamp: \"2024-05-01T13:24:12Z\" generation: 3 name: igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn namespace: zenith-services ownerReferences: - apiVersion: zenith.stackhpc.com/v1alpha1 blockOwnerDeletion: true kind: Service name: igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn uid: 378b39dc-9fce-4553-865e-edad2dd8d8b0 resourceVersion: \"7260\" uid: 62badd6e-a5a7-452d-b346-04c2efd75a6c spec: endpoints: 7tnqm: address: 10.42.0.71 config: backend-protocol: http skip-auth: false port: 42109 status: passing The address should be the pod IP of a Zenith SSHD pod, and the port should be the allocated port reported by the client. OIDC credentials not created Keycloak OIDC credentials for Zenith services for platforms deployed using Azimuth are created by the azimuth-identity-operator . To see if this step has happened, check the status of the realm and platform resources created by the identity operator. They should all be in the Ready phase: On the K3s node, targetting the HA cluster if deployed $ kubectl get realm,platform -A NAMESPACE NAME PHASE TENANCY ID OIDC ISSUER AGE az-demo realm.identity.azimuth.stackhpc.com/az-demo Ready xxxxxxxxxxxxx https://identity.azimuth.example.org/realms/az-demo 4d2h NAMESPACE NAME PHASE AGE az-demo platform.identity.azimuth.stackhpc.com/kube-mykubecluster Ready 114s If any of these resources stay in an unready state for more than a few minutes, try restarting the identity operator: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/azimuth-identity-operator If this doesn't work, check the logs for errors: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deployment/azimuth-identity-operator [ -f ] Kubernetes resources for the Zenith service have not been created If the CRDs for the service look correct, it is possible that the component that watches the Zenith CRDs and creates the Kubernetes ingress for those services is not functioning correctly. This component creates Helm releases to deploy the resources for a service, so first check that a Helm release exists for the service and is in the deployed state: On the K3s node, targetting the HA cluster if deployed $ helm -n zenith-services list -a NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn zenith-services 1 2024 -05-01 13 :24:13.36622944 +0000 UTC deployed zenith-service-0.1.0+846a545e main sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 zenith-services 1 2024 -05-01 13 :24:41.219330845 +0000 UTC deployed zenith-service-0.1.0+846a545e main Also check the state of the Ingress , Service and EndpointSlice s for the service: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get ingress,service,endpointslice NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc nginx igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn.apps.45-135-57-238.sslip.io 192.168.3.49 80, 443 25m ingress.networking.k8s.io/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn nginx igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn.apps.45-135-57-238.sslip.io 192.168.3.49 80, 443 25m ingress.networking.k8s.io/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 nginx sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31.apps.45-135-57-238.sslip.io 192.168.3.49 80, 443 24m ingress.networking.k8s.io/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc nginx sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31.apps.45-135-57-238.sslip.io 192.168.3.49 80, 443 24m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc ClusterIP 10.43.247.54 <none> 80/TCP,44180/TCP 25m service/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn ClusterIP 10.43.125.138 <none> 80/TCP 25m service/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 ClusterIP 10.43.234.107 <none> 80/TCP 24m service/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc ClusterIP 10.43.7.75 <none> 80/TCP,44180/TCP 24m NAME ADDRESSTYPE PORTS ENDPOINTS AGE endpointslice.discovery.k8s.io/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-3010d IPv4 42109 10.42.0.71 25m endpointslice.discovery.k8s.io/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc-kqx8h IPv4 4180,44180 10.42.0.86 25m endpointslice.discovery.k8s.io/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-f2086 IPv4 33857 10.42.0.71 24m endpointslice.discovery.k8s.io/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc-tkvqv IPv4 4180,44180 10.42.0.88 24m Ingress address not assigned If an ingress resource does not have an address, this may be a sign that the ingress controller is not correctly configured or not functioning correctly. Services with OIDC authentication When a service has OIDC authentication enabled, there will be two of each resource for each service, one of which will have the suffix -oidc . Each service with OIDC authentication enabled gets a standalone service that is responsible handling the interactions with the OIDC provider. To check the state of these resources, use: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get deploy,po NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc 1/1 1 1 33m deployment.apps/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc 1/1 1 1 33m NAME READY STATUS RESTARTS AGE pod/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc-7f6656bd98-9rrj2 1/1 Running 0 33m pod/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc-7ffbff4cd6-sr75w 1/1 Running 0 33m If any of these resources look incorrect, try restarting the Zenith sync component: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/zenith-server-sync If this doesn't work, check the logs for errors: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deployment/zenith-server-sync [ -f ] cert-manager fails to obtain a certificate If you are using cert-manager to dynamically allocate certificates for Zenith services it is possible that cert-manager has failed to obtain a certificate for the service, e.g. because it has been rate-limited. To check if this is the case, check the state of the certificates for the Zenith services: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get certificate NAME READY SECRET AGE tls-igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn True tls-igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn 30m tls-sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 True tls-sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 29m If the certificate for the service is not ready, check the details for the certificate using kubectl describe and check for any errors that have occured.","title":"Debugging Zenith services"},{"location":"debugging/zenith-services/#debugging-zenith-services","text":"If a Zenith service does not become available, the most common causes are:","title":"Debugging Zenith services"},{"location":"debugging/zenith-services/#client-not-connecting-to-sshd","text":"The first thing that happens to connect a Zenith service is that the Zenith client must connect to the Zenith SSHD. To see if a Zenith client is connecting, check for the Zenith subdomain in the logs of the SSHD server: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deploy/zenith-server-sshd [ -f ] If there are no logs for the target Zenith subdomain, this usually indicates a problem with the client. Check the logs for the client and restart it if necessary. If problems persist, try restarting the Zenith SSHD: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/zenith-server-sshd","title":"Client not connecting to SSHD"},{"location":"debugging/zenith-services/#client-not-appearing-in-zenith-crds","text":"The components of Zenith communicate using three Kubernetes CRDs : services.zenith.stackhpc.com A reserved domain and associated SSH public key. endpoints.zenith.stackhpc.com The current endpoints for a Zenith service. This resource is updated to add the address, port and configuration of the Zenith SSH tunnel as the SSH tunnel is created. leases.zenith.stackhpc.com Heartbeat information for an individual SSH tunnel. Each Zenith SSH tunnel has its own lease resource that is regularly updated with a heartbeat. If a Zenith service is not functioning as expected, check the state of the CRDs for that service. First, check that the service exists and has an SSH key associated: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get services.zenith NAME FINGERPRINT AGE igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn WLo15SbKRadA5q1WIn6dToWT4Q+j05rZ5T+Zc/so4M0 13m sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 G6sdXwUfvdlosCB2yi40TEf5//ie2bgCxytrig4xpTA 13m Next, check that there is at least one lease for the service and verify that it is being regularly renewed: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get leases.zenith NAME RENEWED TTL REAP AFTER AGE sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-fn75d 7s 20 120 13m igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-7tnqm 5s 20 120 14m Finally, check that the endpoint is registered correctly in the endpoints resource: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get endpoints.zenith igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn -o yaml apiVersion: zenith.stackhpc.com/v1alpha1 kind: Endpoints metadata: creationTimestamp: \"2024-05-01T13:24:12Z\" generation: 3 name: igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn namespace: zenith-services ownerReferences: - apiVersion: zenith.stackhpc.com/v1alpha1 blockOwnerDeletion: true kind: Service name: igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn uid: 378b39dc-9fce-4553-865e-edad2dd8d8b0 resourceVersion: \"7260\" uid: 62badd6e-a5a7-452d-b346-04c2efd75a6c spec: endpoints: 7tnqm: address: 10.42.0.71 config: backend-protocol: http skip-auth: false port: 42109 status: passing The address should be the pod IP of a Zenith SSHD pod, and the port should be the allocated port reported by the client.","title":"Client not appearing in Zenith CRDs"},{"location":"debugging/zenith-services/#oidc-credentials-not-created","text":"Keycloak OIDC credentials for Zenith services for platforms deployed using Azimuth are created by the azimuth-identity-operator . To see if this step has happened, check the status of the realm and platform resources created by the identity operator. They should all be in the Ready phase: On the K3s node, targetting the HA cluster if deployed $ kubectl get realm,platform -A NAMESPACE NAME PHASE TENANCY ID OIDC ISSUER AGE az-demo realm.identity.azimuth.stackhpc.com/az-demo Ready xxxxxxxxxxxxx https://identity.azimuth.example.org/realms/az-demo 4d2h NAMESPACE NAME PHASE AGE az-demo platform.identity.azimuth.stackhpc.com/kube-mykubecluster Ready 114s If any of these resources stay in an unready state for more than a few minutes, try restarting the identity operator: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/azimuth-identity-operator If this doesn't work, check the logs for errors: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deployment/azimuth-identity-operator [ -f ]","title":"OIDC credentials not created"},{"location":"debugging/zenith-services/#kubernetes-resources-for-the-zenith-service-have-not-been-created","text":"If the CRDs for the service look correct, it is possible that the component that watches the Zenith CRDs and creates the Kubernetes ingress for those services is not functioning correctly. This component creates Helm releases to deploy the resources for a service, so first check that a Helm release exists for the service and is in the deployed state: On the K3s node, targetting the HA cluster if deployed $ helm -n zenith-services list -a NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn zenith-services 1 2024 -05-01 13 :24:13.36622944 +0000 UTC deployed zenith-service-0.1.0+846a545e main sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 zenith-services 1 2024 -05-01 13 :24:41.219330845 +0000 UTC deployed zenith-service-0.1.0+846a545e main Also check the state of the Ingress , Service and EndpointSlice s for the service: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get ingress,service,endpointslice NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc nginx igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn.apps.45-135-57-238.sslip.io 192.168.3.49 80, 443 25m ingress.networking.k8s.io/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn nginx igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn.apps.45-135-57-238.sslip.io 192.168.3.49 80, 443 25m ingress.networking.k8s.io/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 nginx sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31.apps.45-135-57-238.sslip.io 192.168.3.49 80, 443 24m ingress.networking.k8s.io/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc nginx sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31.apps.45-135-57-238.sslip.io 192.168.3.49 80, 443 24m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc ClusterIP 10.43.247.54 <none> 80/TCP,44180/TCP 25m service/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn ClusterIP 10.43.125.138 <none> 80/TCP 25m service/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 ClusterIP 10.43.234.107 <none> 80/TCP 24m service/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc ClusterIP 10.43.7.75 <none> 80/TCP,44180/TCP 24m NAME ADDRESSTYPE PORTS ENDPOINTS AGE endpointslice.discovery.k8s.io/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-3010d IPv4 42109 10.42.0.71 25m endpointslice.discovery.k8s.io/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc-kqx8h IPv4 4180,44180 10.42.0.86 25m endpointslice.discovery.k8s.io/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-f2086 IPv4 33857 10.42.0.71 24m endpointslice.discovery.k8s.io/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc-tkvqv IPv4 4180,44180 10.42.0.88 24m Ingress address not assigned If an ingress resource does not have an address, this may be a sign that the ingress controller is not correctly configured or not functioning correctly. Services with OIDC authentication When a service has OIDC authentication enabled, there will be two of each resource for each service, one of which will have the suffix -oidc . Each service with OIDC authentication enabled gets a standalone service that is responsible handling the interactions with the OIDC provider. To check the state of these resources, use: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get deploy,po NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc 1/1 1 1 33m deployment.apps/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc 1/1 1 1 33m NAME READY STATUS RESTARTS AGE pod/igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn-oidc-7f6656bd98-9rrj2 1/1 Running 0 33m pod/sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31-oidc-7ffbff4cd6-sr75w 1/1 Running 0 33m If any of these resources look incorrect, try restarting the Zenith sync component: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth rollout restart deployment/zenith-server-sync If this doesn't work, check the logs for errors: On the K3s node, targetting the HA cluster if deployed kubectl -n azimuth logs deployment/zenith-server-sync [ -f ]","title":"Kubernetes resources for the Zenith service have not been created"},{"location":"debugging/zenith-services/#cert-manager-fails-to-obtain-a-certificate","text":"If you are using cert-manager to dynamically allocate certificates for Zenith services it is possible that cert-manager has failed to obtain a certificate for the service, e.g. because it has been rate-limited. To check if this is the case, check the state of the certificates for the Zenith services: On the K3s node, targetting the HA cluster if deployed $ kubectl -n zenith-services get certificate NAME READY SECRET AGE tls-igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn True tls-igxvo2okpkq834d1qbgtlhmm6xo4laj0dupn 30m tls-sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 True tls-sh20tp1071hl3xtjw5cj4mwdy5t0v7qodj31 29m If the certificate for the service is not ready, check the details for the certificate using kubectl describe and check for any errors that have occured.","title":"cert-manager fails to obtain a certificate"},{"location":"deployment/","text":"Deploying Azimuth Python dependencies The Python requirements for an Azimuth deployment host, including Ansible itself, are contained in requirements.txt and must be installed before you can proceed with a deployment. It is recommended to use a virtual environment in order to keep the dependencies isolated from other Python applications on the host. azimuth-config includes a utility script that will create a Python virtual environment in the configuration directory and install the required dependencies: ./bin/ensure-venv If it exists, this virtual environment will be activated as part of the environment activation (see below). If you prefer to manage your own virtual environments then you must ensure that the correct environment is activated and has the required dependencies installed before continuing. For example, if you use pyenv you can set the PYENV_VERSION environment variable in your azimuth-config environment : env PYENV_VERSION = azimuth-config Activating an environment Before you can deploy Azimuth, you must first activate an environment: source ./bin/activate my-site Warning This script must be source d rather than just executed as it exports environment variables into the current shell that are used to configure the deployment. Deploying an environment Once you are happy with any configuration changes and the environment that you want to deploy to has been activated, run the following command to deploy Azimuth: # Install or update Ansible dependencies ansible-galaxy install -f -r ./requirements.yml # Run the provision playbook from the azimuth-ops collection # The inventory is picked up from the ansible.cfg file in the environment ansible-playbook stackhpc.azimuth_ops.provision Tearing down an environment azimuth-ops is also able to tear down an Azimuth environment, including the K3s and HA Kubernetes clusters as required. After activating the environment that you want to tear down, run the following: ansible-playbook stackhpc.azimuth_ops.destroy","title":"Deploying Azimuth"},{"location":"deployment/#deploying-azimuth","text":"","title":"Deploying Azimuth"},{"location":"deployment/#python-dependencies","text":"The Python requirements for an Azimuth deployment host, including Ansible itself, are contained in requirements.txt and must be installed before you can proceed with a deployment. It is recommended to use a virtual environment in order to keep the dependencies isolated from other Python applications on the host. azimuth-config includes a utility script that will create a Python virtual environment in the configuration directory and install the required dependencies: ./bin/ensure-venv If it exists, this virtual environment will be activated as part of the environment activation (see below). If you prefer to manage your own virtual environments then you must ensure that the correct environment is activated and has the required dependencies installed before continuing. For example, if you use pyenv you can set the PYENV_VERSION environment variable in your azimuth-config environment : env PYENV_VERSION = azimuth-config","title":"Python dependencies"},{"location":"deployment/#activating-an-environment","text":"Before you can deploy Azimuth, you must first activate an environment: source ./bin/activate my-site Warning This script must be source d rather than just executed as it exports environment variables into the current shell that are used to configure the deployment.","title":"Activating an environment"},{"location":"deployment/#deploying-an-environment","text":"Once you are happy with any configuration changes and the environment that you want to deploy to has been activated, run the following command to deploy Azimuth: # Install or update Ansible dependencies ansible-galaxy install -f -r ./requirements.yml # Run the provision playbook from the azimuth-ops collection # The inventory is picked up from the ansible.cfg file in the environment ansible-playbook stackhpc.azimuth_ops.provision","title":"Deploying an environment"},{"location":"deployment/#tearing-down-an-environment","text":"azimuth-ops is also able to tear down an Azimuth environment, including the K3s and HA Kubernetes clusters as required. After activating the environment that you want to tear down, run the following: ansible-playbook stackhpc.azimuth_ops.destroy","title":"Tearing down an environment"},{"location":"deployment/automation/","text":"Automated deployments For a production installation of Azimuth, it is recommended to adopt a continuous delivery approach to deployment rather than running deployment commands manually. Using this approach, configuration changes are automatically deployed to test, staging and production environments, although deployments to production typically include a manual approval. Continuous delivery vs continuous deployment Continuous delivery is very similar to Continuous Deployment with the exception that in continuous deployment, production deployments are also fully automated with no manual intervention or approval. Using a site mixin To get the maximum benefit from automated deployments and the feature branch workflow , you should try to minimise the differences between the production, staging and dynamic review environments. The best way to do this is to use a site mixin that contains all the site-specific configuration that is common between your environments, e.g. extra community images, custom Kubernetes templates, networking configuration, and include it in each of your concrete environments. GitLab CI/CD azimuth-config provides a sample configuration for use with GitLab CI/CD that demonstrates how to set up continuous delivery for an Azimuth configuration repository. Tip If you are using GitLab for your configuration repository, make sure you have configured it to use GitLab-managed Terraform state . Runner configuration Configuration of GitLab runners for executing CI/CD jobs is beyond the scope of this documentation. We assume that a runner is available to the configuration project that is able to execute user-specified images, e.g. using the Docker or Kubernetes executors. One option is to deploy a runner as a VM in an OpenStack project . Automated deployments The sample GitLab CI/CD configuration makes use of GitLab environments to manage deployments of Azimuth, where each GitLab environment uses a concrete configuration environment in your repository. This is a one-to-one relationship except for per-branch dynamic review environments , where multiple GitLab environments will use a single configuration environment. If you are using GitLab-managed Terraform state, each GitLab environment (not configuration environment) will get it's own independent state. The sample configuration defines the following deployment jobs: Each commit to a branch other than main (e.g. a feature branch), triggers an automated deployment to a branch-specific dynamic GitLab environment , using a single concrete configuration environment . These environments are automatically destroyed when the associated merge request is closed. Each commit to main triggers an automated deployment to staging using a static GitLab environment . Each commit to main also creates a job for an automated deployment to production, also using a static environment. However this job requires a manual trigger before it will start. To get started, just copy .gitlab-ci.yml.sample to .gitlab-ci.yml and amend the environment names and paths to match the environments in your configuration. The next commit will begin to trigger deployments. Access to secrets In order for the deployment jobs to access the secrets in your configuration, you will need to provide the git-crypt key as a CI/CD variable for the project . The base64-encoded key should be stored in the GIT_CRYPT_KEY_B64 variable and made available to all branches: git-crypt export-key - | base64 Per-branch dynamic review environments The per-branch dynamic review environments are special in that multiple GitLab environments are provisioned using a single configuration environment. This means that the configuration environment must be capable of producing multiple independent deployments. In particular, it cannot use any fixed floating IPs which also means no fixed DNS entry. Instead it must allocate an IP for itself and use a dynamic DNS service like sslip.io for the ingress domain. This is also how the demo environment works, and is the default if no fixed IP is specified. Single node only At present dynamic review environments must be single node deployments , as HA deployments do not support dynamically allocating a floating IP for the Ingress Controller. A single node is likely to be sufficient for a dynamic review environment, as a full HA deployment for every branch would consume a lot more resources. However you should ensure that you have a full HA deployment as a staging or pre-production environment in order to test that the configuration works. Shared credentials Per-branch dynamic review environments will share the clouds.yaml specified in the configuration environment, and hence will share an OpenStack project. This is not a problem, as multiple isolated deployments can happily coexist in the same project as long as they have different names, but you must ensure that the project has suitable quotas. Activating per-branch review environments Because a single configuration environment is used for multiple deployments, a slight variant of the usual environment activation must be used that specifies both the configuration environment and the GitLab environment name: source ./bin/activate \"<configuration environment>\" \"<gitlab environment>\" A configuration environment for dynamic review environments is set up in the usual way , subject to the caveats above. The following is a minimal set of Ansible variables that will work for most clouds, when combined with the base and singlenode mixin environments (plus any site-specific mixin environments): # Configuration for the K3s node infra_external_network_id : \"<network id>\" infra_flavor_id : \"<flavor id>\" # Azimuth cloud name # This can use the environment name if desired, e.g.: azimuth_current_cloud_name : \"{{ lookup('env', 'CI_ENVIRONMENT_SLUG') }}\" azimuth_current_cloud_label : \"{{ lookup('env', 'CI_ENVIRONMENT_NAME') }}\" # \"Secrets\" # Since the dynamic environments are short-lived, there is not much # risk in using secrets that are not really secret for ease admin_dashboard_ingress_basic_auth_password : admin harbor_admin_password : admin harbor_secret_key : abcdefghijklmnop keycloak_admin_password : admin zenith_registrar_subdomain_token_signing_key : abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789AA azimuth_secret_key : 9876543210ZYXWVUTSRQPONMLKJIHGFEDCBAzyxwvutsrqponmlkjihgfedcda00 Automated upgrades The sample configuration also includes a job that can automatically propose an Azimuth upgrade when a new release becomes available. If the job detects a new release, it will create a new branch, merge the changes into it and create an associated merge request . If you also have per-branch dynamic review environments enabled, then this will automatically trigger a job to deploy the changes for review. The job will only run for a scheduled pipeline , so to enable automated upgrades you must add a pipeline schedule for the main branch of your configuration repository with a suitable interval (e.g. weekly). Because the job needs to write to the repository and call the merge requests API, the CI/CD job token is not sufficient. Instead, you must set the CI/CD variables GITLAB_PAT_TOKEN and GITLAB_PAT_USERNAME for the scheduled pipeline, which should contain an access token and the corresponding username respectively. The token must have permission to write to the repository and the GitLab API. If your GitLab project has Project access tokens available, then one of these can be used by specifying the associated bot user . Unfortunately, this is a paid feature and the only real alternative is to use a Personal access token . Danger Personal access tokens cannot be scoped to a project. Uploading a personal access token as a CI/CD variable means that other members of the project, and the CI/CD jobs that use it, will be able to see your token. Because of the lack of project scope, this means that a malicious actor may be able to obtain the token and use it to access your other projects. If you do not want to pay for Project access tokens, then you could register a separate service account that only belongs to your configuration project and issue a personal access token from that account instead. GitHub CI/CD For site-specific configuration repositories hosted on GitHub, azimuth-config provides two sample workflows for automated deployments to a test or staging environment ( example workflow ) and manually-triggered deployment to a production environment ( example workflow ). These can be used with GitHub Actions to mimic some of the GitLab functionality described above. Each sample file contains a top-level comment describing how to tailor these workflows to a site-specific configuration repository.","title":"Automated deployments"},{"location":"deployment/automation/#automated-deployments","text":"For a production installation of Azimuth, it is recommended to adopt a continuous delivery approach to deployment rather than running deployment commands manually. Using this approach, configuration changes are automatically deployed to test, staging and production environments, although deployments to production typically include a manual approval. Continuous delivery vs continuous deployment Continuous delivery is very similar to Continuous Deployment with the exception that in continuous deployment, production deployments are also fully automated with no manual intervention or approval. Using a site mixin To get the maximum benefit from automated deployments and the feature branch workflow , you should try to minimise the differences between the production, staging and dynamic review environments. The best way to do this is to use a site mixin that contains all the site-specific configuration that is common between your environments, e.g. extra community images, custom Kubernetes templates, networking configuration, and include it in each of your concrete environments.","title":"Automated deployments"},{"location":"deployment/automation/#gitlab-cicd","text":"azimuth-config provides a sample configuration for use with GitLab CI/CD that demonstrates how to set up continuous delivery for an Azimuth configuration repository. Tip If you are using GitLab for your configuration repository, make sure you have configured it to use GitLab-managed Terraform state . Runner configuration Configuration of GitLab runners for executing CI/CD jobs is beyond the scope of this documentation. We assume that a runner is available to the configuration project that is able to execute user-specified images, e.g. using the Docker or Kubernetes executors. One option is to deploy a runner as a VM in an OpenStack project .","title":"GitLab CI/CD"},{"location":"deployment/automation/#automated-deployments_1","text":"The sample GitLab CI/CD configuration makes use of GitLab environments to manage deployments of Azimuth, where each GitLab environment uses a concrete configuration environment in your repository. This is a one-to-one relationship except for per-branch dynamic review environments , where multiple GitLab environments will use a single configuration environment. If you are using GitLab-managed Terraform state, each GitLab environment (not configuration environment) will get it's own independent state. The sample configuration defines the following deployment jobs: Each commit to a branch other than main (e.g. a feature branch), triggers an automated deployment to a branch-specific dynamic GitLab environment , using a single concrete configuration environment . These environments are automatically destroyed when the associated merge request is closed. Each commit to main triggers an automated deployment to staging using a static GitLab environment . Each commit to main also creates a job for an automated deployment to production, also using a static environment. However this job requires a manual trigger before it will start. To get started, just copy .gitlab-ci.yml.sample to .gitlab-ci.yml and amend the environment names and paths to match the environments in your configuration. The next commit will begin to trigger deployments.","title":"Automated deployments"},{"location":"deployment/automation/#automated-upgrades","text":"The sample configuration also includes a job that can automatically propose an Azimuth upgrade when a new release becomes available. If the job detects a new release, it will create a new branch, merge the changes into it and create an associated merge request . If you also have per-branch dynamic review environments enabled, then this will automatically trigger a job to deploy the changes for review. The job will only run for a scheduled pipeline , so to enable automated upgrades you must add a pipeline schedule for the main branch of your configuration repository with a suitable interval (e.g. weekly). Because the job needs to write to the repository and call the merge requests API, the CI/CD job token is not sufficient. Instead, you must set the CI/CD variables GITLAB_PAT_TOKEN and GITLAB_PAT_USERNAME for the scheduled pipeline, which should contain an access token and the corresponding username respectively. The token must have permission to write to the repository and the GitLab API. If your GitLab project has Project access tokens available, then one of these can be used by specifying the associated bot user . Unfortunately, this is a paid feature and the only real alternative is to use a Personal access token . Danger Personal access tokens cannot be scoped to a project. Uploading a personal access token as a CI/CD variable means that other members of the project, and the CI/CD jobs that use it, will be able to see your token. Because of the lack of project scope, this means that a malicious actor may be able to obtain the token and use it to access your other projects. If you do not want to pay for Project access tokens, then you could register a separate service account that only belongs to your configuration project and issue a personal access token from that account instead.","title":"Automated upgrades"},{"location":"deployment/automation/#github-cicd","text":"For site-specific configuration repositories hosted on GitHub, azimuth-config provides two sample workflows for automated deployments to a test or staging environment ( example workflow ) and manually-triggered deployment to a production environment ( example workflow ). These can be used with GitHub Actions to mimic some of the GitLab functionality described above. Each sample file contains a top-level comment describing how to tailor these workflows to a site-specific configuration repository.","title":"GitHub CI/CD"},{"location":"deployment/testing/","text":"Integration testing azimuth-ops is able to generate integration tests for an environment that can be executed against the deployed Azimuth to validate that it is working correctly. Currently, the following tests can be generated: Deploy and verify CaaS clusters Deploy and verify Kubernetes clusters Deploy and verify Kubernetes apps The generated test suites use Robot Framework to handle the execution of the tests and gathering and reporting of results. In Robot Framework, test cases are created by chaining together keywords imported from various libraries . Robot Framework also allows the creation of custom keyword libraries containing domain-specific keywords. The tests generated by azimuth-ops use keywords from a custom library for interacting with Azimuth, which in turn uses the Azimuth Python SDK to interact with the target Azimuth deployment. The result is that a typical test case looks something like this: Example of a generated test case *** Settings *** Name CaaS Library Azimuth Test Tags caas Test Timeout 15 minutes *** Test Cases *** Create workstation [ Tags ] workstation create ${ ctype } = Find Cluster Type By Name workstation ${ params } = Guess Parameter Values For Cluster Type ${ ctype } ${ cluster } = Create Cluster test-rbe6w ${ ctype.name } &{ params } Verify workstation [ Tags ] workstation verify ${ cluster } = Find Cluster By Name test-rbe6w ${ cluster } = Wait For Cluster Ready ${ cluster.id } ${ url } = Get Cluster Service URL ${ cluster } webconsole Open Zenith Service ${ url } Wait Until Page Title Contains Apache Guacamole ${ url } = Get Cluster Service URL ${ cluster } monitoring Open Zenith Service ${ url } Wait Until Page Title Contains Grafana Delete workstation [ Tags ] workstation delete ${ cluster } = Find Cluster By Name test-rbe6w Delete Cluster ${ cluster.id } This example will create a CaaS workstation using parameter values that are guessed based on the cluster type and target Azimuth. It then waits for the workstation to become Ready before verifying that the the Zenith services are behaving as expected. Finally, the workstation is deleted. Prerequisites Config Make sure references to the application credential authenticator in your current config environment are either set to true or removed completely: environments/my-site/clouds.yaml azimuth_authenticator_appcred_enabled : true Firefox package If you are running tests manually outside of a CI environment, you need to ensure you have the correct Firefox package installed for Selium. On Ubuntu: # Remove any old firefox packages sudo apt remove --purge firefox sudo snap remove --purge firefox # Add the Mozilla PPA wget -q https://packages.mozilla.org/apt/repo-signing-key.gpg -O- | sudo tee /etc/apt/keyrings/packages.mozilla.org.asc > /dev/null echo \"deb [signed-by=/etc/apt/keyrings/packages.mozilla.org.asc] https://packages.mozilla.org/apt mozilla main\" | sudo tee -a /etc/apt/sources.list.d/mozilla.list > /dev/null # Set firefox priority to ensure the Mozilla package is always preferred echo ' Package: * Pin: origin packages.mozilla.org Pin-Priority: 1000 ' | sudo tee /etc/apt/preferences.d/mozilla # Install firefox sudo apt update && sudo apt install firefox Credentials for executing tests The generated test suite uses an OpenStack application credential to authenticate with Azimuth, and this credential determines which project test platforms will be created in. This can be the same project that Azimuth is deployed in, but should ideally be a different project. Application credential must be unrestricted Because Azimuth creates an application credential for each deployed platform, the application credential used to authenticate with Azimuth to create test platforms must be able to create and delete other application credentials. This is referred to as unrestricted in OpenStack ( Unrestricted (dangerous) in the Horizon UI). The application credential used for running tests should be appended to the same clouds.yaml as the credential used to deploy Azimuth . environments/my-site/clouds.yaml clouds : # The main application credential used to deploy Azimuth openstack : auth : auth_url : https://openstack.example-cloud.org:5000 application_credential_id : \"<appcred id>\" application_credential_secret : \"<appcred secret>\" region_name : \"RegionOne\" interface : \"public\" identity_api_version : 3 auth_type : \"v3applicationcredential\" # The application credential used to create test platforms via Azimuth unrestricted : auth : auth_url : https://openstack.example-cloud.org:5000 application_credential_id : \"<appcred id>\" application_credential_secret : \"<appcred secret>\" region_name : \"RegionOne\" interface : \"public\" identity_api_version : 3 auth_type : \"v3applicationcredential\" Generating and executing tests Before tests can be generated and executed for an environment , the environment must be successfully deployed either manually or by automation . You must also have the Python dependencies installed on the machine that the tests will be run from. The easiest way to do this is using the ensure-venv script , however the requirements.txt can be used to install them into any Python environment: pip install -U pip pip install -r requirements.txt The integration test suite can then be generated and executed for the environment. Because the test generation connects to the Azimuth deployment to query the installed platform types, this must use the application credential that is used for deployment. The execution of the tests then uses the unrestricted application credential: # Activate the target environment source ./bin/activate my-site # Make sure you have the correct version of the Ansible collection installed ansible-galaxy install -f -r requirements.yml # Generate the test suite ansible-playbook stackhpc.azimuth_ops.generate_tests # Execute the test suite (using the unrestricted credential) OS_CLOUD = unrestricted ./bin/run-tests Configuring test generation By default, tests cases are generated for: All installed CaaS cluster types All installed active (i.e. non-deprecated) Kubernetes cluster types All installed Kubernetes app templates The following sections describe how to enable, disable and configure tests for the different types of platform. Tip The generate_tests role of azimuth-ops is responsible for generating the test cases. It has a large number of variables that can be used to tune the test generation. This document only discusses the most frequently used variables. CaaS cluster types Test cases for CaaS cluster types perform the following steps: Build the cluster parameters using best-effort guesses plus overrides Create a cluster using the parameters Wait for the cluster to become Ready Check that the Zenith services for the cluster are accessible (Optional) Verify that the page title for the Zenith service contains some expected content Delete the cluster Verification of Zenith service page titles Step 5 is used to verify that the Zenith authentication loop brings you back to the correct service. Currently, no other verification of the behaviour of the service is performed. Best-effort guesses for parameter values Best-effort guesses for parameter values are produced in the following way: If the parameter has a default, use that. For cloud.size parameters, use the smallest size that satisfies the constraints. For cloud.ip parameters, find a free external IP or allocate one. All other parameters must be specified explicitly. Guesses can be overridden if not appropriate for the test environment. The generation of tests for CaaS cluster types can be suppressed completely using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_caas_suite_enabled : false By default, a test case is generated for all cluster types except those that are explicitly disabled. This logic can be inverted, so that test cases are only generated for cluster types where they are explicitly enabled , using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_caas_default_test_case_enabled : false The following variables are available to affect the test generation for each cluster type: environments/my-site/inventory/group_vars/all/tests.yml # Indicates if the test case for the cluster type should be enabled generate_tests_caas_test_case_{cluster_type}_enabled : true # Used to override the value of the named parameter generate_tests_caas_test_case_{cluster_type}_param_{parameter_name} : \"<parameter_value>\" # Used to configure the expected title fragment for the named Zenith service generate_tests_caas_test_case_{cluster_type}_service_{service_name}_expected_title : \"<title fragment>\" # If a cluster takes a long time to deploy, the verify timeout can be increased generate_tests_caas_test_case_{cluster_type}_verify_timeout : \"45 minutes\" Warning If the cluster type or service name contain dashes ( - ), they will be replaced with underscores ( _ ). Kubernetes cluster templates For Kubernetes cluster templates, the generated test cases perform the following steps: Build the Kubernetes cluster configuration Create a Kubernetes cluster using the configuration Wait for the Kubernetes cluster to become Ready Check that the Zenith service for the Kubernetes Dashboard is working, if configured Check that the Zenith service for the monitoring is working, if configured Delete the Kubernetes cluster Verification of Zenith services Steps 4 and 5 use the same title-based verification as for CaaS clusters. No validation of actual behaviour is currently performed. The generation of tests for Kubernetes cluster templates can be suppressed completely using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_kubernetes_suite_enabled : false By default, a test case is generated for each active , i.e. non-deprecated, cluster template. The following variables are available to affect the test generation for Kubernetes cluster templates: environments/my-site/inventory/group_vars/all/tests.yml # When false (the default), test cases are generated for all # non-deprecated templates # # When true, test cases will only be generated for templates # that target the latest Kubernetes version generate_tests_kubernetes_test_cases_latest_only : false # The ID of the flavors to use for control plane and worker nodes respectively # By default, the smallest suitable size is used generate_tests_kubernetes_test_case_control_plane_size : generate_tests_kubernetes_test_case_worker_size : # The number of workers that should be deployed for each test case generate_tests_kubernetes_test_case_worker_count : 2 # Indicates whether the dashboard and monitoring should be enabled for tests generate_tests_kubernetes_test_case_dashboard_enabled : true generate_tests_kubernetes_test_case_monitoring_enabled : true Kubernetes app templates For Kubernetes app templates, we first deploy a Kubernetes cluster to host the apps. Once this cluster becomes Ready , the following steps are performed for each app: Create the app with inferred configuration Wait for the app to become Deployed For each specified Zenith service, check that it is accessible (Optional) Verify that the page title for the Zenith service contains some expected content Delete the app Verification of Zenith services As for other platform types, only title-based verification is performed for Zenith services. Overridding inferred configuration Currently, it is not possible to override the inferred configuration for Kubernetes apps. This may mean it is not currently possible to test some Kubernetes apps that have required parameters. Specifying Zenith services With Kubernetes apps, the expected Zenith services are not known up front as part of the app metadata, as they are for CaaS clusters. This means that the expected Zenith services for each app must be declared in config. The generation of tests for Kubernetes app templates can be suppressed completely using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_kubernetes_apps_suite_enabled : false By default, a test case is generated for all app templates except those that are explicitly disabled. This logic can be inverted, so that test cases are only generated for app templates where they are explicitly enabled , using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_kubernetes_apps_default_test_case_enabled : false The Kubernetes cluster on which the apps will be deployed is configured using the following variables: environments/my-site/inventory/group_vars/all/tests.yml # The name of the Kubernetes cluster template to use # If not given, the latest Kubernetes cluster template is used generate_tests_kubernetes_apps_k8s_template : # The ID of the flavors to use for control plane and worker nodes respectively # By default, the smallest suitable size is used generate_tests_kubernetes_apps_k8s_control_plane_size : generate_tests_kubernetes_apps_k8s_worker_size : # The number of workers that should be deployed for the cluster generate_tests_kubernetes_apps_k8s_worker_count : 2 The following variables are available to affect the test generation for each app template: environments/my-site/inventory/group_vars/all/tests.yml # Indicates if the test case for the cluster type should be enabled generate_tests_kubernetes_apps_test_case_{app_template}_enabled : true # The names of the expected Zenith services for the app generate_tests_kubernetes_apps_test_case_{app_template}_services : - service-1 - service-2 # The expected title fragment for the named Zenith service generate_tests_kubernetes_apps_test_case_{app_template}_service_{service_name}_expected_title : \"<title fragment>\" Warning When used in variable names, dashes ( - ) in the app template name or Zenith service names will be replaced with underscores ( _ ). Automated testing If you are using automated deployments , you can also automate tests against your deployments. As an example, the sample GitLab CI/CD configuration file includes jobs that run after each deployment to execute the configured tests against the deployment.","title":"Integration testing"},{"location":"deployment/testing/#integration-testing","text":"azimuth-ops is able to generate integration tests for an environment that can be executed against the deployed Azimuth to validate that it is working correctly. Currently, the following tests can be generated: Deploy and verify CaaS clusters Deploy and verify Kubernetes clusters Deploy and verify Kubernetes apps The generated test suites use Robot Framework to handle the execution of the tests and gathering and reporting of results. In Robot Framework, test cases are created by chaining together keywords imported from various libraries . Robot Framework also allows the creation of custom keyword libraries containing domain-specific keywords. The tests generated by azimuth-ops use keywords from a custom library for interacting with Azimuth, which in turn uses the Azimuth Python SDK to interact with the target Azimuth deployment. The result is that a typical test case looks something like this: Example of a generated test case *** Settings *** Name CaaS Library Azimuth Test Tags caas Test Timeout 15 minutes *** Test Cases *** Create workstation [ Tags ] workstation create ${ ctype } = Find Cluster Type By Name workstation ${ params } = Guess Parameter Values For Cluster Type ${ ctype } ${ cluster } = Create Cluster test-rbe6w ${ ctype.name } &{ params } Verify workstation [ Tags ] workstation verify ${ cluster } = Find Cluster By Name test-rbe6w ${ cluster } = Wait For Cluster Ready ${ cluster.id } ${ url } = Get Cluster Service URL ${ cluster } webconsole Open Zenith Service ${ url } Wait Until Page Title Contains Apache Guacamole ${ url } = Get Cluster Service URL ${ cluster } monitoring Open Zenith Service ${ url } Wait Until Page Title Contains Grafana Delete workstation [ Tags ] workstation delete ${ cluster } = Find Cluster By Name test-rbe6w Delete Cluster ${ cluster.id } This example will create a CaaS workstation using parameter values that are guessed based on the cluster type and target Azimuth. It then waits for the workstation to become Ready before verifying that the the Zenith services are behaving as expected. Finally, the workstation is deleted.","title":"Integration testing"},{"location":"deployment/testing/#prerequisites","text":"","title":"Prerequisites"},{"location":"deployment/testing/#config","text":"Make sure references to the application credential authenticator in your current config environment are either set to true or removed completely: environments/my-site/clouds.yaml azimuth_authenticator_appcred_enabled : true","title":"Config"},{"location":"deployment/testing/#firefox-package","text":"If you are running tests manually outside of a CI environment, you need to ensure you have the correct Firefox package installed for Selium. On Ubuntu: # Remove any old firefox packages sudo apt remove --purge firefox sudo snap remove --purge firefox # Add the Mozilla PPA wget -q https://packages.mozilla.org/apt/repo-signing-key.gpg -O- | sudo tee /etc/apt/keyrings/packages.mozilla.org.asc > /dev/null echo \"deb [signed-by=/etc/apt/keyrings/packages.mozilla.org.asc] https://packages.mozilla.org/apt mozilla main\" | sudo tee -a /etc/apt/sources.list.d/mozilla.list > /dev/null # Set firefox priority to ensure the Mozilla package is always preferred echo ' Package: * Pin: origin packages.mozilla.org Pin-Priority: 1000 ' | sudo tee /etc/apt/preferences.d/mozilla # Install firefox sudo apt update && sudo apt install firefox","title":"Firefox package"},{"location":"deployment/testing/#credentials-for-executing-tests","text":"The generated test suite uses an OpenStack application credential to authenticate with Azimuth, and this credential determines which project test platforms will be created in. This can be the same project that Azimuth is deployed in, but should ideally be a different project. Application credential must be unrestricted Because Azimuth creates an application credential for each deployed platform, the application credential used to authenticate with Azimuth to create test platforms must be able to create and delete other application credentials. This is referred to as unrestricted in OpenStack ( Unrestricted (dangerous) in the Horizon UI). The application credential used for running tests should be appended to the same clouds.yaml as the credential used to deploy Azimuth . environments/my-site/clouds.yaml clouds : # The main application credential used to deploy Azimuth openstack : auth : auth_url : https://openstack.example-cloud.org:5000 application_credential_id : \"<appcred id>\" application_credential_secret : \"<appcred secret>\" region_name : \"RegionOne\" interface : \"public\" identity_api_version : 3 auth_type : \"v3applicationcredential\" # The application credential used to create test platforms via Azimuth unrestricted : auth : auth_url : https://openstack.example-cloud.org:5000 application_credential_id : \"<appcred id>\" application_credential_secret : \"<appcred secret>\" region_name : \"RegionOne\" interface : \"public\" identity_api_version : 3 auth_type : \"v3applicationcredential\"","title":"Credentials for executing tests"},{"location":"deployment/testing/#generating-and-executing-tests","text":"Before tests can be generated and executed for an environment , the environment must be successfully deployed either manually or by automation . You must also have the Python dependencies installed on the machine that the tests will be run from. The easiest way to do this is using the ensure-venv script , however the requirements.txt can be used to install them into any Python environment: pip install -U pip pip install -r requirements.txt The integration test suite can then be generated and executed for the environment. Because the test generation connects to the Azimuth deployment to query the installed platform types, this must use the application credential that is used for deployment. The execution of the tests then uses the unrestricted application credential: # Activate the target environment source ./bin/activate my-site # Make sure you have the correct version of the Ansible collection installed ansible-galaxy install -f -r requirements.yml # Generate the test suite ansible-playbook stackhpc.azimuth_ops.generate_tests # Execute the test suite (using the unrestricted credential) OS_CLOUD = unrestricted ./bin/run-tests","title":"Generating and executing tests"},{"location":"deployment/testing/#configuring-test-generation","text":"By default, tests cases are generated for: All installed CaaS cluster types All installed active (i.e. non-deprecated) Kubernetes cluster types All installed Kubernetes app templates The following sections describe how to enable, disable and configure tests for the different types of platform. Tip The generate_tests role of azimuth-ops is responsible for generating the test cases. It has a large number of variables that can be used to tune the test generation. This document only discusses the most frequently used variables.","title":"Configuring test generation"},{"location":"deployment/testing/#caas-cluster-types","text":"Test cases for CaaS cluster types perform the following steps: Build the cluster parameters using best-effort guesses plus overrides Create a cluster using the parameters Wait for the cluster to become Ready Check that the Zenith services for the cluster are accessible (Optional) Verify that the page title for the Zenith service contains some expected content Delete the cluster Verification of Zenith service page titles Step 5 is used to verify that the Zenith authentication loop brings you back to the correct service. Currently, no other verification of the behaviour of the service is performed. Best-effort guesses for parameter values Best-effort guesses for parameter values are produced in the following way: If the parameter has a default, use that. For cloud.size parameters, use the smallest size that satisfies the constraints. For cloud.ip parameters, find a free external IP or allocate one. All other parameters must be specified explicitly. Guesses can be overridden if not appropriate for the test environment. The generation of tests for CaaS cluster types can be suppressed completely using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_caas_suite_enabled : false By default, a test case is generated for all cluster types except those that are explicitly disabled. This logic can be inverted, so that test cases are only generated for cluster types where they are explicitly enabled , using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_caas_default_test_case_enabled : false The following variables are available to affect the test generation for each cluster type: environments/my-site/inventory/group_vars/all/tests.yml # Indicates if the test case for the cluster type should be enabled generate_tests_caas_test_case_{cluster_type}_enabled : true # Used to override the value of the named parameter generate_tests_caas_test_case_{cluster_type}_param_{parameter_name} : \"<parameter_value>\" # Used to configure the expected title fragment for the named Zenith service generate_tests_caas_test_case_{cluster_type}_service_{service_name}_expected_title : \"<title fragment>\" # If a cluster takes a long time to deploy, the verify timeout can be increased generate_tests_caas_test_case_{cluster_type}_verify_timeout : \"45 minutes\" Warning If the cluster type or service name contain dashes ( - ), they will be replaced with underscores ( _ ).","title":"CaaS cluster types"},{"location":"deployment/testing/#kubernetes-cluster-templates","text":"For Kubernetes cluster templates, the generated test cases perform the following steps: Build the Kubernetes cluster configuration Create a Kubernetes cluster using the configuration Wait for the Kubernetes cluster to become Ready Check that the Zenith service for the Kubernetes Dashboard is working, if configured Check that the Zenith service for the monitoring is working, if configured Delete the Kubernetes cluster Verification of Zenith services Steps 4 and 5 use the same title-based verification as for CaaS clusters. No validation of actual behaviour is currently performed. The generation of tests for Kubernetes cluster templates can be suppressed completely using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_kubernetes_suite_enabled : false By default, a test case is generated for each active , i.e. non-deprecated, cluster template. The following variables are available to affect the test generation for Kubernetes cluster templates: environments/my-site/inventory/group_vars/all/tests.yml # When false (the default), test cases are generated for all # non-deprecated templates # # When true, test cases will only be generated for templates # that target the latest Kubernetes version generate_tests_kubernetes_test_cases_latest_only : false # The ID of the flavors to use for control plane and worker nodes respectively # By default, the smallest suitable size is used generate_tests_kubernetes_test_case_control_plane_size : generate_tests_kubernetes_test_case_worker_size : # The number of workers that should be deployed for each test case generate_tests_kubernetes_test_case_worker_count : 2 # Indicates whether the dashboard and monitoring should be enabled for tests generate_tests_kubernetes_test_case_dashboard_enabled : true generate_tests_kubernetes_test_case_monitoring_enabled : true","title":"Kubernetes cluster templates"},{"location":"deployment/testing/#kubernetes-app-templates","text":"For Kubernetes app templates, we first deploy a Kubernetes cluster to host the apps. Once this cluster becomes Ready , the following steps are performed for each app: Create the app with inferred configuration Wait for the app to become Deployed For each specified Zenith service, check that it is accessible (Optional) Verify that the page title for the Zenith service contains some expected content Delete the app Verification of Zenith services As for other platform types, only title-based verification is performed for Zenith services. Overridding inferred configuration Currently, it is not possible to override the inferred configuration for Kubernetes apps. This may mean it is not currently possible to test some Kubernetes apps that have required parameters. Specifying Zenith services With Kubernetes apps, the expected Zenith services are not known up front as part of the app metadata, as they are for CaaS clusters. This means that the expected Zenith services for each app must be declared in config. The generation of tests for Kubernetes app templates can be suppressed completely using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_kubernetes_apps_suite_enabled : false By default, a test case is generated for all app templates except those that are explicitly disabled. This logic can be inverted, so that test cases are only generated for app templates where they are explicitly enabled , using the following variable: environments/my-site/inventory/group_vars/all/tests.yml generate_tests_kubernetes_apps_default_test_case_enabled : false The Kubernetes cluster on which the apps will be deployed is configured using the following variables: environments/my-site/inventory/group_vars/all/tests.yml # The name of the Kubernetes cluster template to use # If not given, the latest Kubernetes cluster template is used generate_tests_kubernetes_apps_k8s_template : # The ID of the flavors to use for control plane and worker nodes respectively # By default, the smallest suitable size is used generate_tests_kubernetes_apps_k8s_control_plane_size : generate_tests_kubernetes_apps_k8s_worker_size : # The number of workers that should be deployed for the cluster generate_tests_kubernetes_apps_k8s_worker_count : 2 The following variables are available to affect the test generation for each app template: environments/my-site/inventory/group_vars/all/tests.yml # Indicates if the test case for the cluster type should be enabled generate_tests_kubernetes_apps_test_case_{app_template}_enabled : true # The names of the expected Zenith services for the app generate_tests_kubernetes_apps_test_case_{app_template}_services : - service-1 - service-2 # The expected title fragment for the named Zenith service generate_tests_kubernetes_apps_test_case_{app_template}_service_{service_name}_expected_title : \"<title fragment>\" Warning When used in variable names, dashes ( - ) in the app template name or Zenith service names will be replaced with underscores ( _ ).","title":"Kubernetes app templates"},{"location":"deployment/testing/#automated-testing","text":"If you are using automated deployments , you can also automate tests against your deployments. As an example, the sample GitLab CI/CD configuration file includes jobs that run after each deployment to execute the configured tests against the deployment.","title":"Automated testing"},{"location":"developing/","text":"Developing Azimuth An Azimuth deployment consists of several interdependent components, some of which are Azimuth-specific and some of which are third-party components. Plugging code under development into such a system can be tricky, making development difficult and slow. Deploying a dev instance In order to develop Azimuth, you first need a running Azimuth instance. Each developer should have their own independent instance of Azimuth for development, as during development they will make changes to the running Azimuth components that may conflict with or break things for others. Creating a dev environment Azimuth supports using a single configuration environment to deploy multiple independent Azimuth instances. When activating an environment , a unique instance name can be given as a second argument to the activate script, e.g.: # Activating an environment with a unique instance name source ./bin/activate my-environment jbloggs In order for an environment to be used in this way, it must be specially prepared to be more dynamic than an environment that you would use for staging or production. In particular, only single node deployments are usable in this way, as HA deployments do not support dynamically allocating a floating IP for the ingress controller. It is recommended that you create an environment in your Azimuth configuration repository for doing Azimuth development on your cloud. This environment should include any site-specific customisations that are required, usually by building on a site mixin . The demo environment is a good starting point for this, as it is designed to be flexible and dynamic. Producing unique values in your Azimuth configuration The Ansible variable azimuth_environment contains the unique instance name, and can be used in other variables in your configuration where a unique value is required for each developer environment. Developers should use their own application credential You should not include an application credential in your development environment. Instead, each developer can use their own application credential as described in the next section. Using the dev environment The following instructions assume that your Azimuth configuration contains a developer environment called dev . It is assumed that you have your Azimuth configuration checked out and that you have an application credential for the target cloud. To stand up your developer-specific Azimuth instance, using the dev environment, use the following: # Set OpenStack configuration variables export OS_CLOUD = openstack export OS_CLIENT_CONFIG_FILE = /path/to/clouds.yaml # Activate the dev config environment with a specific instance name # # This means that resources created for the instance will not collide # with other deployments that use the dev environment source ./bin/activate dev jbloggs-dev # Install Azimuth as usual ansible-galaxy install -f -r requirements.yml ansible-playbook stackhpc.azimuth_ops.provision Developing Azimuth components Azimuth has a number of components, mostly written in Python: Azimuth API and UI - user-facing API and UI Azimuth CaaS operator - Kubernetes operator implementing CaaS functionality Azimuth CAPI operator - Kubernetes operator implementing Kubernetes and Kubernetes App functionality Azimuth identity operator - Kubernetes operator implementing platform identity Azimuth schedule operator - Kubernetes operator implementing platform scheduling Zenith - secure, tunnelling application proxy used to expose platform services Cluster API addon provider - addons for Cluster API clusters Cluster API janitor for OpenStack - resource cleanup for Cluster API clusters on OpenStack clouds It is useful to develop these components in the context of a running Azimuth installation, as they have dependencies on each other. To enable this, Azimuth uses Tilt to provide a developer environment where code under development is automatically built and injected into a live system that you can interact with. Tilt provides a dashboard that can be used to drill down into build failures and the logs of the components under development. Prerequisites In order to use Tilt to develop Azimuth, the following tools must be available on your development machine (in addition to those required to install Azimuth itself): The Tilt CLI A docker command, e.g. Docker Desktop The kubectl command The Helm CLI For developing the Azimuth UI, the following are also required: node.js The Yarn Classic package manager Configuring a container registry Azimuth's Tilt configuration looks for a file called tilt-settings.yaml that defines settings for the development environment. This file is specific to you and should not be added to version control (it is specified in .gitignore ). In order to get the code under development into your running Azimuth instance, Tilt must have access to a container registry that is accessible to both your development machine and the Azimuth instance. In response to code changes, Tilt will automatically build and push images to this registry and then configure the Azimuth instance to use them. To configure the prefix for images built by Tilt, use the following setting: tilt-settings.yaml # Images will be pushed to: # ghcr.io/jbloggs/azimuth-api # ghcr.io/jbloggs/azimuth-ui # ghcr.io/jbloggs/azimuth-caas-operator # ... image_prefix : ghcr.io/jbloggs Tip A good candidate for this is to use GitHub Packages with your user account, as in the example above. This means that development builds do not require access to or clutter up the production repositories. When using GitHub Packages, the repositories that are created by Tilt when it builds images for the first time will be private. You must log into GitHub and make them public before your Azimuth instance can use them. Until you do this, you will see image pull errors in the Tilt interface. Using the Tilt environment Tilt will look for checkouts of Azimuth components as siblings of your Azimuth configuration and include them in your development environment. For example, the following directory structure will result in a development environment where changes to the Azimuth API, UI and CaaS operators are built and pushed into your development Azimuth instance for testing: . \u251c\u2500\u2500 azimuth \u2502 \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 chart \u2502 \u251c\u2500\u2500 ui \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 azimuth-caas-operator \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u251c\u2500\u2500 azimuth_caas_operator \u2502 \u251c\u2500\u2500 charts \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 myorg-azimuth-config \u251c\u2500\u2500 Tiltfile \u251c\u2500\u2500 bin \u251c\u2500\u2500 environments \u251c\u2500\u2500 tilt-settings.yaml \u2514\u2500\u2500 ... If you wish to prevent a particular component being included in your development environment, even if the checkout exists as a sibling, you can configure this using the enabled flag for the component in tilt-settings.yaml . For example, the following would prevent the CaaS operator from being included in the development environment, even with the directory structure above: tilt-settings.yaml components : azimuth-caas-operator : enabled : false Once you have checked out all of the components that you want to develop, you can start the development environment using: ./bin/tilt-up This will configure Tilt to connect to your Azimuth instance and begin watching your local checkouts for changes. When a change is detected, Tilt will build and push an image for the component before reconfiguring the Helm release for the component to point to the new image. Once this process is complete, you can interact with your changes using the Azimuth UI for your instance. Tip Press the space bar to launch the Tilt user interface . When the tilt-up command is terminated, all of the Helm releases for the components that were under development are rolled back to the version that was running before the command was started. Local UI development Because of how the user interface is optimised for production, the container image build for the Azimuth UI is very slow even for a minor change. Because of this, the usual Tilt flow of build/push/deploy is not suitable for UI development. To improve the feedback cycle for UI development, the Azimuth Tilt environment also builds and runs the Azimuth UI locally using the webpack DevServer . The UI communicates with the Azimuth API on your Azimuth dev instance using a forwarded port (this is necessary in order for the cookie-based authentication to work properly). The local version of the UI is available at http://localhost:3000 . Note The UI container image is still built, pushed and deployed in the background. However changes made to the JS files will be visible in the local version much faster.","title":"Developing Azimuth"},{"location":"developing/#developing-azimuth","text":"An Azimuth deployment consists of several interdependent components, some of which are Azimuth-specific and some of which are third-party components. Plugging code under development into such a system can be tricky, making development difficult and slow.","title":"Developing Azimuth"},{"location":"developing/#deploying-a-dev-instance","text":"In order to develop Azimuth, you first need a running Azimuth instance. Each developer should have their own independent instance of Azimuth for development, as during development they will make changes to the running Azimuth components that may conflict with or break things for others.","title":"Deploying a dev instance"},{"location":"developing/#creating-a-dev-environment","text":"Azimuth supports using a single configuration environment to deploy multiple independent Azimuth instances. When activating an environment , a unique instance name can be given as a second argument to the activate script, e.g.: # Activating an environment with a unique instance name source ./bin/activate my-environment jbloggs In order for an environment to be used in this way, it must be specially prepared to be more dynamic than an environment that you would use for staging or production. In particular, only single node deployments are usable in this way, as HA deployments do not support dynamically allocating a floating IP for the ingress controller. It is recommended that you create an environment in your Azimuth configuration repository for doing Azimuth development on your cloud. This environment should include any site-specific customisations that are required, usually by building on a site mixin . The demo environment is a good starting point for this, as it is designed to be flexible and dynamic. Producing unique values in your Azimuth configuration The Ansible variable azimuth_environment contains the unique instance name, and can be used in other variables in your configuration where a unique value is required for each developer environment. Developers should use their own application credential You should not include an application credential in your development environment. Instead, each developer can use their own application credential as described in the next section.","title":"Creating a dev environment"},{"location":"developing/#using-the-dev-environment","text":"The following instructions assume that your Azimuth configuration contains a developer environment called dev . It is assumed that you have your Azimuth configuration checked out and that you have an application credential for the target cloud. To stand up your developer-specific Azimuth instance, using the dev environment, use the following: # Set OpenStack configuration variables export OS_CLOUD = openstack export OS_CLIENT_CONFIG_FILE = /path/to/clouds.yaml # Activate the dev config environment with a specific instance name # # This means that resources created for the instance will not collide # with other deployments that use the dev environment source ./bin/activate dev jbloggs-dev # Install Azimuth as usual ansible-galaxy install -f -r requirements.yml ansible-playbook stackhpc.azimuth_ops.provision","title":"Using the dev environment"},{"location":"developing/#developing-azimuth-components","text":"Azimuth has a number of components, mostly written in Python: Azimuth API and UI - user-facing API and UI Azimuth CaaS operator - Kubernetes operator implementing CaaS functionality Azimuth CAPI operator - Kubernetes operator implementing Kubernetes and Kubernetes App functionality Azimuth identity operator - Kubernetes operator implementing platform identity Azimuth schedule operator - Kubernetes operator implementing platform scheduling Zenith - secure, tunnelling application proxy used to expose platform services Cluster API addon provider - addons for Cluster API clusters Cluster API janitor for OpenStack - resource cleanup for Cluster API clusters on OpenStack clouds It is useful to develop these components in the context of a running Azimuth installation, as they have dependencies on each other. To enable this, Azimuth uses Tilt to provide a developer environment where code under development is automatically built and injected into a live system that you can interact with. Tilt provides a dashboard that can be used to drill down into build failures and the logs of the components under development.","title":"Developing Azimuth components"},{"location":"developing/#prerequisites","text":"In order to use Tilt to develop Azimuth, the following tools must be available on your development machine (in addition to those required to install Azimuth itself): The Tilt CLI A docker command, e.g. Docker Desktop The kubectl command The Helm CLI For developing the Azimuth UI, the following are also required: node.js The Yarn Classic package manager","title":"Prerequisites"},{"location":"developing/#configuring-a-container-registry","text":"Azimuth's Tilt configuration looks for a file called tilt-settings.yaml that defines settings for the development environment. This file is specific to you and should not be added to version control (it is specified in .gitignore ). In order to get the code under development into your running Azimuth instance, Tilt must have access to a container registry that is accessible to both your development machine and the Azimuth instance. In response to code changes, Tilt will automatically build and push images to this registry and then configure the Azimuth instance to use them. To configure the prefix for images built by Tilt, use the following setting: tilt-settings.yaml # Images will be pushed to: # ghcr.io/jbloggs/azimuth-api # ghcr.io/jbloggs/azimuth-ui # ghcr.io/jbloggs/azimuth-caas-operator # ... image_prefix : ghcr.io/jbloggs Tip A good candidate for this is to use GitHub Packages with your user account, as in the example above. This means that development builds do not require access to or clutter up the production repositories. When using GitHub Packages, the repositories that are created by Tilt when it builds images for the first time will be private. You must log into GitHub and make them public before your Azimuth instance can use them. Until you do this, you will see image pull errors in the Tilt interface.","title":"Configuring a container registry"},{"location":"developing/#using-the-tilt-environment","text":"Tilt will look for checkouts of Azimuth components as siblings of your Azimuth configuration and include them in your development environment. For example, the following directory structure will result in a development environment where changes to the Azimuth API, UI and CaaS operators are built and pushed into your development Azimuth instance for testing: . \u251c\u2500\u2500 azimuth \u2502 \u251c\u2500\u2500 api \u2502 \u251c\u2500\u2500 chart \u2502 \u251c\u2500\u2500 ui \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 azimuth-caas-operator \u2502 \u251c\u2500\u2500 Dockerfile \u2502 \u251c\u2500\u2500 azimuth_caas_operator \u2502 \u251c\u2500\u2500 charts \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 myorg-azimuth-config \u251c\u2500\u2500 Tiltfile \u251c\u2500\u2500 bin \u251c\u2500\u2500 environments \u251c\u2500\u2500 tilt-settings.yaml \u2514\u2500\u2500 ... If you wish to prevent a particular component being included in your development environment, even if the checkout exists as a sibling, you can configure this using the enabled flag for the component in tilt-settings.yaml . For example, the following would prevent the CaaS operator from being included in the development environment, even with the directory structure above: tilt-settings.yaml components : azimuth-caas-operator : enabled : false Once you have checked out all of the components that you want to develop, you can start the development environment using: ./bin/tilt-up This will configure Tilt to connect to your Azimuth instance and begin watching your local checkouts for changes. When a change is detected, Tilt will build and push an image for the component before reconfiguring the Helm release for the component to point to the new image. Once this process is complete, you can interact with your changes using the Azimuth UI for your instance. Tip Press the space bar to launch the Tilt user interface . When the tilt-up command is terminated, all of the Helm releases for the components that were under development are rolled back to the version that was running before the command was started.","title":"Using the Tilt environment"},{"location":"developing/#local-ui-development","text":"Because of how the user interface is optimised for production, the container image build for the Azimuth UI is very slow even for a minor change. Because of this, the usual Tilt flow of build/push/deploy is not suitable for UI development. To improve the feedback cycle for UI development, the Azimuth Tilt environment also builds and runs the Azimuth UI locally using the webpack DevServer . The UI communicates with the Azimuth API on your Azimuth dev instance using a forwarded port (this is necessary in order for the cookie-based authentication to work properly). The local version of the UI is available at http://localhost:3000 . Note The UI container image is still built, pushed and deployed in the background. However changes made to the JS files will be visible in the local version much faster.","title":"Local UI development"},{"location":"repository/","text":"Azimuth configuration repository The azimuth-config repository provides best-practice configuration for Azimuth deployments that can be inherited by site-specific configuration repositories using Git . Using Git makes it easy to pick up new Azimuth releases when they become available. Initial repository setup First make an empty Git repository using your service of choice (e.g. GitHub or GitLab ), then execute the following commands to turn the new empty repository into a copy of the azimuth-config repository: # Clone the azimuth-config repository git clone https://github.com/stackhpc/azimuth-config.git my-azimuth-config cd my-azimuth-config # Maintain the existing origin remote as upstream git remote rename origin upstream # Create a new origin remote for the repository location git remote add origin git@<repo location>/my-azimuth-config.git # Checkout stable to get the latest release git checkout stable # Create a new main branch from stable # This will be the branch that is deployed into production git checkout -b main # Push the main branch to the origin git push --set-upstream origin main You now have an independent copy of the azimuth-config repository that has a link back to the source repository via the upstream remote. Branch protection rules It is a good idea to apply branch protection rules to the main branch that enforce that all changes are made via a merge (or pull) request. This should ensure that changes are not accidentally pushed into production without being reviewed. Instructions are available on how to set this up for GitHub or GitLab . Creating a new environment Your new repository does not yet contain any site-specific configuration. The best way to do this is to copy the example environment as a starting point: cp -r ./environments/example ./environments/my-site Tip Copying the example environment, rather than just renaming it, avoids conflicts when synchronising changes from the azimuth-config repository where the example environment has changed. Once you have your new environment, you can make the required changes for your site. As you make changes to your environment, remember to commit and push them regularly: git add ./environments/my-site git commit -m \"Made some changes to my environment\" git push Making changes to your configuration Once you have an environment deployed, it is recommended to use a feature branch workflow when making changes to your configuration repository. Automated deployments The feature branch workflow works particularly well when you use a continuous delivery approach to automate deployments . In this workflow, the required changes are made on a branch in the configuration repository. Once you are happy with the changes, you create a merge (or pull) request proposing the changes to main . These changes can then be reviewed before being merged to main . If you have automated deployments, the branch may even get a dynamic environment created for it where the result of the changes can be verified before the merge takes place. Upgrading to a new Azimuth release When a new Azimuth release becomes available, you will need to synchronise the changes from azimuth-config into your site configuration repository in order to pick up new component versions, upgraded dependencies and new images. Choosing a release The available releases, with associated release notes, can be reviewed on the Azimuth releases page . Automating upgrades If you have automated deployments, which is recommended for a production installation, this process can also be automated . To upgrade your Azimuth configuration to a new release, use the following steps to create a new branch containing the upgrade: # Make sure the local checkout is up to date with any site-specific changes git checkout main git pull # Fetch the tags from the upstream repo git remote update # Create a new branch to contain the Azimuth upgrade git checkout -b upgrade/ $RELEASE_TAG # Merge in the tag for the new release git merge $RELEASE_TAG At this point, you will need to fix any conflicts where you have made changes to the same files that have been changed by azimuth-config . Avoiding conflicts To avoid conflicts, you should never directly modify any files that come from azimuth-config - instead you should use the environment layering to override variables where required, and copy files if necessary. Once any conflicts have been resolved, you can commit and push the changes: git commit -m \"Upgrade Azimuth to $RELEASE_TAG \" git push --set-upstream origin upgrade/ $RELEASE_TAG You can now open a merge (or pull) request proposing the upgrade to your main branch that can be reviewed like any other.","title":"Azimuth configuration repository"},{"location":"repository/#azimuth-configuration-repository","text":"The azimuth-config repository provides best-practice configuration for Azimuth deployments that can be inherited by site-specific configuration repositories using Git . Using Git makes it easy to pick up new Azimuth releases when they become available.","title":"Azimuth configuration repository"},{"location":"repository/#initial-repository-setup","text":"First make an empty Git repository using your service of choice (e.g. GitHub or GitLab ), then execute the following commands to turn the new empty repository into a copy of the azimuth-config repository: # Clone the azimuth-config repository git clone https://github.com/stackhpc/azimuth-config.git my-azimuth-config cd my-azimuth-config # Maintain the existing origin remote as upstream git remote rename origin upstream # Create a new origin remote for the repository location git remote add origin git@<repo location>/my-azimuth-config.git # Checkout stable to get the latest release git checkout stable # Create a new main branch from stable # This will be the branch that is deployed into production git checkout -b main # Push the main branch to the origin git push --set-upstream origin main You now have an independent copy of the azimuth-config repository that has a link back to the source repository via the upstream remote. Branch protection rules It is a good idea to apply branch protection rules to the main branch that enforce that all changes are made via a merge (or pull) request. This should ensure that changes are not accidentally pushed into production without being reviewed. Instructions are available on how to set this up for GitHub or GitLab .","title":"Initial repository setup"},{"location":"repository/#creating-a-new-environment","text":"Your new repository does not yet contain any site-specific configuration. The best way to do this is to copy the example environment as a starting point: cp -r ./environments/example ./environments/my-site Tip Copying the example environment, rather than just renaming it, avoids conflicts when synchronising changes from the azimuth-config repository where the example environment has changed. Once you have your new environment, you can make the required changes for your site. As you make changes to your environment, remember to commit and push them regularly: git add ./environments/my-site git commit -m \"Made some changes to my environment\" git push","title":"Creating a new environment"},{"location":"repository/#making-changes-to-your-configuration","text":"Once you have an environment deployed, it is recommended to use a feature branch workflow when making changes to your configuration repository. Automated deployments The feature branch workflow works particularly well when you use a continuous delivery approach to automate deployments . In this workflow, the required changes are made on a branch in the configuration repository. Once you are happy with the changes, you create a merge (or pull) request proposing the changes to main . These changes can then be reviewed before being merged to main . If you have automated deployments, the branch may even get a dynamic environment created for it where the result of the changes can be verified before the merge takes place.","title":"Making changes to your configuration"},{"location":"repository/#upgrading-to-a-new-azimuth-release","text":"When a new Azimuth release becomes available, you will need to synchronise the changes from azimuth-config into your site configuration repository in order to pick up new component versions, upgraded dependencies and new images. Choosing a release The available releases, with associated release notes, can be reviewed on the Azimuth releases page . Automating upgrades If you have automated deployments, which is recommended for a production installation, this process can also be automated . To upgrade your Azimuth configuration to a new release, use the following steps to create a new branch containing the upgrade: # Make sure the local checkout is up to date with any site-specific changes git checkout main git pull # Fetch the tags from the upstream repo git remote update # Create a new branch to contain the Azimuth upgrade git checkout -b upgrade/ $RELEASE_TAG # Merge in the tag for the new release git merge $RELEASE_TAG At this point, you will need to fix any conflicts where you have made changes to the same files that have been changed by azimuth-config . Avoiding conflicts To avoid conflicts, you should never directly modify any files that come from azimuth-config - instead you should use the environment layering to override variables where required, and copy files if necessary. Once any conflicts have been resolved, you can commit and push the changes: git commit -m \"Upgrade Azimuth to $RELEASE_TAG \" git push --set-upstream origin upgrade/ $RELEASE_TAG You can now open a merge (or pull) request proposing the upgrade to your main branch that can be reviewed like any other.","title":"Upgrading to a new Azimuth release"},{"location":"repository/opentofu/","text":"OpenTofu state azimuth-ops uses OpenTofu to manage the K3s node in both the single node and high-availability deployment methods. Info OpenTofu is an open-source fork of Terraform . In most cases, variable names etc. used in Azimuth currently still refer to terraform_ . In order to keep track of the resources that it has created, and how they map to the resources in the OpenTofu configuration generated by azimuth-ops , OpenTofu must store its state somewhere. The location of the state is determined by the backend configuration . Each environment in an azimuth-config repository has a corresponding OpenTofu state, and they are independent from each other. Local state By default azimuth-ops will use the local backend, which stores the OpenTofu state as a file on the local disk in the .work directory. This requires no explicit configuration, but comes with the usual caveats about keeping important state on your local machine. Not suitable for production Local state is sufficient for a demonstration or evaluation, but for a shared or production deployment it is recommended to use remote state. Remote state OpenTofu supports a number of remote backends that can be used to persist state independently of where a deployment is run. This allows deployments to be made from anywhere that can access the state without corrupting or conflicting with any existing resources from previous deployments. Tip If you want to use the same remote backend configuration for multiple environments , consider using a site mixin environment to avoid specifying the configuration multiple times. Warning In order to avoid multiple writers when using remote state, it is recommended to use a backend that supports state locking . Secret configuration Some of the configuration variables for remote backends, e.g. passwords and keys, should be kept secret. If you want to keep such variables in Git - which is recommended where possible - then they must be encrypted . GitLab Tip This is the recommended option if you are using GitLab for your config repository. If you are using GitLab to host your configuration repository, either gitlab.com or self-hosted, you can use GitLab-managed Terraform state to store the states for your environments. GitLab provides a HTTP backend that can be configured as follows: environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : http # The API base URL for the target GitLab project # For a self-hosted GitLab instance, replace gitlab.com with your domain gitlab_project_url : \"https://gitlab.com/api/v4/projects/<project id>\" # The state endpoint for the environment # # Using the azimuth_environment variable as the state name means that each # concrete environment gets a separate managed Terraform state even if this # configuration is in a shared mixin environment terraform_http_address : \"{{ gitlab_project_url }}/terraform/state/{{ azimuth_environment }}\" # The state-locking and unlocking endpoints for the environment terraform_http_lock_address : \"{{ terraform_http_address }}/lock\" terraform_http_lock_method : POST terraform_http_unlock_address : \"{{ terraform_http_lock_address }}\" terraform_http_unlock_method : DELETE terraform_backend_config : address : \"{{ terraform_http_address }}\" lock_address : \"{{ terraform_http_lock_address }}\" lock_method : \"{{ terraform_http_lock_method }}\" unlock_address : \"{{ terraform_http_unlock_address }}\" unlock_method : \"{{ terraform_http_unlock_method }}\" Tip If you want to use the same backend configuration for multiple environments , consider using a site mixin environment to avoid specifying the configuration multiple times. The username and password (or token) that are used to authenticate with GitLab to manage the states are set using the TF_HTTP_USERNAME and TF_HTTP_PASSWORD environment variables respectively. If you are using GitLab CI/CD to automate deployments , then the pipeline will be issued with a suitable token. The sample configuration includes configuration to populate these variables using this token. If you are not using automation but your GitLab installation has project access tokens available, you can configure a project access token and store it (encrypted!) in the env.secret file, referencing the bot username: env.secret TF_HTTP_USERNAME = \"project_<id>_bot\" TF_HTTP_PASSWORD = \"<project access token>\" If you need to access an environment deployed using automation, or you do not have project access tokens available, then you can use a Personal access token , which at least avoids using your password. Never commit personal access tokens You should never commit a personal access token to the configuration repository, even encrypted, because it is not possible to set a project scope. If using a personal access token, you should export the relevant variables before activating an environment: export TF_HTTP_USERNAME = \"<username>\" export TF_HTTP_PASSWORD = \"<token>\" source ./bin/activate my-site S3 OpenTofu also has an S3 backend that is able to store state in any S3-compatible object store, such as Amazon S3 or Ceph Object Gateway . Depending on the provider of your object store, the specific configuration options in the following section may differ. The example configuration shown here is for a Ceph object store as Ceph is often used together with OpenStack. See the OpenTofu docs for the S3 backend for all the available options. environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : s3 # The endpoint of the object store terraform_s3_endpoint : object.example.com # The region to use # Ceph does not normally use the region, but OpenTofu requires it terraform_s3_region : not-used-but-required terraform_s3_skip_region_validation : \"true\" # The bucket to put OpenTofu states in # NOTE: This bucket must already exist - it will not be created by OpenTofu terraform_s3_bucket : azimuth-opentofu-states # The key to use for the state for the environment # # Using the azimuth_environment variable in the key means that the state # for each concrete environment is stored in a separate key, even if this # configuration is in a shared mixin environment terraform_s3_key : \"{{ azimuth_environment }}.tfstate\" # The STS API doesn't exist for Ceph terraform_s3_skip_credentials_validation : \"true\" # Tell OpenTofu to use path-style URLs, e.g. <host>/<bucket>, instead of # subdomain-style URLs, e.g. <bucket>.<host> terraform_s3_force_path_style : \"true\" terraform_backend_config : endpoint : \"{{ terraform_s3_endpoint }}\" region : \"{{ terraform_s3_region }}\" bucket : \"{{ terraform_s3_bucket }}\" key : \"{{ terraform_s3_key }}\" skip_credentials_validation : \"{{ terraform_s3_skip_credentials_validation }}\" force_path_style : \"{{ terraform_s3_force_path_style }}\" skip_region_validation : \"{{ terraform_s3_skip_region_validation }}\" The S3 credentials (access key ID and secret), can be specified either as environment variables or as Ansible variables. To use environment variables, just place the credentials in env.secret : env.secret AWS_ACCESS_KEY_ID = \"<access key id>\" AWS_SECRET_ACCESS_KEY = \"<secret key>\" To use Ansible variables, they should be added to secrets.yml and referenced in the terraform_backend_config variable: environments/my-site/inventory/group_vars/all/secrets.yml terraform_s3_access_key : \"<access key id>\" terraform_s3_secret_key : \"<secret key>\" environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_config : # ... other options ... access_key : \"{{ terraform_s3_access_key }}\" secret_key : \"{{ terraform_s3_secret_key }}\"","title":"OpenTofu state"},{"location":"repository/opentofu/#opentofu-state","text":"azimuth-ops uses OpenTofu to manage the K3s node in both the single node and high-availability deployment methods. Info OpenTofu is an open-source fork of Terraform . In most cases, variable names etc. used in Azimuth currently still refer to terraform_ . In order to keep track of the resources that it has created, and how they map to the resources in the OpenTofu configuration generated by azimuth-ops , OpenTofu must store its state somewhere. The location of the state is determined by the backend configuration . Each environment in an azimuth-config repository has a corresponding OpenTofu state, and they are independent from each other.","title":"OpenTofu state"},{"location":"repository/opentofu/#local-state","text":"By default azimuth-ops will use the local backend, which stores the OpenTofu state as a file on the local disk in the .work directory. This requires no explicit configuration, but comes with the usual caveats about keeping important state on your local machine. Not suitable for production Local state is sufficient for a demonstration or evaluation, but for a shared or production deployment it is recommended to use remote state.","title":"Local state"},{"location":"repository/opentofu/#remote-state","text":"OpenTofu supports a number of remote backends that can be used to persist state independently of where a deployment is run. This allows deployments to be made from anywhere that can access the state without corrupting or conflicting with any existing resources from previous deployments. Tip If you want to use the same remote backend configuration for multiple environments , consider using a site mixin environment to avoid specifying the configuration multiple times. Warning In order to avoid multiple writers when using remote state, it is recommended to use a backend that supports state locking . Secret configuration Some of the configuration variables for remote backends, e.g. passwords and keys, should be kept secret. If you want to keep such variables in Git - which is recommended where possible - then they must be encrypted .","title":"Remote state"},{"location":"repository/opentofu/#gitlab","text":"Tip This is the recommended option if you are using GitLab for your config repository. If you are using GitLab to host your configuration repository, either gitlab.com or self-hosted, you can use GitLab-managed Terraform state to store the states for your environments. GitLab provides a HTTP backend that can be configured as follows: environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : http # The API base URL for the target GitLab project # For a self-hosted GitLab instance, replace gitlab.com with your domain gitlab_project_url : \"https://gitlab.com/api/v4/projects/<project id>\" # The state endpoint for the environment # # Using the azimuth_environment variable as the state name means that each # concrete environment gets a separate managed Terraform state even if this # configuration is in a shared mixin environment terraform_http_address : \"{{ gitlab_project_url }}/terraform/state/{{ azimuth_environment }}\" # The state-locking and unlocking endpoints for the environment terraform_http_lock_address : \"{{ terraform_http_address }}/lock\" terraform_http_lock_method : POST terraform_http_unlock_address : \"{{ terraform_http_lock_address }}\" terraform_http_unlock_method : DELETE terraform_backend_config : address : \"{{ terraform_http_address }}\" lock_address : \"{{ terraform_http_lock_address }}\" lock_method : \"{{ terraform_http_lock_method }}\" unlock_address : \"{{ terraform_http_unlock_address }}\" unlock_method : \"{{ terraform_http_unlock_method }}\" Tip If you want to use the same backend configuration for multiple environments , consider using a site mixin environment to avoid specifying the configuration multiple times. The username and password (or token) that are used to authenticate with GitLab to manage the states are set using the TF_HTTP_USERNAME and TF_HTTP_PASSWORD environment variables respectively. If you are using GitLab CI/CD to automate deployments , then the pipeline will be issued with a suitable token. The sample configuration includes configuration to populate these variables using this token. If you are not using automation but your GitLab installation has project access tokens available, you can configure a project access token and store it (encrypted!) in the env.secret file, referencing the bot username: env.secret TF_HTTP_USERNAME = \"project_<id>_bot\" TF_HTTP_PASSWORD = \"<project access token>\" If you need to access an environment deployed using automation, or you do not have project access tokens available, then you can use a Personal access token , which at least avoids using your password. Never commit personal access tokens You should never commit a personal access token to the configuration repository, even encrypted, because it is not possible to set a project scope. If using a personal access token, you should export the relevant variables before activating an environment: export TF_HTTP_USERNAME = \"<username>\" export TF_HTTP_PASSWORD = \"<token>\" source ./bin/activate my-site","title":"GitLab"},{"location":"repository/opentofu/#s3","text":"OpenTofu also has an S3 backend that is able to store state in any S3-compatible object store, such as Amazon S3 or Ceph Object Gateway . Depending on the provider of your object store, the specific configuration options in the following section may differ. The example configuration shown here is for a Ceph object store as Ceph is often used together with OpenStack. See the OpenTofu docs for the S3 backend for all the available options. environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_type : s3 # The endpoint of the object store terraform_s3_endpoint : object.example.com # The region to use # Ceph does not normally use the region, but OpenTofu requires it terraform_s3_region : not-used-but-required terraform_s3_skip_region_validation : \"true\" # The bucket to put OpenTofu states in # NOTE: This bucket must already exist - it will not be created by OpenTofu terraform_s3_bucket : azimuth-opentofu-states # The key to use for the state for the environment # # Using the azimuth_environment variable in the key means that the state # for each concrete environment is stored in a separate key, even if this # configuration is in a shared mixin environment terraform_s3_key : \"{{ azimuth_environment }}.tfstate\" # The STS API doesn't exist for Ceph terraform_s3_skip_credentials_validation : \"true\" # Tell OpenTofu to use path-style URLs, e.g. <host>/<bucket>, instead of # subdomain-style URLs, e.g. <bucket>.<host> terraform_s3_force_path_style : \"true\" terraform_backend_config : endpoint : \"{{ terraform_s3_endpoint }}\" region : \"{{ terraform_s3_region }}\" bucket : \"{{ terraform_s3_bucket }}\" key : \"{{ terraform_s3_key }}\" skip_credentials_validation : \"{{ terraform_s3_skip_credentials_validation }}\" force_path_style : \"{{ terraform_s3_force_path_style }}\" skip_region_validation : \"{{ terraform_s3_skip_region_validation }}\" The S3 credentials (access key ID and secret), can be specified either as environment variables or as Ansible variables. To use environment variables, just place the credentials in env.secret : env.secret AWS_ACCESS_KEY_ID = \"<access key id>\" AWS_SECRET_ACCESS_KEY = \"<secret key>\" To use Ansible variables, they should be added to secrets.yml and referenced in the terraform_backend_config variable: environments/my-site/inventory/group_vars/all/secrets.yml terraform_s3_access_key : \"<access key id>\" terraform_s3_secret_key : \"<secret key>\" environments/my-site/inventory/group_vars/all/variables.yml terraform_backend_config : # ... other options ... access_key : \"{{ terraform_s3_access_key }}\" secret_key : \"{{ terraform_s3_secret_key }}\"","title":"S3"},{"location":"repository/secrets/","text":"Managing secrets Each Azimuth environment contains a small number of files and variable values that must be kept secret. However we would also like to have these files and variables in version control so that we can track when they have changed and share them with others in our team. The way to achieve both these goals is to encrypt the secrets when at rest in the Git repository. For an Azimuth configuration repository, the recommended method for doing this is using git-crypt . git-crypt provides transparent encryption and decryption of files in a Git repository using Git filters . This allows for a mix of encrypted and unencrypted content in the same repository, which is exactly what we want for an Azimuth configuration. By encrypting files when they are committed and decrypting them when they are checked out, git-crypt allows you to work as if the encryption is not present while being confident that your secrets remain private. Team members can also work on the public parts of the repository without decrypting the private parts if you wish to maintain separation of privilege. git-crypt works on entire files, not at the variable level, so it is recommended that encrypted files contain only secret values so that information is not hidden unnecessarily. To this end, a typical azimuth-config environment for a site will have one or more group_vars files in the inventory that are unencrypted, but secret values will be placed in a secrets.yaml that is encrypted. Initialising git-crypt To initialise git-crypt for your config repository, first make sure that the CLI is installed. This can be installed using the package manager on most major Linux operating systems, using Homebrew on Mac OSX or by building from source . Then execute the following command to begin encrypting files: git-crypt init Danger If you lose access to the key that git-crypt generates to encrypt your repository, you will be locked out of the repository for good. It is recommended that you export the key (see below) and store it somewhere safe, e.g. in your organisation's secret store. Verifying which files are encrypted azimuth-config contains a .gitattributes file that ensures all clouds.yaml , secrets.yaml , env.secret and TLS key files in the repository will be encrypted (except those for the example environment). If required, you can add additional patterns to the file to encrypt other files. You can check the files that git-crypt is encrypting using the following command: git-crypt status -e Granting access to others As mentioned above, team members can work on the public parts of the repository without decrypting the encrypted files. However in order to make a deployment, the secrets must be decrypted - git-crypt refers to this process as \"unlocking\". Using a shared secret key The simplest way to grant access to the encrypted files in a repository is by sharing the secret key generated by git-crypt . The key is binary, so the best way to share the key is by base64-encoding it: git-crypt export-key - | base64 To unlock the repository you must first clone it, then use the key to unlock it: # Clone the repository git clone $REPOSITORY_URL # Move into the repository directory cd $REPOSITORY_DIR # Unlock it using the base64-encoded key echo $GIT_CRYPT_KEY_B64 | base64 -d | git-crypt unlock - Using GPG keys git-crypt is also able to grant access to the repository using GPG keys . This avoids the use of shared secrets and makes it explicit in the repository who has access. A full discussion of the use of GPG is beyond the scope of this documentation, including the generation of keys, as it differs substantially depending on your operating system. To add a GPG key to your repository, use the following command: git-crypt add-gpg-user USER_ID where USER_ID is a key ID, a full fingerprint, an email address, or anything else that uniquely identifies a public key to GPG. You will then need to push the changes to the repository that encode the user's access: git push To unlock a repository using your GPG identity, just execute: git-crypt unlock","title":"Managing secrets"},{"location":"repository/secrets/#managing-secrets","text":"Each Azimuth environment contains a small number of files and variable values that must be kept secret. However we would also like to have these files and variables in version control so that we can track when they have changed and share them with others in our team. The way to achieve both these goals is to encrypt the secrets when at rest in the Git repository. For an Azimuth configuration repository, the recommended method for doing this is using git-crypt . git-crypt provides transparent encryption and decryption of files in a Git repository using Git filters . This allows for a mix of encrypted and unencrypted content in the same repository, which is exactly what we want for an Azimuth configuration. By encrypting files when they are committed and decrypting them when they are checked out, git-crypt allows you to work as if the encryption is not present while being confident that your secrets remain private. Team members can also work on the public parts of the repository without decrypting the private parts if you wish to maintain separation of privilege. git-crypt works on entire files, not at the variable level, so it is recommended that encrypted files contain only secret values so that information is not hidden unnecessarily. To this end, a typical azimuth-config environment for a site will have one or more group_vars files in the inventory that are unencrypted, but secret values will be placed in a secrets.yaml that is encrypted.","title":"Managing secrets"},{"location":"repository/secrets/#initialising-git-crypt","text":"To initialise git-crypt for your config repository, first make sure that the CLI is installed. This can be installed using the package manager on most major Linux operating systems, using Homebrew on Mac OSX or by building from source . Then execute the following command to begin encrypting files: git-crypt init Danger If you lose access to the key that git-crypt generates to encrypt your repository, you will be locked out of the repository for good. It is recommended that you export the key (see below) and store it somewhere safe, e.g. in your organisation's secret store.","title":"Initialising git-crypt"},{"location":"repository/secrets/#verifying-which-files-are-encrypted","text":"azimuth-config contains a .gitattributes file that ensures all clouds.yaml , secrets.yaml , env.secret and TLS key files in the repository will be encrypted (except those for the example environment). If required, you can add additional patterns to the file to encrypt other files. You can check the files that git-crypt is encrypting using the following command: git-crypt status -e","title":"Verifying which files are encrypted"},{"location":"repository/secrets/#granting-access-to-others","text":"As mentioned above, team members can work on the public parts of the repository without decrypting the encrypted files. However in order to make a deployment, the secrets must be decrypted - git-crypt refers to this process as \"unlocking\".","title":"Granting access to others"},{"location":"repository/secrets/#using-a-shared-secret-key","text":"The simplest way to grant access to the encrypted files in a repository is by sharing the secret key generated by git-crypt . The key is binary, so the best way to share the key is by base64-encoding it: git-crypt export-key - | base64 To unlock the repository you must first clone it, then use the key to unlock it: # Clone the repository git clone $REPOSITORY_URL # Move into the repository directory cd $REPOSITORY_DIR # Unlock it using the base64-encoded key echo $GIT_CRYPT_KEY_B64 | base64 -d | git-crypt unlock -","title":"Using a shared secret key"},{"location":"repository/secrets/#using-gpg-keys","text":"git-crypt is also able to grant access to the repository using GPG keys . This avoids the use of shared secrets and makes it explicit in the repository who has access. A full discussion of the use of GPG is beyond the scope of this documentation, including the generation of keys, as it differs substantially depending on your operating system. To add a GPG key to your repository, use the following command: git-crypt add-gpg-user USER_ID where USER_ID is a key ID, a full fingerprint, an email address, or anything else that uniquely identifies a public key to GPG. You will then need to push the changes to the repository that encode the user's access: git push To unlock a repository using your GPG identity, just execute: git-crypt unlock","title":"Using GPG keys"}]}